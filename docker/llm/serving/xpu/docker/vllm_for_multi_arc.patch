diff --git a/CMakeLists.txt b/CMakeLists.txt
index 15db4a4f4..1870e976e 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -80,6 +80,24 @@ endif()
 #
 find_package(Torch REQUIRED)
 
+#
+message(STATUS "Enabling core extension.")
+
+# # Define _core_C extension
+# #  built for (almost) every target platform, (excludes TPU and Neuron)
+
+# set(VLLM_EXT_SRC
+#   "csrc/core/torch_bindings.cpp")
+
+# define_gpu_extension_target(
+#   _core_C
+#   DESTINATION vllm
+#   LANGUAGE CXX
+#   SOURCES ${VLLM_EXT_SRC}
+#   COMPILE_FLAGS ${CXX_COMPILE_FLAGS}
+#   USE_SABI 3
+#   WITH_SOABI)
+
 #
 # Forward the non-CUDA device extensions to external CMake scripts.
 #
@@ -87,6 +105,10 @@ if (NOT VLLM_TARGET_DEVICE STREQUAL "cuda" AND
     NOT VLLM_TARGET_DEVICE STREQUAL "rocm")
     if (VLLM_TARGET_DEVICE STREQUAL "cpu")
         include(${CMAKE_CURRENT_LIST_DIR}/cmake/cpu_extension.cmake)
+    elseif(VLLM_TARGET_DEVICE STREQUAL "xpu")
+        message(STATUS "Building XPU")
+        set(VLLM_GPU_LANG "SYCL")
+        include(${CMAKE_CURRENT_LIST_DIR}/cmake/xpu_extension.cmake)
     else()
         return()
     endif()
diff --git a/benchmarks/backend_request_func.py b/benchmarks/backend_request_func.py
index ea70a1f48..d311ec9d9 100644
--- a/benchmarks/backend_request_func.py
+++ b/benchmarks/backend_request_func.py
@@ -259,7 +259,7 @@ async def async_request_openai_completions(
             "prompt": request_func_input.prompt,
             "temperature": 0.0,
             "max_tokens": request_func_input.output_len,
-            "logprobs": request_func_input.logprobs,
+            "min_tokens": request_func_input.output_len,
             "stream": True,
             "stream_options": {
                 "include_usage": True,
diff --git a/benchmarks/benchmark_prefix_caching.py b/benchmarks/benchmark_prefix_caching.py
index 4fff7a8fc..531770259 100644
--- a/benchmarks/benchmark_prefix_caching.py
+++ b/benchmarks/benchmark_prefix_caching.py
@@ -35,7 +35,8 @@ from typing import Optional
 
 from transformers import PreTrainedTokenizerBase
 
-from vllm import LLM, SamplingParams
+from vllm import SamplingParams
+from ipex_llm.vllm.xpu.engine import IPEXLLMClass as LLM
 from vllm.engine.arg_utils import EngineArgs
 from vllm.utils import FlexibleArgumentParser
 
@@ -192,7 +193,7 @@ def main(args):
 
     engine_args = EngineArgs.from_cli_args(args)
 
-    llm = LLM(**dataclasses.asdict(engine_args))
+    llm = LLM(**dataclasses.asdict(engine_args), load_in_low_bit=args.load_in_low_bit)
 
     sampling_params = SamplingParams(temperature=0,
                                      max_tokens=args.output_len,
@@ -252,6 +253,13 @@ if __name__ == "__main__":
               "detokenization time in the latency measurement)"),
     )
 
+    parser.add_argument(
+        "--load-in-low-bit",
+        type=str,
+        choices=["sym_int4", "fp8", "fp8_e4m3", "fp16", "fp6"],
+        default="sym_int4",
+        help="Low-bit format quantization with IPEX-LLM")
+
     parser = EngineArgs.add_cli_args(parser)
     args = parser.parse_args()
-    main(args)
+    main(args)
\ No newline at end of file
diff --git a/cmake/utils.cmake b/cmake/utils.cmake
index c9cd099b8..7c6c46f38 100644
--- a/cmake/utils.cmake
+++ b/cmake/utils.cmake
@@ -400,7 +400,7 @@ function (define_gpu_extension_target GPU_MOD_NAME)
     GPU
     "WITH_SOABI"
     "DESTINATION;LANGUAGE;USE_SABI"
-    "SOURCES;ARCHITECTURES;COMPILE_FLAGS;INCLUDE_DIRECTORIES;LIBRARIES")
+    "SOURCES;ARCHITECTURES;COMPILE_FLAGS;INCLUDE_DIRECTORIES;LIBRARIES;LINK_FLAGS")
 
   # Add hipify preprocessing step when building with HIP/ROCm.
   if (GPU_LANGUAGE STREQUAL "HIP")
@@ -442,6 +442,11 @@ function (define_gpu_extension_target GPU_MOD_NAME)
 
   target_link_libraries(${GPU_MOD_NAME} PRIVATE torch ${GPU_LIBRARIES})
 
+  if (GPU_LANGUAGE STREQUAL "SYCL")
+    target_compile_options(${GPU_MOD_NAME} PRIVATE ${GPU_COMPILE_FLAGS})
+    target_link_options(${GPU_MOD_NAME} PRIVATE ${GPU_LINK_FLAGS})
+  endif()   
+
   # Don't use `TORCH_LIBRARIES` for CUDA since it pulls in a bunch of
   # dependencies that are not necessary and may not be installed.
   if (GPU_LANGUAGE STREQUAL "CUDA")
diff --git a/cmake/xpu_extension.cmake b/cmake/xpu_extension.cmake
new file mode 100644
index 000000000..fd671a6bf
--- /dev/null
+++ b/cmake/xpu_extension.cmake
@@ -0,0 +1,62 @@
+set(CMAKE_EXPORT_COMPILE_COMMANDS ON)
+
+#
+# Define environment variables for special configurations
+#
+# TODO: detect Intel GPU Architecture(PVC or Arc) to add AOT flag.
+
+#
+# Check the compile flags
+#
+# append_cmake_prefix_path("intel_extension_for_pytorch" "intel_extension_for_pytorch.cmake_prefix_path")
+# find_package(IPEX REQUIRED)
+# IPEX will overwrite TORCH_LIBRARIES, so re-add torch_python lib.
+append_torchlib_if_found(torch_python)
+# include_directories(${IPEX_INCLUDE_DIRS})
+set(CMPLR_ROOT $ENV{CMPLR_ROOT})
+set(CMAKE_CXX_COMPILER icpx)
+set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wno-narrowing")
+set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -O3")
+set(VLLM_EXTRA_INCLUDE_DIRECTORIES ${CMPLR_ROOT}/include/sycl)
+
+list(APPEND VLLM_GPU_FLAGS "-fsycl" "-fsycl-targets=spir64")
+list(APPEND VLLM_GPU_LINK_FLAGS "-fsycl" "-fsycl-targets=spir64")
+list(APPEND VLLM_LINK_LIBRARIES "sycl" "OpenCL" "pthread" "m" "dl" "dnnl" )
+
+#
+# Define extension targets
+#
+
+#
+# _C extension
+#
+set(VLLM_EXT_SRC
+    "csrc/xpu/activation_xpu.cpp"
+    "csrc/xpu/attention_xpu.cpp"
+    "csrc/xpu/attention_xpu_fp8.cpp"
+    "csrc/xpu/cache_ops_xpu.cpp"
+    "csrc/xpu/cache_ops_xpu_fp8.cpp"
+    "csrc/xpu/gemm_kernels_xpu.cpp"
+    "csrc/xpu/layernorm_xpu.cpp"
+    "csrc/xpu/pos_encoding_xpu.cpp"
+    "csrc/xpu/utils.cpp"
+    "csrc/xpu/fused_moe.cpp"
+    "csrc/xpu/pybind.cpp")
+
+define_gpu_extension_target(
+    _C
+    DESTINATION vllm
+    LANGUAGE ${VLLM_GPU_LANG}
+    SOURCES ${VLLM_EXT_SRC}
+    COMPILE_FLAGS ${VLLM_GPU_FLAGS}
+    LINK_FLAGS ${VLLM_GPU_LINK_FLAGS}
+    ARCHITECTURES ${VLLM_GPU_ARCHES}
+    INCLUDE_DIRECTORIES ${VLLM_EXTRA_INCLUDE_DIRECTORIES}
+    LIBRARIES ${VLLM_LINK_LIBRARIES}
+    WITH_SOABI
+)
+
+add_custom_target(default_xpu)
+message(STATUS "Enabling C extension.")
+add_dependencies(default_xpu _C)
+
diff --git a/csrc/xpu/activation_xpu.cpp b/csrc/xpu/activation_xpu.cpp
new file mode 100644
index 000000000..6f98ddbb3
--- /dev/null
+++ b/csrc/xpu/activation_xpu.cpp
@@ -0,0 +1,278 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+// clang-format on
+#include "xpu_types.h"
+
+#include <torch/extension.h>
+#include "utils.h"
+
+template <typename T>
+__inline__ T silu_xpu(const T& x) {
+  // x * sigmoid(x)
+  return (T)(((float)x) / (1.0f + sycl::exp((float)-x)));
+}
+
+template<typename T>
+__inline__ T gelu_xpu(const T& x) {
+  // Equivalent to PyTorch GELU with 'none' approximation.
+  // Refer to:
+  // https://github.com/pytorch/pytorch/blob/8ac9b20d4b090c213799e81acf48a55ea8d437d6/aten/src/ATen/native/cuda/ActivationGeluKernel.cu#L38
+  const float f = (float) x;
+  constexpr float ALPHA = M_SQRT1_2;
+  return (T) (f * 0.5f * (1.0f + sycl::erf(f * ALPHA)));
+}
+
+template<typename T>
+__inline__ T gelu_tanh_xpu(const T& x) {
+  const float f = (float) x;
+  constexpr float BETA = M_SQRT2 * M_2_SQRTPI * 0.5f;
+  constexpr float KAPPA = 0.044715;
+  float x_cube = f * f * f;
+  float inner = BETA * (f + KAPPA * x_cube);
+  return (T) (0.5f * f * (1.0f + ::tanhf(inner)));
+}
+
+template <typename scalar_t>
+void silu_and_mul_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., 2, d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = input[token_idx * 2 * d + idx];
+    const scalar_t y = input[token_idx * 2 * d + d + idx];
+    out[token_idx * d + idx] = silu_xpu(x) * y;
+  }
+}
+
+template <typename scalar_t>
+void gelu_and_mul_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., 2, d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = input[token_idx * 2 * d + idx];
+    const scalar_t y = input[token_idx * 2 * d + d + idx];
+    out[token_idx * d + idx] = gelu_xpu(x) * y;
+  }
+}
+
+template <typename scalar_t>
+void gelu_tanh_and_mul_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., 2, d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = input[token_idx * 2 * d + idx];
+    const scalar_t y = input[token_idx * 2 * d + d + idx];
+    out[token_idx * d + idx] = gelu_tanh_xpu(x) * y;
+  }
+}
+
+
+template <typename scalar_t>
+void call_silu_and_mul_kernel(
+    int num_tokens,
+    int d,
+    const scalar_t* __restrict__ input,
+    scalar_t* __restrict__ output) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          silu_and_mul_kernel<sycl_t>(
+              (sycl_t*)output, (const sycl_t*)input, d, item_ct1);
+        });
+  });
+}
+
+template <typename scalar_t>
+void call_gelu_and_mul_kernel(
+    int num_tokens,
+    int d,
+    const scalar_t* __restrict__ input,
+    scalar_t* __restrict__ output) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          gelu_and_mul_kernel<sycl_t>(
+              (sycl_t*)output, (const sycl_t*)input, d, item_ct1);
+        });
+  });
+}
+
+template <typename scalar_t>
+void call_gelu_tanh_and_mul_kernel(
+    int num_tokens,
+    int d,
+    const scalar_t* __restrict__ input,
+    scalar_t* __restrict__ output) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          gelu_tanh_and_mul_kernel<sycl_t>(
+              (sycl_t*)output, (const sycl_t*)input, d, item_ct1);
+        });
+  });
+}
+
+void silu_and_mul(torch::Tensor& out, torch::Tensor& input) {
+  int num_tokens = input.numel() / input.size(-1);
+  int d = input.size(-1) / 2;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_silu_and_mul_kernel", [&] {
+        call_silu_and_mul_kernel(
+            num_tokens,
+            d,
+            input.data_ptr<scalar_t>(),
+            out.data_ptr<scalar_t>());
+      });
+}
+
+// Element-wise activation kernel template.
+template <typename scalar_t, scalar_t (*ACT_FN)(const scalar_t&)>
+void activation_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = VLLM_LDG(&input[token_idx * d + idx]);
+    out[token_idx * d + idx] = ACT_FN(x);
+  }
+}
+
+template <typename T>
+__inline__ T gelu_new_kernel(const T& x) {
+  const float x3 = (float)(x * x * x);
+  const T t = (T)tanhf((T)(0.79788456f * (float)(x + (T)(0.044715f * x3))));
+  return ((T)0.5) * x * (((T)1.0) + t);
+}
+
+template <typename T>
+__inline__ T gelu_fast_kernel(const T& x) {
+  const float f = (float)x;
+  const T t =
+      (T)tanhf(((T)(f * 0.79788456f)) * (((T)1.0) + (T)(0.044715f * f) * x));
+  return ((T)0.5) * x * (((T)1.0) + t);
+}
+
+template <typename scalar_t>
+void call_gelu_new_activation_kernel(torch::Tensor& out, torch::Tensor& input) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int d = input.size(-1);
+  int64_t num_tokens = input.numel() / d;
+  auto out_ptr = out.data_ptr<scalar_t>();
+  auto input_ptr = input.data_ptr<scalar_t>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          activation_kernel<sycl_t, gelu_new_kernel>(
+              (sycl_t* __restrict__)out_ptr,
+              (const sycl_t* __restrict__)input_ptr,
+              d,
+              item_ct1);
+        });
+  });
+}
+
+template <typename scalar_t>
+void call_gelu_fast_activation_kernel(
+    torch::Tensor& out,
+    torch::Tensor& input) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int d = input.size(-1);
+  int64_t num_tokens = input.numel() / d;
+  auto out_ptr = out.data_ptr<scalar_t>();
+  auto input_ptr = input.data_ptr<scalar_t>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          activation_kernel<sycl_t, gelu_fast_kernel>(
+              (sycl_t* __restrict__)out_ptr,
+              (const sycl_t* __restrict__)input_ptr,
+              d,
+              item_ct1);
+        });
+  });
+}
+
+void gelu_new(torch::Tensor& out, torch::Tensor& input) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      out.scalar_type(), "call_gelu_new_activation_kernel", [&] {
+        call_gelu_new_activation_kernel<scalar_t>(out, input);
+      });
+}
+
+void gelu_fast(torch::Tensor& out, torch::Tensor& input) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      out.scalar_type(), "call_gelu_fast_activation_kernel", [&] {
+        call_gelu_fast_activation_kernel<scalar_t>(
+            out, input);
+      });
+}
+
+void gelu_and_mul(
+  torch::Tensor& out,      // [..., d]
+  torch::Tensor& input)    // [..., 2 * d]
+{
+    int num_tokens = input.numel() / input.size(-1);
+  int d = input.size(-1) / 2;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_gelu_and_mul_kernel", [&] {
+        call_gelu_and_mul_kernel(
+            num_tokens,
+            d,
+            input.data_ptr<scalar_t>(),
+            out.data_ptr<scalar_t>());
+      });
+}
+
+void gelu_tanh_and_mul(
+  torch::Tensor& out,      // [..., d]
+  torch::Tensor& input)    // [..., 2 * d]
+{
+    int num_tokens = input.numel() / input.size(-1);
+  int d = input.size(-1) / 2;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_gelu_tanh_and_mul_kernel", [&] {
+        call_gelu_tanh_and_mul_kernel(
+            num_tokens,
+            d,
+            input.data_ptr<scalar_t>(),
+            out.data_ptr<scalar_t>());
+      });
+}
\ No newline at end of file
diff --git a/csrc/xpu/attention_generic.h b/csrc/xpu/attention_generic.h
new file mode 100644
index 000000000..ab3688c82
--- /dev/null
+++ b/csrc/xpu/attention_generic.h
@@ -0,0 +1,64 @@
+/*
+ * Copyright (c) 2023, The vLLM team.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <dpct/dpct.hpp>
+#include <stdint.h>
+#include <sycl/sycl.hpp>
+
+namespace vllm {
+
+// A vector type to store Q, K, V elements.
+template <typename T, int VEC_SIZE>
+struct Vec {};
+
+// A vector type to store FP32 accumulators.
+template <typename T>
+struct FloatVec {};
+
+// Template vector operations.
+template <typename Acc, typename A, typename B>
+inline Acc mul(A a, B b);
+
+template <typename T>
+inline float sum(T v);
+
+template <typename T>
+inline float dot(T a, T b) {
+  return sum(mul<T, T, T>(a, b));
+}
+
+template <typename A, typename T>
+inline float dot(T a, T b) {
+  return sum(mul<A, T, T>(a, b));
+}
+
+template <typename T>
+inline void zero(T& dst) {
+  constexpr int WORDS = (sizeof(T) / 4) == 0 ? 1 : (sizeof(T) / 4);
+  union {
+    T raw;
+    uint32_t words[WORDS];
+  } tmp;
+
+#pragma unroll
+  for (int ii = 0; ii < WORDS; ++ii) {
+    tmp.words[ii] = 0u;
+  }
+  dst = tmp.raw;
+}
+
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/attention_xpu.cpp b/csrc/xpu/attention_xpu.cpp
new file mode 100644
index 000000000..97d5c0c21
--- /dev/null
+++ b/csrc/xpu/attention_xpu.cpp
@@ -0,0 +1,3031 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <ext/intel/esimd.hpp>
+
+// clang-format on
+#include <float.h>
+#include <torch/extension.h>
+#include <stdexcept>
+#include "utils.h"
+#include "xpu_types.h"
+// #include "dtype_bfloat16.dp.hpp"
+#include "dtype_float16.h"
+#include "dtype_float32.h"
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+#include <c10/xpu/XPUStream.h>
+#endif
+
+#include <functional>
+// #include <ipex.h>
+
+#define WARP_SIZE 32
+#define MAX(a, b) ((a) > (b) ? (a) : (b))
+#define MIN(a, b) ((a) < (b) ? (a) : (b))
+#define DIVIDE_ROUND_UP(a, b) (((a) + (b)-1) / (b))
+using namespace sycl::ext::intel::esimd;
+
+template<typename T>
+static inline T attn_softcapping(T qk, float attn_logit_softcapping) {
+    qk = qk / attn_logit_softcapping;
+    qk = (sycl::exp(qk) - sycl::exp(-qk)) / (sycl::exp(qk) + sycl::exp(-qk));
+    qk = qk * attn_logit_softcapping;
+    return qk;
+}
+
+template <typename T>
+struct Float_Trait {
+  using Type = T;
+};
+
+template <>
+struct Float_Trait<c10::Half> {
+  using Type = uint16_t;
+};
+
+template <>
+struct Float_Trait<c10::BFloat16> {
+  using Type = sycl::ext::oneapi::bfloat16;
+};
+
+namespace vllm {
+
+// Q*K^T operation.
+template <int THREAD_GROUP_SIZE, typename Vec, int N>
+inline float qk_dot_(
+    const Vec* q,
+    const Vec* k,
+    const sycl::nd_item<3>& item_ct1) {
+  using A_vec = typename FloatVec<Vec>::Type;
+  // Compute the parallel products for Q*K^T (treat vector lanes separately).
+  A_vec qk_vec = mul<A_vec, Vec, Vec>(q[0], k[0]);
+#pragma unroll
+  for (int ii = 1; ii < N; ++ii) {
+    qk_vec = fma(q[ii], k[ii], qk_vec);
+  }
+
+  // Finalize the reduction across lanes.
+  float qk = sum(qk_vec);
+#pragma unroll
+  for (int mask = THREAD_GROUP_SIZE / 2; mask >= 1; mask /= 2) {
+    
+    qk += dpct::permute_sub_group_by_xor(
+        item_ct1.get_sub_group(), qk, mask);
+  }
+  return qk;
+}
+
+template <typename T, int THREAD_GROUP_SIZE>
+struct Qk_dot {
+  template <typename Vec, int N>
+  static inline float dot(
+      const Vec* q,
+      const Vec* k,
+      const sycl::nd_item<3>& item_ct1) {
+    return qk_dot_<THREAD_GROUP_SIZE, Vec, N>(q, k, item_ct1);
+  }
+};
+
+template <int NUM_WARPS>
+inline float block_sum(
+    float* red_smem,
+    float sum,
+    const sycl::nd_item<3>& item_ct1) {
+  // Decompose the thread index into warp / lane.
+  int warp = item_ct1.get_local_id(2) / WARP_SIZE;
+  int lane = item_ct1.get_local_id(2) % WARP_SIZE;
+
+  // Compute the sum per warp.
+#pragma unroll
+  for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:42: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    sum += dpct::permute_sub_group_by_xor(
+        item_ct1.get_sub_group(), sum, mask);
+  }
+
+  // Warp leaders store the data to shared memory.
+  if (lane == 0) {
+    red_smem[warp] = sum;
+  }
+
+  // Make sure the data is in shared memory.
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // The warps compute the final sums.
+  if (lane < NUM_WARPS) {
+    sum = red_smem[lane];
+  }
+
+  // Parallel reduction inside the warp.
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:43: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    sum += dpct::permute_sub_group_by_xor(
+        item_ct1.get_sub_group(), sum, mask);
+  }
+
+  // Broadcast to other threads.
+  
+  /*
+  DPCT1096:44: The right-most dimension of the work-group used in the SYCL
+  kernel that calls this function may be less than "32". The function
+  "dpct::select_from_sub_group" may return an unexpected result on the CPU
+  device. Modify the size of the work-group to ensure that the value of the
+  right-most dimension is a multiple of "32".
+  */
+  return dpct::select_from_sub_group(
+        item_ct1.get_sub_group(), sum, 0);
+}
+
+template <typename scalar_t, int GS, int HD>
+void context_attention_kernel_v1_reshaped(
+    void* query, void* key, void* value, const void* block_tables,
+    const float scale, const void* query_start_loc, const void* seq_lens,
+    const void* context_lens, const int block_size,
+    // const int x,  // x in kv_cache
+    void* out,    // output
+    const int block_table_stride_batch, const int block_table_stride_seq,
+    const int query_stride_bs, const int query_stride_head,
+    const int query_stride_dim, const int k_cache_stride_tokens,
+    const int k_cache_stride_head, const int k_cache_stride_block_size,
+    const int k_cache_stride_dim,
+    const int v_cache_stride_tokens, const int v_cache_stride_head,
+    const int v_cache_stride_block_size, const int v_cache_stride_dim,
+    const int out_stride_tokens, const int out_stride_head,
+    const int num_queries_per_kv, const int max_input_length,
+    const int batch_size, const int num_heads) {
+  static_assert(GS * HD * sizeof(scalar_t) * 2 < 64 * 1024);
+
+  const size_t key_slm_offset = 0;
+  const size_t value_slm_offset = GS * HD * sizeof(scalar_t);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+
+  // Get the maximum seq_lens
+  sycl::range<3> global_size(batch_size, num_heads,
+                             (max_input_length + GS - 1) / GS * GS);
+  sycl::range<3> local_size(1, 1, GS);
+
+  auto cgf = [&](sycl::handler& handle) {
+    handle.parallel_for(
+        sycl::nd_range<3>(global_size, local_size),
+        [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+          slm_init<GS * HD * sizeof(scalar_t) * 2>();
+
+          const size_t bsz_idx = item.get_global_id(0);
+          const size_t head_idx = item.get_global_id(1);
+          // Assuming we have 32 query head and 8 kv_heads. Then
+          // num_queries_per_group should be 4 For head_idx 13, then
+          // kv_head_idx = 13 / 4 = 3, which is correct
+          const size_t kv_head_idx = head_idx / num_queries_per_kv;
+          const int32_t seq_idx = item.get_global_id(2);
+          const size_t gid = item.get_group(2);
+          const size_t tid = item.get_local_id(2);
+
+          // const int64_t * seq_len = (const int64_t *) seq_lens;
+          const int32_t* seq_len = (const int32_t*)seq_lens;
+          int32_t seq_bound = seq_len[bsz_idx];
+
+          const int32_t* query_loc = (const int32_t*)query_start_loc;
+          // There is a possibility that the current token index pass
+          // over the seq_len, therefore: token_idx is the position in
+          // the query
+          int32_t token_idx =
+              query_loc[bsz_idx] + std::min(seq_idx, seq_bound - 1);
+
+          const int32_t* context_len_pointer = (const int32_t*)context_lens;
+
+          const int* block_tables_ptr = (const int*)block_tables;
+          const int* block_table =
+              block_tables_ptr + bsz_idx * block_table_stride_batch;
+          // I guess this context_len should be 0...
+          const int32_t context_len = context_len_pointer[bsz_idx];
+
+          // Position in the sequence
+          // context + seq_idx
+          // const int32_t token_position =
+          //     context_len + std::min(seq_idx, seq_bound - 1);
+          const int32_t token_position = context_len + seq_idx;
+
+          const scalar_t* query_head = (const scalar_t*)query +
+                                       token_idx * query_stride_bs +
+                                       head_idx * query_stride_head;
+          // Target output
+          scalar_t* out_head =
+              (scalar_t*)out +
+              (query_loc[bsz_idx] + seq_idx) * out_stride_tokens +
+              head_idx * out_stride_head;
+
+          int32_t context_groups = context_len / GS;
+
+          // Each token load its query_row
+          simd<scalar_t, HD> query_row =
+              block_load<scalar_t, HD>(query_head) * scale;
+          simd<scalar_t, HD> accv = 0;
+          simd<scalar_t, GS> softmaxv = 0;
+          scalar_t max_attn = -sycl::detail::max_v<scalar_t>();
+
+          // ################# Handle n * GS context part ######################
+          int32_t n = context_len / GS;
+          int32_t context_offset = context_len % GS;
+
+          for (int32_t group = 0; group < n; ++group) {
+            size_t target_key_position = group * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            // Now key shape is [num_blocks, num_heads, block_size, head_dim]
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            simd<scalar_t, HD> key_row = block_load<scalar_t, HD>(key_head);
+            slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t), key_row);
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot * v_cache_stride_block_size;
+            simd<scalar_t, HD> value_row = block_load<scalar_t, HD>(value_head);
+            slm_block_store(value_slm_offset + tid * HD * sizeof(scalar_t),
+                            value_row);
+            barrier();
+
+            // Calculate QK^T for this group...
+            simd<scalar_t, GS> attnv;
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              attnv[r] = attn;
+            }
+            scalar_t new_max_attn =
+                std::max(hmax<scalar_t, scalar_t, GS>(attnv), max_attn);
+            scalar_t attn_exp = exp(max_attn - new_max_attn);
+            accv = accv * attn_exp;
+            softmaxv = softmaxv * attn_exp;
+            max_attn = new_max_attn;
+            const simd<scalar_t, GS> attn_expv = exp(attnv - max_attn);
+#pragma unorll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              accv += value_row * attn_expv[r];
+            }
+            softmaxv += attn_expv;
+            barrier();
+          }
+
+          // ########## End for handling context n * GS part ###########
+
+          // ########## Handle n * GS ################
+          for (size_t group = 0; group < gid; ++group) {
+            // 1. begins to load each position's key and value
+            size_t target_key_position = context_len + group * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            simd<scalar_t, HD> key_row = block_load<scalar_t, HD>(key_head);
+            slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t),
+                            key_row);
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot * v_cache_stride_block_size;
+            simd<scalar_t, HD> value_row = block_load<scalar_t, HD>(value_head);
+            slm_block_store(value_slm_offset + tid * HD * sizeof(scalar_t),
+                            value_row);
+            barrier();
+            simd<scalar_t, GS> attnv;
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              attnv[r] = attn;
+            }
+
+            scalar_t new_max_attn =
+                std::max(hmax<scalar_t, scalar_t, GS>(attnv), max_attn);
+            scalar_t attn_exp = exp(max_attn - new_max_attn);
+            accv = accv * attn_exp;
+
+            softmaxv = softmaxv * attn_exp;
+            max_attn = new_max_attn;
+            const simd<scalar_t, GS> attn_expv = exp(attnv - max_attn);
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              accv += value_row * attn_expv[r];
+            }
+            softmaxv += attn_expv;
+            barrier();
+          }
+
+          // ######### End of handle n * GS part ##########
+
+          // ################ Handle offset part ####################
+          scalar_t softmax =
+              sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, GS>(
+                  softmaxv);
+
+          // ########### handle context offset ############
+          if (tid < context_offset) {
+            size_t target_key_position = n * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            simd<scalar_t, HD> key_row = block_load<scalar_t, HD>(key_head);
+            slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t),
+                            key_row);
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head +
+                which_slot * v_cache_stride_block_size;
+            simd<scalar_t, HD> value_row = block_load<scalar_t, HD>(value_head);
+            slm_block_store(value_slm_offset + tid * HD * sizeof(scalar_t),
+                            value_row);
+          }
+
+          barrier();
+
+          if (token_position < seq_bound) {
+#pragma unroll
+            for (size_t r = 0; r < context_offset; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              if (attn <= max_attn) {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(attn - max_attn);
+                accv += value_row * attn_exp;
+                softmax += attn_exp;
+              } else {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(max_attn - attn);
+                accv = accv * attn_exp + value_row;
+                softmax = softmax * attn_exp + 1;
+                max_attn = attn;
+              }
+            }
+          }
+          barrier();
+
+          // ############## handle seq offset #################
+          if (token_position < seq_bound) {
+            const int64_t which_block =
+                static_cast<int64_t>(token_position / block_size);
+            const int64_t which_slot =
+                static_cast<int64_t>(token_position % block_size);
+
+            const int64_t physical_block_number =
+                static_cast<int64_t>(block_table[which_block]);
+
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            simd<scalar_t, HD> key_row = block_load<scalar_t, HD>(key_head);
+            slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t),
+                            key_row);
+
+            // [num_blocks, num_kv_heads, head_size, block_size]
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head +
+                which_slot * v_cache_stride_block_size;
+            simd<scalar_t, HD> value_row = block_load<scalar_t, HD>(value_head);
+            slm_block_store(value_slm_offset + tid * HD * sizeof(scalar_t),
+                            value_row);
+          }
+          barrier();
+
+          if (token_position < seq_bound) {
+            for (size_t r = 0; r <= tid; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              if (attn <= max_attn) {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(attn - max_attn);
+                accv += value_row * attn_exp;
+                softmax += attn_exp;
+              } else {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(max_attn - attn);
+                accv = accv * attn_exp + value_row;
+                softmax = softmax * attn_exp + 1;
+                max_attn = attn;
+              }
+            }
+
+            if (softmax > 0) {
+              simd<scalar_t, HD> result = accv / softmax;
+              block_store(out_head, result);
+            } else {
+              simd<scalar_t, HD> result = 0;
+              block_store(out_head, result);
+            }
+          }
+          // ######## Ending of handling seq offset ##########
+        });
+  };
+  queue.submit(cgf);
+}
+
+// How about implement a first edition that can be used with non-chunked
+// prefill requests, so that we can make sure the reference for heads is
+// correct
+template <typename scalar_t, int GS, int HD>
+void context_attention_kernel_v1(
+    void* query, void* key, void* value, const void* block_tables,
+    const float scale, const void* query_start_loc, const void* seq_lens,
+    const void* context_lens, const int block_size,
+    const int x,  // x in kv_cache
+    void* out,    // output
+    const int block_table_stride_batch, const int block_table_stride_seq,
+    const int query_stride_bs, const int query_stride_head,
+    const int query_stride_dim, const int k_cache_stride_tokens,
+    const int k_cache_stride_head, const int k_cache_stride_dim,
+    const int k_cache_stride_block_size, const int k_cache_stride_x,
+    const int v_cache_stride_tokens, const int v_cache_stride_head,
+    const int v_cache_stride_dim, const int v_cache_stride_block_size,
+    const int out_stride_tokens, const int out_stride_head,
+    const int num_queries_per_kv, const int max_input_length,
+    const int batch_size, const int num_heads) {
+  static_assert(GS * HD * sizeof(scalar_t) * 2 < 64 * 1024);
+
+  const size_t key_slm_offset = 0;
+  const size_t value_slm_offset = GS * HD * sizeof(scalar_t);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+
+  // Get the maximum seq_lens
+  sycl::range<3> global_size(batch_size, num_heads,
+                             (max_input_length + GS - 1) / GS * GS);
+  sycl::range<3> local_size(1, 1, GS);
+
+  auto cgf = [&](sycl::handler& handle) {
+    handle.parallel_for(
+        sycl::nd_range<3>(global_size, local_size),
+        [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+          slm_init<GS * HD * sizeof(scalar_t) * 2>();
+
+          const size_t bsz_idx = item.get_global_id(0);
+          const size_t head_idx = item.get_global_id(1);
+          // Assuming we have 32 query head and 8 kv_heads. Then
+          // num_queries_per_group should be 4 For head_idx 13, then
+          // kv_head_idx = 13 / 4 = 3, which is correct
+          const size_t kv_head_idx = head_idx / num_queries_per_kv;
+          const int32_t seq_idx = item.get_global_id(2);
+          const size_t gid = item.get_group(2);
+          const size_t tid = item.get_local_id(2);
+
+          // const int64_t * seq_len = (const int64_t *) seq_lens;
+          const int32_t* seq_len = (const int32_t*)seq_lens;
+          int32_t seq_bound = seq_len[bsz_idx];
+
+          const int32_t* query_loc = (const int32_t*)query_start_loc;
+          // There is a possibility that the current token index pass
+          // over the seq_len, therefore: token_idx is the position in
+          // the query
+          int32_t token_idx =
+              query_loc[bsz_idx] + std::min(seq_idx, seq_bound - 1);
+
+          const int32_t* context_len_pointer = (const int32_t*)context_lens;
+
+          const int* block_tables_ptr = (const int*)block_tables;
+          const int* block_table =
+              block_tables_ptr + bsz_idx * block_table_stride_batch;
+          // I guess this context_len should be 0...
+          const int32_t context_len = context_len_pointer[bsz_idx];
+
+          // Position in the sequence
+          // context + seq_idx
+          // const int32_t token_position =
+          //     context_len + std::min(seq_idx, seq_bound - 1);
+          const int32_t token_position = context_len + seq_idx;
+
+          // static const CONSTANT char FMT[] =
+          //     "Invoke target function...\n ";
+
+          // sycl::ext::oneapi::experimental::printf(FMT);
+          // static const CONSTANT char FMT[] =
+          //     "GroupID = %6d bsz_idx = %6d seq_len = %6d seq_idx =
+          //     %6d" "local_id = "
+          //     "%6d "
+          //     "token_idx = %6d "
+          //     "context_len = %6d "
+          //     "v_cache_stride_head_dim = %6d "
+          //     "token_position = %6d\n";
+          // sycl::ext::oneapi::experimental::printf(
+          //     FMT, gid, bsz_idx, seq_bound, seq_idx, tid,
+          //     token_idx, context_len, v_cache_stride_dim,
+          //     token_position);
+
+          const scalar_t* query_head = (const scalar_t*)query +
+                                       token_idx * query_stride_bs +
+                                       head_idx * query_stride_head;
+          // Target output
+          scalar_t* out_head =
+              (scalar_t*)out +
+              (query_loc[bsz_idx] + seq_idx) * out_stride_tokens +
+              head_idx * out_stride_head;
+
+          int32_t context_groups = context_len / GS;
+
+          // Each token load its query_row
+          simd<scalar_t, HD> query_row =
+              block_load<scalar_t, HD>(query_head) * scale;
+          simd<scalar_t, HD> accv = 0;
+          simd<scalar_t, GS> softmaxv = 0;
+          scalar_t max_attn = -sycl::detail::max_v<scalar_t>();
+
+          // ################# Handle n * GS context part ######################
+          int32_t n = context_len / GS;
+          int32_t context_offset = context_len % GS;
+
+          for (int32_t group = 0; group < n; ++group) {
+            size_t target_key_position = group * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            for (int i = 0; i < HD / x; i++) {
+              // Load 8 elements, decided by x
+              simd<scalar_t, 8> key_row =
+                  block_load<scalar_t, 8>(key_head + i * k_cache_stride_dim);
+              slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t) +
+                                  8 * i * sizeof(scalar_t),
+                              key_row);
+            }
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot;
+            for (int i = 0; i < HD; i++) {
+              scalar_t temp_value = value_head[i * v_cache_stride_dim];
+              slm_scalar_store<scalar_t>(value_slm_offset +
+                                             tid * HD * sizeof(scalar_t) +
+                                             i * sizeof(scalar_t),
+                                         temp_value);
+            }
+            barrier();
+
+            // Calculate QK^T for this group...
+            simd<scalar_t, GS> attnv;
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              attnv[r] = attn;
+            }
+            scalar_t new_max_attn =
+                std::max(hmax<scalar_t, scalar_t, GS>(attnv), max_attn);
+            scalar_t attn_exp = exp(max_attn - new_max_attn);
+            accv = accv * attn_exp;
+            softmaxv = softmaxv * attn_exp;
+            max_attn = new_max_attn;
+            const simd<scalar_t, GS> attn_expv = exp(attnv - max_attn);
+#pragma unorll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              accv += value_row * attn_expv[r];
+            }
+            softmaxv += attn_expv;
+            barrier();
+          }
+
+          // ########## End for handling context n * GS part ###########
+
+          // ########## Handle n * GS ################
+          for (size_t group = 0; group < gid; ++group) {
+            // 1. begins to load each position's key and value
+            size_t target_key_position = context_len + group * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            for (int i = 0; i < HD / x; i++) {
+              // Load 8 elements
+              simd<scalar_t, 8> key_row =
+                  block_load<scalar_t, 8>(key_head + i * k_cache_stride_dim);
+              slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t) +
+                                  8 * i * sizeof(scalar_t),
+                              key_row);
+            }
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot;
+            for (int i = 0; i < HD; i++) {
+              scalar_t temp_value = value_head[i * v_cache_stride_dim];
+              slm_scalar_store<scalar_t>(value_slm_offset +
+                                             tid * HD * sizeof(scalar_t) +
+                                             i * sizeof(scalar_t),
+                                         temp_value);
+            }
+            barrier();
+            simd<scalar_t, GS> attnv;
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              attnv[r] = attn;
+            }
+
+            scalar_t new_max_attn =
+                std::max(hmax<scalar_t, scalar_t, GS>(attnv), max_attn);
+            scalar_t attn_exp = exp(max_attn - new_max_attn);
+            accv = accv * attn_exp;
+
+            softmaxv = softmaxv * attn_exp;
+            max_attn = new_max_attn;
+            const simd<scalar_t, GS> attn_expv = exp(attnv - max_attn);
+#pragma unroll
+            for (size_t r = 0; r < GS; ++r) {
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              accv += value_row * attn_expv[r];
+            }
+            softmaxv += attn_expv;
+            barrier();
+          }
+
+          // ######### End of handle n * GS part ##########
+
+          // ################ Handle offset part ####################
+          scalar_t softmax =
+              sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, GS>(
+                  softmaxv);
+
+          // ########### handle context offset ############
+          if (tid < context_offset) {
+            size_t target_key_position = n * GS + tid;
+            int which_block = target_key_position / block_size;
+            int which_slot = target_key_position % block_size;
+
+            int physical_block_number = block_table[which_block];
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+            for (int i = 0; i < HD / x; i++) {
+              // Load 8 elements
+              simd<scalar_t, 8> key_row =
+                  block_load<scalar_t, 8>(key_head + i * k_cache_stride_dim);
+              slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t) +
+                                  8 * i * sizeof(scalar_t),
+                              key_row);
+            }
+
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot;
+            for (int i = 0; i < HD; i++) {
+              // Seems to have an error here
+              scalar_t temp_value = value_head[i * v_cache_stride_dim];
+              slm_scalar_store<scalar_t>(value_slm_offset +
+                                             tid * HD * sizeof(scalar_t) +
+                                             i * sizeof(scalar_t),
+                                         temp_value);
+            }
+          }
+
+          barrier();
+
+          if (token_position < seq_bound) {
+#pragma unroll
+            for (size_t r = 0; r < context_offset; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              if (attn <= max_attn) {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(attn - max_attn);
+                accv += value_row * attn_exp;
+                softmax += attn_exp;
+              } else {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(max_attn - attn);
+                accv = accv * attn_exp + value_row;
+                softmax = softmax * attn_exp + 1;
+                max_attn = attn;
+              }
+            }
+          }
+          barrier();
+
+          // ############## handle seq offset #################
+          if (token_position < seq_bound) {
+            const int64_t which_block =
+                static_cast<int64_t>(token_position / block_size);
+            const int64_t which_slot =
+                static_cast<int64_t>(token_position % block_size);
+
+            const int64_t physical_block_number =
+                static_cast<int64_t>(block_table[which_block]);
+
+            const scalar_t* key_head =
+                (const scalar_t*)key +
+                physical_block_number * k_cache_stride_tokens +
+                kv_head_idx * k_cache_stride_head +
+                which_slot * k_cache_stride_block_size;
+
+            for (int i = 0; i < HD / x; i++) {
+              // Load 8 elements
+              simd<scalar_t, 8> key_row =
+                  block_load<scalar_t, 8>(key_head + i * k_cache_stride_dim);
+              slm_block_store(key_slm_offset + tid * HD * sizeof(scalar_t) +
+                                  8 * i * sizeof(scalar_t),
+                              key_row);
+            }
+
+            // [num_blocks, num_kv_heads, head_size, block_size]
+            const scalar_t* value_head =
+                (const scalar_t*)value +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head + which_slot;
+            for (int i = 0; i < HD; i++) {
+              scalar_t temp_value = value_head[i * v_cache_stride_dim];
+              slm_scalar_store<scalar_t>(value_slm_offset +
+                                             tid * HD * sizeof(scalar_t) +
+                                             i * sizeof(scalar_t),
+                                         temp_value);
+            }
+          }
+          barrier();
+
+          if (token_position < seq_bound) {
+            for (size_t r = 0; r <= tid; ++r) {
+              simd<scalar_t, HD> key_row = slm_block_load<scalar_t, HD>(
+                  key_slm_offset + r * HD * sizeof(scalar_t));
+              simd<scalar_t, HD> value_row = slm_block_load<scalar_t, HD>(
+                  value_slm_offset + r * HD * sizeof(scalar_t));
+              scalar_t attn =
+                  sycl::ext::intel::esimd::detail::sum<scalar_t, scalar_t, HD>(
+                      query_row * key_row);
+              if (attn <= max_attn) {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(attn - max_attn);
+                accv += value_row * attn_exp;
+                softmax += attn_exp;
+              } else {
+                scalar_t attn_exp =
+                    sycl::ext::intel::esimd::exp(max_attn - attn);
+                accv = accv * attn_exp + value_row;
+                softmax = softmax * attn_exp + 1;
+                max_attn = attn;
+              }
+            }
+
+            if (softmax > 0) {
+              simd<scalar_t, HD> result = accv / softmax;
+              block_store(out_head, result);
+            } else {
+              simd<scalar_t, HD> result = 0;
+              block_store(out_head, result);
+            }
+          }
+          // ######## Ending of handling seq offset ##########
+        });
+  };
+  queue.submit(cgf);
+}
+
+template <typename T, int GS, int HD>
+void context_attention_kernel_v2(
+    void* query, void* key, void* value, const void* block_tables,
+    const float scale, const void* query_start_loc, const void* seq_lens,
+    const void* context_lens, const int block_size,
+    const int x,  // x in kv_cache
+    void* out,    // output
+    const int block_table_stride_batch, const int block_table_stride_seq,
+    const int query_stride_bs, const int query_stride_head,
+    const int query_stride_dim, const int k_cache_stride_tokens,
+    const int k_cache_stride_head, const int k_cache_stride_dim,
+    const int k_cache_stride_block_size, const int k_cache_stride_x,
+    const int v_cache_stride_tokens, const int v_cache_stride_head,
+    const int v_cache_stride_dim, const int v_cache_stride_block_size,
+    const int out_stride_tokens, const int out_stride_head,
+    const int num_queries_per_kv, const int max_input_length,
+    const int batch_size, const int num_heads, const int num_tokens,
+    const int max_context_len, const int max_q_len) {
+  constexpr int BLOCK_SIZE = 8;
+  constexpr int NUM_THREADS = 128;
+  // Each wrap handles one context block, therefore, each thread_group_size is
+  // this.
+  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  // Each query, and key thread_group loads 16 bytes
+  // Assume TGS=4 then 16 / 4 / sizeof(half) = 2
+  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(T)), 1);
+  using sycl_t = vllm::xpu::SyclTypeTrait<T>::Type;
+  using Q_Vec = typename Vec<sycl_t, VEC_SIZE>::Type;
+
+  // Assuming HD = 128, TGS = 2, then 128 / 2 / 2 = 32
+  int num_vecs_per_thread = HD / THREAD_GROUP_SIZE / VEC_SIZE;
+  sycl_t* out_p = reinterpret_cast<sycl_t*>(out);
+  sycl_t* query_ptr = reinterpret_cast<sycl_t*>(query);
+  sycl_t* key_cache_ptr = reinterpret_cast<sycl_t*>(key);
+  sycl_t* value_cache_ptr = reinterpret_cast<sycl_t*>(value);
+  const int* query_loc_ptr = reinterpret_cast<const int*>(query_start_loc);
+  const int* block_tables_ptr = reinterpret_cast<const int*>(block_tables);
+  const int* context_lens_ptr = reinterpret_cast<const int*>(context_lens);
+  const int* seq_lens_ptr = reinterpret_cast<const int*>(seq_lens);
+
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  int padded_max_context_len =
+      DIVIDE_ROUND_UP(max_context_len + 1 + max_q_len, BLOCK_SIZE) * BLOCK_SIZE;
+  int logits_size = padded_max_context_len * sizeof(float);
+  int outputs_size = (NUM_WARPS / 2) * HD * sizeof(float);
+  // Python-side check in
+  // vllm.worker.worker._check_if_can_support_max_seq_len Keep that in
+  // sync with the logic here!
+  int shared_mem_size = std::max(logits_size, outputs_size);
+  // WARN: we have changed this...
+  sycl::range<3> grid(batch_size, num_heads, max_q_len);
+  // One work-group that is executing on the device
+  sycl::range<3> block(1, 1, NUM_THREADS);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+
+  auto cgf = [&](sycl::handler& handle) {
+    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(
+        sycl::range<1>(shared_mem_size), handle);
+    sycl::local_accessor<Q_Vec, 1> q_vecs_acc_ct1(
+        sycl::range<1>(THREAD_GROUP_SIZE * num_vecs_per_thread), handle);
+    sycl::local_accessor<float, 1> red_smem_acc_ct1(
+        sycl::range<1>(2 * NUM_WARPS), handle);
+
+    handle.parallel_for(
+        sycl::nd_range<3>(grid * block, block),
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {
+          const int bsz_idx = item_ct1.get_group(0);
+          const int seq_idx = item_ct1.get_group(2);
+          constexpr bool USE_PARTITIONING = false;
+          int context_len = context_lens_ptr[bsz_idx] + seq_idx;
+          const int seq_len = seq_lens_ptr[bsz_idx];
+          uint8_t* dpct_local = dpct_local_acc_ct1.get_pointer();
+          Q_Vec* q_vecs = q_vecs_acc_ct1.get_pointer();
+          float* red_smem = red_smem_acc_ct1.get_pointer();
+
+          // output_stream << "Original context_len: " <<
+          // context_lens_ptr[bsz_idx] << sycl::endl; output_stream <<
+          // "Batch_idx: " << bsz_idx << " Seq_idx: " << seq_idx
+          //     << " Context_len: " << context_len << " Original context_len: "
+          //     << context_lens_ptr[bsz_idx] << " Seq_len: " << seq_len
+          //     << " Max input length: " << max_input_length
+          //     << sycl::endl;
+          if (context_len >= seq_len) {
+            return;
+          }
+
+          context_len = context_len + 1;
+
+          const int num_context_blocks =
+              DIVIDE_ROUND_UP(context_len, BLOCK_SIZE);
+          const int num_blocks_per_partition = num_context_blocks;
+
+          const int start_block_idx = 0;
+          const int end_block_idx =
+              MIN(start_block_idx + num_context_blocks, num_context_blocks);
+
+          const int num_blocks = end_block_idx - start_block_idx;
+          const int start_token_idx = start_block_idx * BLOCK_SIZE;
+          const int end_token_idx =
+              MIN(start_token_idx + num_blocks * BLOCK_SIZE, context_len);
+          const int num_tokens = end_token_idx - start_token_idx;
+          constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+          constexpr int NUM_THREAD_GROUPS =
+              NUM_THREADS /
+              THREAD_GROUP_SIZE;  // Note: This assumes THREAD_GROUP_SIZE
+          constexpr int NUM_TOKENS_PER_THREAD_GROUP =
+              DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
+          constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+          const int thread_idx = item_ct1.get_local_id(2);
+          const int warp_idx = thread_idx / WARP_SIZE;
+          const int lane = thread_idx % WARP_SIZE;
+          const int head_idx = item_ct1.get_group(1);
+          const int num_heads = item_ct1.get_group_range(1);
+          const int kv_head_idx = head_idx / num_queries_per_kv;
+          // TODO: consider alibi_slope later
+          constexpr int NUM_ELEMS_PER_THREAD = HD / THREAD_GROUP_SIZE;
+          constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
+          const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;
+          const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;
+          const sycl_t* q_ptr =
+              query_ptr + (query_loc_ptr[bsz_idx] + seq_idx) * query_stride_bs +
+              head_idx * HD;
+
+#pragma unroll
+          for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;
+               i += NUM_THREAD_GROUPS) {
+            const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
+            q_vecs[thread_group_offset * NUM_VECS_PER_THREAD + i] =
+                *reinterpret_cast<const Q_Vec*>(q_ptr + vec_idx * VEC_SIZE);
+          }
+          // Loaded q_vecs
+          item_ct1.barrier(sycl::access::fence_space::local_space);
+          auto shared_mem = (char*)dpct_local;
+          float* logits = reinterpret_cast<float*>(shared_mem);
+          constexpr int x = 16 / sizeof(sycl_t);
+          float qk_max = -FLT_MAX;
+          const int* block_table =
+              block_tables_ptr + bsz_idx * block_table_stride_batch;
+
+          // Loading key
+          for (int block_idx = start_block_idx + warp_idx;
+               block_idx < end_block_idx; block_idx += NUM_WARPS) {
+            const int64_t physical_block_number =
+                static_cast<int64_t>(block_table[block_idx]);
+            for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+              const int physical_block_offset =
+                  (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
+              const int token_idx =
+                  block_idx * BLOCK_SIZE + physical_block_offset;
+
+              Q_Vec k_vecs[NUM_VECS_PER_THREAD];
+
+#pragma unroll
+              for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+                const sycl_t* k_ptr =
+                    key_cache_ptr +
+                    physical_block_number * k_cache_stride_tokens +
+                    kv_head_idx * k_cache_stride_head +
+                    physical_block_offset * x;
+
+                const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
+                const int offset1 = (vec_idx * VEC_SIZE) / x;
+                const int offset2 = (vec_idx * VEC_SIZE) % x;
+                k_vecs[j] = *reinterpret_cast<const Q_Vec*>(
+                    k_ptr + offset1 * BLOCK_SIZE * x + offset2);
+              }
+
+              // Compute dot product.
+              // This includes a reduction across the threads in the
+              // same thread group. Q_Vec_t
+              // q_vec_[NUM_VECS_PER_THREAD] = q_vecs +
+              // thread_group_offset * THREAD_GROUP_SIZE;
+              float qk = scale *
+                         Qk_dot<sycl_t, THREAD_GROUP_SIZE>::template dot<
+                             Q_Vec, NUM_VECS_PER_THREAD>(
+                             q_vecs + thread_group_offset * NUM_VECS_PER_THREAD,
+                             k_vecs, item_ct1);
+
+              if (thread_group_offset == 0) {
+                // Store the partial reductions to shared memory.
+                // NOTE(woosuk): It is required to zero out the
+                // masked logits.
+                const bool mask = token_idx > context_len;
+                logits[token_idx - start_token_idx] = mask ? 0.f : qk;
+                qk_max = mask ? qk_max : sycl::fmax(qk_max, qk);
+              }
+            }
+          }
+#pragma unroll
+          for (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
+            /*
+            DPCT1096:38: The right-most dimension of the work-group used
+            in the SYCL kernel that calls this function may be less than
+            "32". The function "dpct::permute_sub_group_by_xor" may
+            return an unexpected result on the CPU device. Modify the
+            size of the work-group to ensure that the value of the
+            right-most dimension is a multiple of "32".
+            */
+            qk_max =
+                sycl::fmax(qk_max, dpct::permute_sub_group_by_xor(
+                                       item_ct1.get_sub_group(), qk_max, mask));
+          }
+          if (lane == 0) {
+            red_smem[warp_idx] = qk_max;
+          }
+          item_ct1.barrier(sycl::access::fence_space::local_space);
+          // TODO(woosuk): Refactor this part.
+          // Get the max qk value for the sequence.
+          qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+#pragma unroll
+          for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+            /*
+            DPCT1096:39: The right-most dimension of the work-group used
+            in the SYCL kernel that calls this function may be less than
+            "32". The function "dpct::permute_sub_group_by_xor" may
+            return an unexpected result on the CPU device. Modify the
+            size of the work-group to ensure that the value of the
+            right-most dimension is a multiple of "32".
+            */
+            qk_max =
+                sycl::fmax(qk_max, dpct::permute_sub_group_by_xor(
+                                       item_ct1.get_sub_group(), qk_max, mask));
+          }
+          qk_max =
+              dpct::select_from_sub_group(item_ct1.get_sub_group(), qk_max, 0);
+
+          // Get the sum of the exp values.
+          float exp_sum = 0.f;
+          for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+            float val = sycl::exp(logits[i] - qk_max);
+            logits[i] = val;
+            exp_sum += val;
+          }
+          exp_sum =
+              block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum, item_ct1);
+          // Compute softmax.
+          const float inv_sum = 1.f / (exp_sum + 1e-6f);
+#pragma unroll
+          for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+            logits[i] *= inv_sum;
+          }
+
+          item_ct1.barrier(sycl::access::fence_space::local_space);
+          constexpr int V_VEC_SIZE = MIN(16 / sizeof(sycl_t), BLOCK_SIZE);
+          using V_vec = typename Vec<sycl_t, V_VEC_SIZE>::Type;
+          using L_vec = typename Vec<sycl_t, V_VEC_SIZE>::Type;
+          using Float_L_vec = typename FloatVec<L_vec>::Type;
+          constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
+          constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
+          constexpr int NUM_ROWS_PER_THREAD =
+              DIVIDE_ROUND_UP(HD, NUM_ROWS_PER_ITER);
+          // NOTE(woosuk): We use FP32 for the accumulator for better
+          // accuracy.
+          float accs[NUM_ROWS_PER_THREAD];
+#pragma unroll
+          for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+            accs[i] = 0.f;
+          }
+
+          sycl_t zero_value;
+          zero(zero_value);
+          for (int block_idx = start_block_idx + warp_idx;
+               block_idx < end_block_idx; block_idx += NUM_WARPS) {
+            // NOTE(woosuk): The block number is stored in int32.
+            // However, we cast it to int64 because int32 can lead to
+            // overflow when this variable is multiplied by large
+            // numbers (e.g., kv_block_stride).
+            const int64_t physical_block_number =
+                static_cast<int64_t>(block_table[block_idx]);
+            const int physical_block_offset =
+                (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;
+            const int token_idx =
+                block_idx * BLOCK_SIZE + physical_block_offset;
+            L_vec logits_vec;
+            vllm::from_float(
+                logits_vec, *reinterpret_cast<Float_L_vec*>(logits + token_idx -
+                                                            start_token_idx));
+
+            const sycl_t* v_ptr =
+                value_cache_ptr +
+                physical_block_number * v_cache_stride_tokens +
+                kv_head_idx * v_cache_stride_head;
+#pragma unroll
+            for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+              const int row_idx =
+                  lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+              if (row_idx < HD) {
+                const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
+                V_vec v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);
+                if (block_idx == num_context_blocks - 1) {
+                  // NOTE(woosuk): When v_vec contains the tokens
+                  // that are out of the context, we should
+                  // explicitly zero out the values since they may
+                  // contain NaNs. See
+                  // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472
+                  sycl_t* v_vec_ptr = reinterpret_cast<sycl_t*>(&v_vec);
+#pragma unroll
+                  for (int j = 0; j < V_VEC_SIZE; j++) {
+                    v_vec_ptr[j] =
+                        token_idx + j < context_len ? v_vec_ptr[j] : zero_value;
+                  }
+                }
+                accs[i] += vllm::dot(logits_vec, v_vec);
+              }
+            }
+          }
+      // Perform reduction within each warp.
+#pragma unroll
+          for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+            float acc = accs[i];
+#pragma unroll
+            for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {
+              /*
+              DPCT1096:41: The right-most dimension of the work-group
+              used in the SYCL kernel that calls this function may be
+              less than "32". The function
+              "dpct::permute_sub_group_by_xor" may return an
+              unexpected result on the CPU device. Modify the size of
+              the work-group to ensure that the value of the
+              right-most dimension is a multiple of "32".
+              */
+              acc += dpct::permute_sub_group_by_xor(item_ct1.get_sub_group(),
+                                                    acc, mask);
+            }
+            accs[i] = acc;
+          }
+
+          // NOTE(woosuk): A barrier is required because the shared memory
+          // space for logits is reused for the output.
+
+          item_ct1.barrier(sycl::access::fence_space::local_space);
+
+          // Perform reduction across warps.
+          float* out_smem = reinterpret_cast<float*>(shared_mem);
+#pragma unroll
+          for (int i = NUM_WARPS; i > 1; i /= 2) {
+            int mid = i / 2;
+            // Upper warps write to shared memory.
+            if (warp_idx >= mid && warp_idx < i) {
+              float* dst = &out_smem[(warp_idx - mid) * HD];
+#pragma unroll
+              for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+                const int row_idx =
+                    lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+                if (row_idx < HD && lane % NUM_V_VECS_PER_ROW == 0) {
+                  dst[row_idx] = accs[i];
+                }
+              }
+            }
+
+            item_ct1.barrier(sycl::access::fence_space::local_space);
+
+            // Lower warps update the output.
+            if (warp_idx < mid) {
+              const float* src = &out_smem[warp_idx * HD];
+#pragma unroll
+              for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+                const int row_idx =
+                    lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+                if (row_idx < HD && lane % NUM_V_VECS_PER_ROW == 0) {
+                  accs[i] += src[row_idx];
+                }
+              }
+            }
+
+            item_ct1.barrier(sycl::access::fence_space::local_space);
+          }
+
+          // Write the final output.
+          if (warp_idx == 0) {
+            sycl_t* out_ptr =
+                out_p + (query_loc_ptr[bsz_idx] + seq_idx) * out_stride_tokens +
+                head_idx * out_stride_head;
+
+#pragma unroll
+            for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+              const int row_idx =
+                  lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+              if (row_idx < HD && lane % NUM_V_VECS_PER_ROW == 0) {
+                vllm::from_float(*(out_ptr + row_idx), accs[i]);
+              }
+            }
+          }
+        });
+    // Each thread_group handles one token
+  };
+  queue.submit(cgf);
+}
+
+template <
+    typename scalar_t,
+    typename Q_Vec_t,
+    int HEAD_SIZE,
+    int BLOCK_SIZE,
+    int NUM_THREADS,
+    int VEC_SIZE,
+    int PARTITION_SIZE = 0> // Zero means no partitioning.
+void paged_attention_kernel(
+    float* __restrict__ exp_sums, // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits, // [num_seqs, num_heads, max_num_partitions]
+    scalar_t* __restrict__ out, // [num_seqs, num_heads, max_num_partitions,
+                                // head_size]
+    const scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ k_cache, // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const scalar_t* __restrict__ v_cache, // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads, // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables, // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ context_lens, // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes, // [num_heads]
+    const int q_stride,
+    const int kv_block_stride,
+    const int kv_head_stride,
+    const float attn_logit_softcapping,
+    const sycl::nd_item<3>& item_ct1,
+    uint8_t* dpct_local,
+    Q_Vec_t* q_vecs,
+    float* red_smem) {
+  const int seq_idx = item_ct1.get_group(1);
+  const int partition_idx = item_ct1.get_group(0);
+  const int max_num_partitions = item_ct1.get_group_range(0);
+  constexpr bool USE_PARTITIONING = PARTITION_SIZE > 0;
+  const int context_len = context_lens[seq_idx];
+  if (USE_PARTITIONING && partition_idx * PARTITION_SIZE >= context_len) {
+    // No work to do. Terminate the thread block.
+    return;
+  }
+
+  const int num_context_blocks = DIVIDE_ROUND_UP(context_len, BLOCK_SIZE);
+  const int num_blocks_per_partition =
+      USE_PARTITIONING ? PARTITION_SIZE / BLOCK_SIZE : num_context_blocks;
+
+  // [start_block_idx, end_block_idx) is the range of blocks to process.
+  const int start_block_idx =
+      USE_PARTITIONING ? partition_idx * num_blocks_per_partition : 0;
+  const int end_block_idx =
+      MIN(start_block_idx + num_blocks_per_partition, num_context_blocks);
+  const int num_blocks = end_block_idx - start_block_idx;
+
+  // [start_token_idx, end_token_idx) is the range of tokens to process.
+  const int start_token_idx = start_block_idx * BLOCK_SIZE;
+  const int end_token_idx =
+      MIN(start_token_idx + num_blocks * BLOCK_SIZE, context_len);
+  const int num_tokens = end_token_idx - start_token_idx;
+
+  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  constexpr int NUM_THREAD_GROUPS =
+      NUM_THREADS / THREAD_GROUP_SIZE; // Note: This assumes THREAD_GROUP_SIZE
+                                       // divides NUM_THREADS
+  assert(NUM_THREADS % THREAD_GROUP_SIZE == 0);
+  constexpr int NUM_TOKENS_PER_THREAD_GROUP =
+      DIVIDE_ROUND_UP(BLOCK_SIZE, WARP_SIZE);
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  const int thread_idx = item_ct1.get_local_id(2);
+  const int warp_idx = thread_idx / WARP_SIZE;
+  const int lane = thread_idx % WARP_SIZE;
+
+  const int head_idx = item_ct1.get_group(2);
+  const int num_heads = item_ct1.get_group_range(2);
+  const int num_queries_per_kv = num_heads / num_kv_heads;
+
+  const int kv_head_idx = head_idx / num_queries_per_kv;
+  ;
+  const float alibi_slope =
+      alibi_slopes == nullptr ? 0.f : alibi_slopes[head_idx];
+
+  // A vector type to store a part of a key or a query.
+  // The vector size is configured in such a way that the threads in a thread
+  // group fetch or compute 16 bytes at a time. For example, if the size of a
+  // thread group is 4 and the data type is half, then the vector size is 16 /
+  // (4 * sizeof(half)) == 2.
+
+  // constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(scalar_t)),
+  // 1);
+
+  constexpr int NUM_ELEMS_PER_THREAD = HEAD_SIZE / THREAD_GROUP_SIZE;
+  constexpr int NUM_VECS_PER_THREAD = NUM_ELEMS_PER_THREAD / VEC_SIZE;
+
+  const int thread_group_idx = thread_idx / THREAD_GROUP_SIZE;
+  const int thread_group_offset = thread_idx % THREAD_GROUP_SIZE;
+
+  // Load the query to registers.
+  // Each thread in a thread group has a different part of the query.
+  // For example, if the the thread group size is 4, then the first thread in
+  // the group has 0, 4, 8, ... th vectors of the query, and the second thread
+  // has 1, 5, 9, ... th vectors of the query, and so on. NOTE(woosuk): Because
+  // q is split from a qkv tensor, it may not be contiguous.
+  const scalar_t* q_ptr = q + seq_idx * q_stride + head_idx * HEAD_SIZE;
+
+#pragma unroll
+  for (int i = thread_group_idx; i < NUM_VECS_PER_THREAD;
+       i += NUM_THREAD_GROUPS) {
+    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;
+    q_vecs[thread_group_offset * NUM_VECS_PER_THREAD + i] =
+        *reinterpret_cast<const Q_Vec_t*>(q_ptr + vec_idx * VEC_SIZE);
+  }
+  /*
+  DPCT1065:5: Consider replacing sycl::nd_item::barrier() with
+  sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better
+  performance if there is no access to global memory.
+  */
+  item_ct1.barrier(sycl::access::fence_space::local_space); // TODO(naed90): possible speedup if this is replaced with
+                      // a memory wall right before we use q_vecs
+
+  // Memory planning.
+  auto shared_mem = (char*)dpct_local;
+  // NOTE(woosuk): We use FP32 for the softmax logits for better accuracy.
+  float* logits = reinterpret_cast<float*>(shared_mem);
+  // Workspace for reduction.
+
+  // x == THREAD_GROUP_SIZE * VEC_SIZE
+  // Each thread group fetches x elements from the key at a time.
+  constexpr int x = 16 / sizeof(scalar_t);
+  float qk_max = -FLT_MAX;
+
+  // Iterate over the key blocks.
+  // Each warp fetches a block of keys for each iteration.
+  // Each thread group in a warp fetches a key from the block, and computes
+  // dot product with the query.
+  const int* block_table = block_tables + seq_idx * max_num_blocks_per_seq;
+
+  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
+       block_idx += NUM_WARPS) {
+    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
+    // int64 because int32 can lead to overflow when this variable is multiplied
+    // by large numbers (e.g., kv_block_stride).
+    const int64_t physical_block_number =
+        static_cast<int64_t>(block_table[block_idx]);
+
+    // Load a key to registers.
+    // Each thread in a thread group has a different part of the key.
+    // For example, if the the thread group size is 4, then the first thread in
+    // the group has 0, 4, 8, ... th vectors of the key, and the second thread
+    // has 1, 5, 9, ... th vectors of the key, and so on.
+
+    for (int i = 0; i < NUM_TOKENS_PER_THREAD_GROUP; i++) {
+      const int physical_block_offset =
+          (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;
+      const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+
+      Q_Vec_t k_vecs[NUM_VECS_PER_THREAD];
+
+#pragma unroll
+      for (int j = 0; j < NUM_VECS_PER_THREAD; j++) {
+        const scalar_t* k_ptr = k_cache +
+            physical_block_number * kv_block_stride +
+            kv_head_idx * kv_head_stride + physical_block_offset * x;
+
+        const int vec_idx = thread_group_offset + j * THREAD_GROUP_SIZE;
+        const int offset1 = (vec_idx * VEC_SIZE) / x;
+        const int offset2 = (vec_idx * VEC_SIZE) % x;
+        k_vecs[j] = *reinterpret_cast<const Q_Vec_t*>(
+            k_ptr + offset1 * BLOCK_SIZE * x + offset2);
+      }
+
+      // Compute dot product.
+      // This includes a reduction across the threads in the same thread group.
+      // Q_Vec_t q_vec_[NUM_VECS_PER_THREAD] = q_vecs + thread_group_offset *
+      // THREAD_GROUP_SIZE;
+      float qk = scale *
+          Qk_dot<scalar_t, THREAD_GROUP_SIZE>::
+              template dot<Q_Vec_t, NUM_VECS_PER_THREAD>(
+                     q_vecs + thread_group_offset * NUM_VECS_PER_THREAD,
+                     k_vecs,
+                     item_ct1);
+      // Add the ALiBi bias if slopes are given.
+      qk +=
+          (alibi_slope != 0) ? alibi_slope * (token_idx - context_len + 1) : 0;
+
+      // Add the attn_logit_softcapp if given.
+      if (attn_logit_softcapping != 0.0) {
+          qk = attn_softcapping(qk, attn_logit_softcapping);
+      }
+      if (thread_group_offset == 0) {
+        // Store the partial reductions to shared memory.
+        // NOTE(woosuk): It is required to zero out the masked logits.
+        const bool mask = token_idx >= context_len;
+        logits[token_idx - start_token_idx] = mask ? 0.f : qk;
+        // Update the max value.
+        qk_max = mask ? qk_max : sycl::fmax(qk_max, qk);
+      }
+    }
+  }
+
+  // Perform reduction across the threads in the same warp to get the
+  // max qk value for each "warp" (not across the thread block yet).
+  // The 0-th thread of each thread group already has its max qk value.
+#pragma unroll
+  for (int mask = WARP_SIZE / 2; mask >= THREAD_GROUP_SIZE; mask /= 2) {
+  
+    /*
+    DPCT1096:38: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    qk_max = sycl::fmax(
+        qk_max,
+        dpct::permute_sub_group_by_xor(
+            item_ct1.get_sub_group(), qk_max, mask));
+  }
+  if (lane == 0) {
+    red_smem[warp_idx] = qk_max;
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // TODO(woosuk): Refactor this part.
+  // Get the max qk value for the sequence.
+  qk_max = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:39: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    qk_max = sycl::fmax(
+        qk_max,
+        dpct::permute_sub_group_by_xor(
+            item_ct1.get_sub_group(), qk_max, mask));
+  }
+  // Broadcast the max qk value to all threads.
+  
+  /*
+  DPCT1096:40: The right-most dimension of the work-group used in the SYCL
+  kernel that calls this function may be less than "32". The function
+  "dpct::select_from_sub_group" may return an unexpected result on the CPU
+  device. Modify the size of the work-group to ensure that the value of the
+  right-most dimension is a multiple of "32".
+  */
+  qk_max = dpct::select_from_sub_group(
+          item_ct1.get_sub_group(), qk_max, 0);
+
+  // Get the sum of the exp values.
+  float exp_sum = 0.f;
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    float val = sycl::exp(logits[i] - qk_max);
+    logits[i] = val;
+    exp_sum += val;
+  }
+  exp_sum = block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], exp_sum, item_ct1);
+
+  // Compute softmax.
+  const float inv_sum = 1.f / (exp_sum + 1e-6f);
+  for (int i = thread_idx; i < num_tokens; i += NUM_THREADS) {
+    logits[i] *= inv_sum;
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // If partitioning is enabled, store the max logit and exp_sum.
+  if (USE_PARTITIONING && thread_idx == 0) {
+    float* max_logits_ptr = max_logits +
+        seq_idx * num_heads * max_num_partitions +
+        head_idx * max_num_partitions + partition_idx;
+    *max_logits_ptr = qk_max;
+    float* exp_sums_ptr = exp_sums + seq_idx * num_heads * max_num_partitions +
+        head_idx * max_num_partitions + partition_idx;
+    *exp_sums_ptr = exp_sum;
+  }
+
+  // Each thread will fetch 16 bytes from the value cache at a time.
+  constexpr int V_VEC_SIZE = MIN(16 / sizeof(scalar_t), BLOCK_SIZE);
+  using V_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  using L_vec = typename Vec<scalar_t, V_VEC_SIZE>::Type;
+  using Float_L_vec = typename FloatVec<L_vec>::Type;
+
+  constexpr int NUM_V_VECS_PER_ROW = BLOCK_SIZE / V_VEC_SIZE;
+  constexpr int NUM_ROWS_PER_ITER = WARP_SIZE / NUM_V_VECS_PER_ROW;
+  constexpr int NUM_ROWS_PER_THREAD =
+      DIVIDE_ROUND_UP(HEAD_SIZE, NUM_ROWS_PER_ITER);
+
+  // NOTE(woosuk): We use FP32 for the accumulator for better accuracy.
+  float accs[NUM_ROWS_PER_THREAD];
+#pragma unroll
+  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+    accs[i] = 0.f;
+  }
+
+  scalar_t zero_value;
+  zero(zero_value);
+  for (int block_idx = start_block_idx + warp_idx; block_idx < end_block_idx;
+       block_idx += NUM_WARPS) {
+    // NOTE(woosuk): The block number is stored in int32. However, we cast it to
+    // int64 because int32 can lead to overflow when this variable is multiplied
+    // by large numbers (e.g., kv_block_stride).
+    const int64_t physical_block_number =
+        static_cast<int64_t>(block_table[block_idx]);
+    const int physical_block_offset = (lane % NUM_V_VECS_PER_ROW) * V_VEC_SIZE;
+    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;
+    L_vec logits_vec;
+    vllm::from_float(
+        logits_vec,
+        *reinterpret_cast<Float_L_vec*>(logits + token_idx - start_token_idx));
+
+    const scalar_t* v_ptr = v_cache + physical_block_number * kv_block_stride +
+        kv_head_idx * kv_head_stride;
+#pragma unroll
+    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+      if (row_idx < HEAD_SIZE) {
+        const int offset = row_idx * BLOCK_SIZE + physical_block_offset;
+        V_vec v_vec = *reinterpret_cast<const V_vec*>(v_ptr + offset);
+        if (block_idx == num_context_blocks - 1) {
+          // NOTE(woosuk): When v_vec contains the tokens that are out of the
+          // context, we should explicitly zero out the values since they may
+          // contain NaNs. See
+          // https://github.com/vllm-project/vllm/issues/641#issuecomment-1682544472
+          scalar_t* v_vec_ptr = reinterpret_cast<scalar_t*>(&v_vec);
+#pragma unroll
+          for (int j = 0; j < V_VEC_SIZE; j++) {
+            v_vec_ptr[j] =
+                token_idx + j < context_len ? v_vec_ptr[j] : zero_value;
+          }
+        }
+        accs[i] += vllm::dot(logits_vec, v_vec);
+      }
+    }
+  }
+
+  // Perform reduction within each warp.
+#pragma unroll
+  for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+    float acc = accs[i];
+#pragma unroll
+    for (int mask = NUM_V_VECS_PER_ROW / 2; mask >= 1; mask /= 2) {
+     
+      /*
+      DPCT1096:41: The right-most dimension of the work-group used in the SYCL
+      kernel that calls this function may be less than "32". The function
+      "dpct::permute_sub_group_by_xor" may return an unexpected result on the
+      CPU device. Modify the size of the work-group to ensure that the value of
+      the right-most dimension is a multiple of "32".
+      */
+      acc += dpct::permute_sub_group_by_xor(
+          item_ct1.get_sub_group(), acc, mask);
+    }
+    accs[i] = acc;
+  }
+
+  // NOTE(woosuk): A barrier is required because the shared memory space for
+  // logits is reused for the output.
+
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // Perform reduction across warps.
+  float* out_smem = reinterpret_cast<float*>(shared_mem);
+#pragma unroll
+  for (int i = NUM_WARPS; i > 1; i /= 2) {
+    int mid = i / 2;
+    // Upper warps write to shared memory.
+    if (warp_idx >= mid && warp_idx < i) {
+      float* dst = &out_smem[(warp_idx - mid) * HEAD_SIZE];
+#pragma unroll
+      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
+          dst[row_idx] = accs[i];
+        }
+      }
+    }
+    
+    item_ct1.barrier(sycl::access::fence_space::local_space);
+
+    // Lower warps update the output.
+    if (warp_idx < mid) {
+      const float* src = &out_smem[warp_idx * HEAD_SIZE];
+#pragma unroll
+      for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+        const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+        if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
+          accs[i] += src[row_idx];
+        }
+      }
+    }
+    
+    item_ct1.barrier(sycl::access::fence_space::local_space);
+  }
+
+  // Write the final output.
+  if (warp_idx == 0) {
+    scalar_t* out_ptr = out +
+        seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
+        head_idx * max_num_partitions * HEAD_SIZE + partition_idx * HEAD_SIZE;
+#pragma unroll
+    for (int i = 0; i < NUM_ROWS_PER_THREAD; i++) {
+      const int row_idx = lane / NUM_V_VECS_PER_ROW + i * NUM_ROWS_PER_ITER;
+      if (row_idx < HEAD_SIZE && lane % NUM_V_VECS_PER_ROW == 0) {
+        vllm::from_float(*(out_ptr + row_idx), accs[i]);
+      }
+    }
+  }
+}
+
+// Grid: (num_heads, num_seqs, 1).
+template <
+    typename scalar_t,
+    typename Q_Vec_t,
+    int HEAD_SIZE,
+    int BLOCK_SIZE,
+    int NUM_THREADS,
+    int VEC_SIZE>
+void paged_attention_v1_kernel(
+    scalar_t* __restrict__ out, // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ k_cache, // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const scalar_t* __restrict__ v_cache, // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads, // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables, // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ context_lens, // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes, // [num_heads]
+    const int q_stride,
+    const int kv_block_stride,
+    const int kv_head_stride,
+    const float attn_logit_softcapping,
+    const sycl::nd_item<3>& item_ct1,
+    uint8_t* dpct_local,
+    Q_Vec_t* q_vecs,
+    float* red_smem) {
+  paged_attention_kernel<
+      scalar_t,
+      Q_Vec_t,
+      HEAD_SIZE,
+      BLOCK_SIZE,
+      NUM_THREADS,
+      VEC_SIZE>(
+      /* exp_sums */ nullptr,
+      /* max_logits */ nullptr,
+      out,
+      q,
+      k_cache,
+      v_cache,
+      num_kv_heads,
+      scale,
+      block_tables,
+      context_lens,
+      max_num_blocks_per_seq,
+      alibi_slopes,
+      q_stride,
+      kv_block_stride,
+      kv_head_stride,
+      attn_logit_softcapping,
+      item_ct1,
+      dpct_local,
+      q_vecs,
+      red_smem);
+}
+
+#define LAUNCH_ATTENTION_KERNEL(T, HEAD_SIZE, BLOCK_SIZE)      \
+  paged_attention_xpu_v1_impl<T, HEAD_SIZE, BLOCK_SIZE>::call( \
+      out_ptr,                                                 \
+      query_ptr,                                               \
+      key_cache_ptr,                                           \
+      value_cache_ptr,                                         \
+      num_kv_heads,                                            \
+      scale,                                                   \
+      block_tables_ptr,                                        \
+      context_lens_ptr,                                        \
+      max_num_blocks_per_seq,                                  \
+      alibi_slopes_ptr,                                        \
+      q_stride,                                                \
+      kv_block_stride,                                         \
+      kv_head_stride,                                          \
+      num_seqs,                                                \
+      num_heads,                                               \
+      num_blocks);
+
+#define LAUNCH_PAGED_ATTENTION_V1(HEAD_SIZE)                                \
+  event = queue.submit([&](sycl::handler& cgh) {                            \
+    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(                    \
+        sycl::range<1>(shared_mem_size), cgh);                              \
+    sycl::local_accessor<Q_Vec, 1> q_vecs_acc_ct1(                          \
+        sycl::range<1>(THREAD_GROUP_SIZE * num_vecs_per_thread), cgh);      \
+    sycl::local_accessor<float, 1> red_smem_acc_ct1(                        \
+        sycl::range<1>(2 * NUM_WARPS), cgh);                                \
+                                                                            \
+    auto out_ptr_ct0 = out_ptr;                                             \
+    auto query_ptr_ct1 = query_ptr;                                         \
+    auto key_cache_ptr_ct2 = key_cache_ptr;                                 \
+    auto value_cache_ptr_ct3 = value_cache_ptr;                             \
+    auto scale_ct5 = scale;                                                 \
+    auto block_tables_ptr_ct6 = block_tables_ptr;                           \
+    auto context_lens_ptr_ct7 = context_lens_ptr;                           \
+    auto max_num_blocks_per_seq_ct8 = max_num_blocks_per_seq;               \
+    auto alibi_slopes_ptr_ct9 = alibi_slopes_ptr;                           \
+    auto q_stride_ct10 = q_stride;                                          \
+    auto kv_block_stride_ct11 = kv_block_stride;                            \
+    auto kv_head_stride_ct12 = kv_head_stride;                              \
+    auto attn_logit_softcapping_ct13 = attn_logit_softcapping;              \
+                                                                            \
+    cgh.parallel_for(                                                       \
+        sycl::nd_range<3>(grid * block, block),                             \
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] { \
+          paged_attention_v1_kernel<                                        \
+              sycl_t,                                                       \
+              Q_Vec,                                                        \
+              HEAD_SIZE,                                                    \
+              BLOCK_SIZE,                                                   \
+              NUM_THREADS,                                                  \
+              VEC_SIZE>(                                                    \
+              out_ptr_ct0,                                                  \
+              query_ptr_ct1,                                                \
+              key_cache_ptr_ct2,                                            \
+              value_cache_ptr_ct3,                                          \
+              num_kv_heads,                                                 \
+              scale_ct5,                                                    \
+              block_tables_ptr_ct6,                                         \
+              context_lens_ptr_ct7,                                         \
+              max_num_blocks_per_seq_ct8,                                   \
+              alibi_slopes_ptr_ct9,                                         \
+              q_stride_ct10,                                                \
+              kv_block_stride_ct11,                                         \
+              kv_head_stride_ct12,                                          \
+              attn_logit_softcapping_ct13,                                  \
+              item_ct1,                                                     \
+              dpct_local_acc_ct1.get_pointer(),                             \
+              q_vecs_acc_ct1.get_pointer(),                                 \
+              red_smem_acc_ct1.get_pointer());                              \
+        });                                                                 \
+  });
+
+template <typename T, int BLOCK_SIZE, int NUM_THREADS = 512>
+void paged_attention_xpu_v1_impl_launcher(
+    torch::Tensor& out,
+    torch::Tensor& query,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    int num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int max_context_len,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const float attn_logit_softcapping) {
+  int num_seqs = query.size(0);
+  int num_heads = query.size(1);
+  int head_size = query.size(2);
+  int max_num_blocks_per_seq = block_tables.size(1);
+  int q_stride = query.stride(0);
+  int kv_block_stride = key_cache.stride(0);
+  int kv_head_stride = key_cache.stride(1);
+
+  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(T)), 1);
+  using sycl_t = vllm::xpu::SyclTypeTrait<T>::Type;
+  using Q_Vec = typename Vec<sycl_t, VEC_SIZE>::Type;
+
+  int num_vecs_per_thread = head_size / THREAD_GROUP_SIZE / VEC_SIZE;
+  assert(head_size % THREAD_GROUP_SIZE == 0);
+
+  // NOTE: alibi_slopes is optional.
+  const float* alibi_slopes_ptr = alibi_slopes
+      ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
+      : nullptr;
+
+  sycl_t* out_ptr = reinterpret_cast<sycl_t*>(out.data_ptr());
+  sycl_t* query_ptr = reinterpret_cast<sycl_t*>(query.data_ptr());
+  sycl_t* key_cache_ptr = reinterpret_cast<sycl_t*>(key_cache.data_ptr());
+  sycl_t* value_cache_ptr = reinterpret_cast<sycl_t*>(value_cache.data_ptr());
+  int* block_tables_ptr = block_tables.data_ptr<int>();
+  int* context_lens_ptr = context_lens.data_ptr<int>();
+
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  int padded_max_context_len =
+      DIVIDE_ROUND_UP(max_context_len, BLOCK_SIZE) * BLOCK_SIZE;
+  
+  int logits_size = padded_max_context_len * sizeof(float);
+  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
+  // Python-side check in vllm.worker.worker._check_if_can_support_max_seq_len
+  // Keep that in sync with the logic here!
+  int shared_mem_size = std::max(logits_size, outputs_size);
+
+  sycl::range<3> grid(1, num_seqs, num_heads);
+  sycl::range<3> block(1, 1, NUM_THREADS);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+  sycl::event event;
+
+  switch (head_size) {
+    // NOTE(woosuk): To reduce the compilation time, we only compile for the
+    // head sizes that we use in the model. However, we can easily extend this
+    // to support any head size which is a multiple of 16.
+    case 64:
+      LAUNCH_PAGED_ATTENTION_V1(64);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 80:
+      LAUNCH_PAGED_ATTENTION_V1(80);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 96:
+      LAUNCH_PAGED_ATTENTION_V1(96);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 112:
+      LAUNCH_PAGED_ATTENTION_V1(112);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 128:
+      LAUNCH_PAGED_ATTENTION_V1(128);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    case 256:
+      LAUNCH_PAGED_ATTENTION_V1(256);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v1", event);
+#endif
+      break;
+    default:
+      TORCH_CHECK(false, "Unsupported head size: ", head_size);
+      break;
+  }
+  // queue.wait();
+}
+
+#define CALL_KERNEL_LAUNCHER(T, BLOCK_SIZE)                  \
+  vllm::paged_attention_xpu_v1_impl_launcher<T, BLOCK_SIZE>( \
+      out,                                                   \
+      query,                                                 \
+      key_cache,                                             \
+      value_cache,                                           \
+      num_kv_heads,                                          \
+      scale,                                                 \
+      block_tables,                                          \
+      context_lens,                                          \
+      max_context_len,                                       \
+      alibi_slopes,                                          \
+      attn_logit_softcapping);
+
+#define CALL_KERNEL_LAUNCHER_BLOCK_SIZE(T)                        \
+  switch (block_size) {                                           \
+    case 8:                                                      \
+      CALL_KERNEL_LAUNCHER(T, 8);                                \
+      break;                                                      \
+    case 16:                                                      \
+      CALL_KERNEL_LAUNCHER(T, 16);                                \
+      break;                                                      \
+    case 32:                                                      \
+      CALL_KERNEL_LAUNCHER(T, 32);                                \
+      break;                                                      \
+    case 64:                                                      \
+      CALL_KERNEL_LAUNCHER(T, 64);                                \
+      break;                                                      \
+    default:                                                      \
+      TORCH_CHECK(false, "Unsupported block size: ", block_size); \
+      break;                                                      \
+  }
+
+// Grid: (num_heads, num_seqs).
+template <
+    typename scalar_t,
+    int HEAD_SIZE,
+    int NUM_THREADS,
+    int PARTITION_SIZE>
+void paged_attention_v2_reduce_kernel(
+    scalar_t* __restrict__ out, // [num_seqs, num_heads, head_size]
+    const float* __restrict__ exp_sums, // [num_seqs, num_heads,
+                                        // max_num_partitions]
+    const float* __restrict__ max_logits, // [num_seqs, num_heads,
+                                          // max_num_partitions]
+    const scalar_t* __restrict__ tmp_out, // [num_seqs, num_heads,
+                                          // max_num_partitions, head_size]
+    const int* __restrict__ context_lens, // [num_seqs]
+    const int max_num_partitions,
+    const sycl::nd_item<3>& item_ct1,
+    uint8_t* dpct_local,
+    float* red_smem) {
+  const int num_heads = item_ct1.get_group_range(2);
+  const int head_idx = item_ct1.get_group(2);
+  const int seq_idx = item_ct1.get_group(1);
+  const int context_len = context_lens[seq_idx];
+  const int num_partitions = DIVIDE_ROUND_UP(context_len, PARTITION_SIZE);
+  if (num_partitions == 1) {
+    // No need to reduce. Only copy tmp_out to out.
+    scalar_t* out_ptr =
+        out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
+    const scalar_t* tmp_out_ptr = tmp_out +
+        seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
+        head_idx * max_num_partitions * HEAD_SIZE;
+    for (int i = item_ct1.get_local_id(2); i < HEAD_SIZE;
+         i += item_ct1.get_local_range(2)) {
+      out_ptr[i] = tmp_out_ptr[i];
+    }
+    // Terminate the thread block.
+    return;
+  }
+
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  const int warp_idx = item_ct1.get_local_id(2) / WARP_SIZE;
+  const int lane = item_ct1.get_local_id(2) % WARP_SIZE;
+
+  // Size: 2 * num_partitions.
+  auto shared_mem = (char*)dpct_local;
+  // Workspace for reduction.
+
+  // Load max logits to shared memory.
+  float* shared_max_logits = reinterpret_cast<float*>(shared_mem);
+  const float* max_logits_ptr = max_logits +
+      seq_idx * num_heads * max_num_partitions + head_idx * max_num_partitions;
+  float max_logit = -FLT_MAX;
+  for (int i = item_ct1.get_local_id(2); i < num_partitions;
+       i += item_ct1.get_local_range(2)) {
+    const float l = max_logits_ptr[i];
+    shared_max_logits[i] = l;
+    max_logit = sycl::fmax(max_logit, (float)l);
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  // Get the global max logit.
+  // Reduce within the warp.
+#pragma unroll
+  for (int mask = WARP_SIZE / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:45: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    max_logit = sycl::fmax(
+        max_logit,
+        dpct::permute_sub_group_by_xor(
+            item_ct1.get_sub_group(), max_logit, mask));
+  }
+  if (lane == 0) {
+    red_smem[warp_idx] = max_logit;
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+  // Reduce across warps.
+  max_logit = lane < NUM_WARPS ? red_smem[lane] : -FLT_MAX;
+#pragma unroll
+  for (int mask = NUM_WARPS / 2; mask >= 1; mask /= 2) {
+    
+    /*
+    DPCT1096:46: The right-most dimension of the work-group used in the SYCL
+    kernel that calls this function may be less than "32". The function
+    "dpct::permute_sub_group_by_xor" may return an unexpected result on the CPU
+    device. Modify the size of the work-group to ensure that the value of the
+    right-most dimension is a multiple of "32".
+    */
+    max_logit = sycl::fmax(
+        max_logit,
+        dpct::permute_sub_group_by_xor(
+            item_ct1.get_sub_group(), max_logit, mask));
+  }
+  // Broadcast the max value to all threads.
+  
+  /*
+  DPCT1096:47: The right-most dimension of the work-group used in the SYCL
+  kernel that calls this function may be less than "32". The function
+  "dpct::select_from_sub_group" may return an unexpected result on the CPU
+  device. Modify the size of the work-group to ensure that the value of the
+  right-most dimension is a multiple of "32".
+  */
+  max_logit = dpct::select_from_sub_group(
+      item_ct1.get_sub_group(), max_logit, 0);
+
+  // Load rescaled exp sums to shared memory.
+  float* shared_exp_sums =
+      reinterpret_cast<float*>(shared_mem + sizeof(float) * num_partitions);
+  const float* exp_sums_ptr = exp_sums +
+      seq_idx * num_heads * max_num_partitions + head_idx * max_num_partitions;
+  float global_exp_sum = 0.0f;
+  for (int i = item_ct1.get_local_id(2); i < num_partitions;
+       i += item_ct1.get_local_range(2)) {
+    float l = shared_max_logits[i];
+    float rescaled_exp_sum = exp_sums_ptr[i] * sycl::exp(l - max_logit);
+    global_exp_sum += rescaled_exp_sum;
+    shared_exp_sums[i] = rescaled_exp_sum;
+  }
+  
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+  global_exp_sum =
+      block_sum<NUM_WARPS>(&red_smem[NUM_WARPS], global_exp_sum, item_ct1);
+  const float inv_global_exp_sum = 1.0f / (global_exp_sum + 1e-6f);
+
+  // Aggregate tmp_out to out.
+  const scalar_t* tmp_out_ptr = tmp_out +
+      seq_idx * num_heads * max_num_partitions * HEAD_SIZE +
+      head_idx * max_num_partitions * HEAD_SIZE;
+  scalar_t* out_ptr =
+      out + seq_idx * num_heads * HEAD_SIZE + head_idx * HEAD_SIZE;
+#pragma unroll
+  for (int i = item_ct1.get_local_id(2); i < HEAD_SIZE; i += NUM_THREADS) {
+    float acc = 0.0f;
+    for (int j = 0; j < num_partitions; ++j) {
+      acc += to_float(tmp_out_ptr[j * HEAD_SIZE + i]) * shared_exp_sums[j] *
+          inv_global_exp_sum;
+    }
+    from_float(out_ptr[i], acc);
+  }
+}
+
+// Grid: (num_heads, num_seqs, max_num_partitions).
+template <
+    typename scalar_t,
+    typename Q_Vec_t,
+    int HEAD_SIZE,
+    int BLOCK_SIZE,
+    int NUM_THREADS,
+    int VEC_SIZE,
+    int PARTITION_SIZE>
+void paged_attention_v2_kernel(
+    float* __restrict__ exp_sums, // [num_seqs, num_heads, max_num_partitions]
+    float* __restrict__ max_logits, // [num_seqs, num_heads, max_num_partitions]
+    scalar_t* __restrict__ tmp_out, // [num_seqs, num_heads, max_num_partitions,
+                                    // head_size]
+    const scalar_t* __restrict__ q, // [num_seqs, num_heads, head_size]
+    const scalar_t* __restrict__ k_cache, // [num_blocks, num_kv_heads,
+                                          // head_size/x, block_size, x]
+    const scalar_t* __restrict__ v_cache, // [num_blocks, num_kv_heads,
+                                          // head_size, block_size]
+    const int num_kv_heads, // [num_heads]
+    const float scale,
+    const int* __restrict__ block_tables, // [num_seqs, max_num_blocks_per_seq]
+    const int* __restrict__ context_lens, // [num_seqs]
+    const int max_num_blocks_per_seq,
+    const float* __restrict__ alibi_slopes, // [num_heads]
+    const int q_stride,
+    const int kv_block_stride,
+    const int kv_head_stride,
+    const float attn_logit_softcapping,
+    const sycl::nd_item<3>& item_ct1,
+    uint8_t* dpct_local,
+    Q_Vec_t* q_vecs,
+    float* red_smem) {
+  paged_attention_kernel<
+      scalar_t,
+      Q_Vec_t,
+      HEAD_SIZE,
+      BLOCK_SIZE,
+      NUM_THREADS,
+      VEC_SIZE,
+      PARTITION_SIZE>(
+      exp_sums,
+      max_logits,
+      tmp_out,
+      q,
+      k_cache,
+      v_cache,
+      num_kv_heads,
+      scale,
+      block_tables,
+      context_lens,
+      max_num_blocks_per_seq,
+      alibi_slopes,
+      q_stride,
+      kv_block_stride,
+      kv_head_stride,
+      attn_logit_softcapping,
+      item_ct1,
+      dpct_local,
+      q_vecs,
+      red_smem);
+}
+
+#define LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(HEAD_SIZE)                     \
+  event = queue.submit([&](sycl::handler& cgh) {                            \
+    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(                    \
+        sycl::range<1>(shared_mem_size), cgh);                              \
+    sycl::local_accessor<Q_Vec, 1> q_vecs_acc_ct1(                          \
+        sycl::range<1>(THREAD_GROUP_SIZE * num_vecs_per_thread), cgh);      \
+    sycl::local_accessor<float, 1> red_smem_acc_ct1(                        \
+        sycl::range<1>(2 * NUM_WARPS), cgh);                                \
+                                                                            \
+    auto exp_sums_ptr_ct0 = exp_sums_ptr;                                   \
+    auto max_logits_ptr_ct1 = max_logits_ptr;                               \
+    auto tmp_out_ptr_ct2 = tmp_out_ptr;                                     \
+    auto query_ptr_ct3 = query_ptr;                                         \
+    auto key_cache_ptr_ct4 = key_cache_ptr;                                 \
+    auto value_cache_ptr_ct5 = value_cache_ptr;                             \
+    auto scale_ct7 = scale;                                                 \
+    auto block_tables_ptr_ct8 = block_tables_ptr;                           \
+    auto context_lens_ptr_ct9 = context_lens_ptr;                           \
+    auto max_num_blocks_per_seq_ct10 = max_num_blocks_per_seq;              \
+    auto alibi_slopes_ptr_ct11 = alibi_slopes_ptr;                          \
+    auto q_stride_ct12 = q_stride;                                          \
+    auto kv_block_stride_ct13 = kv_block_stride;                            \
+    auto kv_head_stride_ct14 = kv_head_stride;                              \
+    auto attn_logit_softcapping_ct15 = attn_logit_softcapping;              \
+                                                                            \
+    cgh.parallel_for(                                                       \
+        sycl::nd_range<3>(grid * block, block),                             \
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] { \
+          vllm::paged_attention_v2_kernel<                                  \
+              sycl_t,                                                       \
+              Q_Vec,                                                        \
+              HEAD_SIZE,                                                    \
+              BLOCK_SIZE,                                                   \
+              NUM_THREADS,                                                  \
+              VEC_SIZE,                                                     \
+              PARTITION_SIZE>(                                              \
+              exp_sums_ptr_ct0,                                             \
+              max_logits_ptr_ct1,                                           \
+              tmp_out_ptr_ct2,                                              \
+              query_ptr_ct3,                                                \
+              key_cache_ptr_ct4,                                            \
+              value_cache_ptr_ct5,                                          \
+              num_kv_heads,                                                 \
+              scale_ct7,                                                    \
+              block_tables_ptr_ct8,                                         \
+              context_lens_ptr_ct9,                                         \
+              max_num_blocks_per_seq_ct10,                                  \
+              alibi_slopes_ptr_ct11,                                        \
+              q_stride_ct12,                                                \
+              kv_block_stride_ct13,                                         \
+              kv_head_stride_ct14,                                          \
+              attn_logit_softcapping_ct15,                                  \
+              item_ct1,                                                     \
+              dpct_local_acc_ct1.get_pointer(),                             \
+              q_vecs_acc_ct1.get_pointer(),                                 \
+              red_smem_acc_ct1.get_pointer());                              \
+        });                                                                 \
+  });
+
+#define LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(HEAD_SIZE)                    \
+  event2 = queue.submit([&](sycl::handler& cgh) {                           \
+    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(                    \
+        sycl::range<1>(reduce_shared_mem_size), cgh);                       \
+    sycl::local_accessor<float, 1> red_smem_acc_ct1(                        \
+        sycl::range<1>(2 * NUM_WARPS), cgh);                                \
+                                                                            \
+    auto out_ptr_ct0 = out_ptr;                                             \
+    auto exp_sums_ptr_ct1 = exp_sums_ptr;                                   \
+    auto max_logits_ptr_ct2 = max_logits_ptr;                               \
+    auto tmp_out_ptr_ct3 = tmp_out_ptr;                                     \
+    auto context_lens_ptr_ct4 = context_lens_ptr;                           \
+    auto max_num_partitions_ct5 = max_num_partitions;                       \
+                                                                            \
+    cgh.parallel_for(                                                       \
+        sycl::nd_range<3>(reduce_grid * block, block),                      \
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] { \
+          vllm::paged_attention_v2_reduce_kernel<                           \
+              sycl_t,                                                       \
+              HEAD_SIZE,                                                    \
+              NUM_THREADS,                                                  \
+              PARTITION_SIZE>(                                              \
+              out_ptr_ct0,                                                  \
+              exp_sums_ptr_ct1,                                             \
+              max_logits_ptr_ct2,                                           \
+              tmp_out_ptr_ct3,                                              \
+              context_lens_ptr_ct4,                                         \
+              max_num_partitions_ct5,                                       \
+              item_ct1,                                                     \
+              dpct_local_acc_ct1.get_pointer(),                             \
+              red_smem_acc_ct1.get_pointer());                              \
+        });                                                                 \
+  });
+
+template <
+    typename T,
+    int BLOCK_SIZE,
+    int NUM_THREADS = 512,
+    int PARTITION_SIZE = 512>
+void paged_attention_v2_launcher(
+    torch::Tensor& out,
+    torch::Tensor& exp_sums,
+    torch::Tensor& max_logits,
+    torch::Tensor& tmp_out,
+    torch::Tensor& query,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    int num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int max_context_len,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const float attn_logit_softcapping) {
+  int num_seqs = query.size(0);
+  int num_heads = query.size(1);
+  int head_size = query.size(2);
+  int max_num_blocks_per_seq = block_tables.size(1);
+  int q_stride = query.stride(0);
+  int kv_block_stride = key_cache.stride(0);
+  int kv_head_stride = key_cache.stride(1);
+
+  constexpr int THREAD_GROUP_SIZE = MAX(WARP_SIZE / BLOCK_SIZE, 1);
+  assert(head_size % THREAD_GROUP_SIZE == 0);
+  constexpr int VEC_SIZE = MAX(16 / (THREAD_GROUP_SIZE * sizeof(T)), 1);
+  using sycl_t = vllm::xpu::SyclTypeTrait<T>::Type;
+  using Q_Vec = typename Vec<sycl_t, VEC_SIZE>::Type;
+
+  int num_vecs_per_thread = head_size / THREAD_GROUP_SIZE / VEC_SIZE;
+  assert(head_size % THREAD_GROUP_SIZE == 0);
+
+  // NOTE: alibi_slopes is optional.
+  const float* alibi_slopes_ptr = alibi_slopes
+      ? reinterpret_cast<const float*>(alibi_slopes.value().data_ptr())
+      : nullptr;
+
+  sycl_t* out_ptr = reinterpret_cast<sycl_t*>(out.data_ptr());
+  float* exp_sums_ptr = reinterpret_cast<float*>(exp_sums.data_ptr());
+  float* max_logits_ptr = reinterpret_cast<float*>(max_logits.data_ptr());
+  sycl_t* tmp_out_ptr = reinterpret_cast<sycl_t*>(tmp_out.data_ptr());
+  sycl_t* query_ptr = reinterpret_cast<sycl_t*>(query.data_ptr());
+  sycl_t* key_cache_ptr = reinterpret_cast<sycl_t*>(key_cache.data_ptr());
+  sycl_t* value_cache_ptr = reinterpret_cast<sycl_t*>(value_cache.data_ptr());
+  int* block_tables_ptr = block_tables.data_ptr<int>();
+  int* context_lens_ptr = context_lens.data_ptr<int>();
+
+  constexpr int NUM_WARPS = NUM_THREADS / WARP_SIZE;
+  int max_num_partitions = DIVIDE_ROUND_UP(max_context_len, PARTITION_SIZE);
+  
+  int logits_size = PARTITION_SIZE * sizeof(float);
+  int outputs_size = (NUM_WARPS / 2) * head_size * sizeof(float);
+
+  // For paged attention v2 kernel.
+  sycl::range<3> grid(max_num_partitions, num_seqs, num_heads);
+  int shared_mem_size = std::max(logits_size, outputs_size);
+  // For paged attention v2 reduce kernel.
+  sycl::range<3> reduce_grid(1, num_seqs, num_heads);
+  
+  int reduce_shared_mem_size = 2 * max_num_partitions * sizeof(float);
+
+  sycl::range<3> block(1, 1, NUM_THREADS);
+  sycl::queue& queue = vllm::xpu::vllmGetQueue();
+  sycl::event event;
+  sycl::event event2;
+  switch (head_size) {
+    // NOTE(woosuk): To reduce the compilation time, we only compile for the
+    // head sizes that we use in the model. However, we can easily extend this
+    // to support any head size which is a multiple of 16.
+    case 64:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(64);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(64);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 80:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(80);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(80);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 96:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(96);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(96);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 112:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(112);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(112);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 128:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(128);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(128);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    case 256:
+      LAUNCH_PAGED_ATTENTION_V2_FIRST_HALF(256);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event);
+#endif
+      LAUNCH_PAGED_ATTENTION_V2_SECOND_HALF(256);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+    // xpu::profiler_record(event_desc, event);  // Uncomment when needed
+#else
+    ::xpu::profiler_record("paged attn v2", event2);
+#endif
+      break;
+    default:
+      TORCH_CHECK(false, "Unsupported head size: ", head_size);
+      break;
+  }
+}
+
+#define CALL_V2_LAUNCHER(T, BLOCK_SIZE)             \
+  vllm::paged_attention_v2_launcher<T, BLOCK_SIZE>( \
+      out,                                          \
+      exp_sums,                                     \
+      max_logits,                                   \
+      tmp_out,                                      \
+      query,                                        \
+      key_cache,                                    \
+      value_cache,                                  \
+      num_kv_heads,                                 \
+      scale,                                        \
+      block_tables,                                 \
+      context_lens,                                 \
+      max_context_len,                              \
+      alibi_slopes,                                 \
+      attn_logit_softcapping);
+
+#define CALL_V2_LAUNCHER_BLOCK_SIZE(T)                            \
+  switch (block_size) {                                           \
+    case 8:                                                       \
+      CALL_V2_LAUNCHER(T, 8);                                     \
+      break;                                                      \
+    case 16:                                                      \
+      CALL_V2_LAUNCHER(T, 16);                                    \
+      break;                                                      \
+    case 32:                                                      \
+      CALL_V2_LAUNCHER(T, 32);                                    \
+      break;                                                      \
+    case 64:                                                      \
+      CALL_V2_LAUNCHER(T, 64);                                    \
+      break;                                                      \
+    default:                                                      \
+      TORCH_CHECK(false, "Unsupported block size: ", block_size); \
+      break;                                                      \
+  }
+
+} // namespace vllm
+
+void paged_attention_v1(
+    torch::Tensor& out,
+    torch::Tensor& query,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    int num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int block_size,
+    int max_context_len,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const std::string& kv_cache_dtype,
+    const float kv_scale,
+    const float attn_logit_softcapping) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES_FLOAT_ONLY(
+      query.scalar_type(), "paged_attention_xpu_v1_impl", [&] {
+        CALL_KERNEL_LAUNCHER_BLOCK_SIZE(scalar_t);
+      });
+}
+
+void paged_attention_v2(
+    torch::Tensor& out,
+    torch::Tensor& exp_sums,
+    torch::Tensor& max_logits,
+    torch::Tensor& tmp_out,
+    torch::Tensor& query,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    int num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int block_size,
+    int max_context_len,
+    const c10::optional<torch::Tensor>& alibi_slopes,
+    const std::string& kv_cache_dtype,
+    const float kv_scale,
+    const float attn_logit_softcapping) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES_FLOAT_ONLY(
+      query.scalar_type(), "paged_attention_xpu_v2_impl", [&] {
+        CALL_V2_LAUNCHER_BLOCK_SIZE(scalar_t);
+      });
+}
+
+torch::Tensor context_attention_forward_v2(
+    torch::Tensor query,  // [num_tokens, num_kv_head, head_dim]
+    torch::Tensor key,    // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor value,  // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor block_tables, torch::Tensor query_start_loc,
+    torch::Tensor seq_lens, torch::Tensor context_lens, int max_input_length,
+    int max_context_length, int max_q_length) {
+  // Currently, only support fp16 here
+  int64_t num_tokens = query.size(0);
+  int64_t num_heads = query.size(1);
+  int64_t head_dim = query.size(2);
+  int64_t batch_size = seq_lens.size(0);
+  int num_kv_heads = value.size(1);
+
+  int key_dimension = key.dim();
+  auto output = at::empty({query.size(0), query.size(1), query.size(2)},
+                          at::device(query.device()).dtype(query.dtype()));
+
+  assert(key_dimension == 5);
+  assert(query.scalar_type() == key.scalar_type() &&
+         query.scalar_type() == value.scalar_type());
+  assert(head_dim == 128);
+  assert(query.scalar_type() == at::ScalarType::Half);
+
+  int query_stride_token = query.stride(0);
+  int query_stride_head = query.stride(1);
+  int query_stride_dim = query.stride(2);
+  const float attn_scale = 1 / std::sqrt((float)head_dim);
+
+  assert(num_heads % num_kv_heads == 0);
+  int num_queries_per_kv = num_heads / num_kv_heads;
+
+
+  // key: num_blocks, num_kv_heads, head_size // x, num_blocks, x)
+  // value: [num_blocks, num_kv_heads, head_size, block_dim]
+  int block_size = value.size(3);
+  // Currently, only block_size 16 is supported...
+  assert(block_size == 16);
+  int x = key.size(4);
+  int block_table_stride_bsz = block_tables.stride(0);
+  int block_table_stride_seq = block_tables.stride(1);
+  int k_cache_stride_token = key.stride(0);
+  int k_cache_stride_head = key.stride(1);
+  int k_cache_stride_head_dim = key.stride(2);
+  int k_cache_stride_block = key.stride(3);
+  int k_cache_stride_x = key.stride(4);
+
+  int v_cache_stride_token = value.stride(0);
+  int v_cache_stride_head = value.stride(1);
+  int v_cache_stride_head_dim = value.stride(2);
+  int v_cache_stride_block = value.stride(3);
+  switch(head_dim) {
+    case 128:
+      vllm::context_attention_kernel_v2<sycl::half, 32, 128>(
+        query.data_ptr(), key.data_ptr(), value.data_ptr(),
+        block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+        seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+        output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+        query_stride_token, query_stride_head, query_stride_dim,
+        k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+        k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+        v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+        output.stride(0), output.stride(1), num_queries_per_kv,
+        max_input_length, batch_size, num_heads, query.size(0),
+        max_context_length, max_q_length);
+      break;
+    case 64:
+      vllm::context_attention_kernel_v2<sycl::half, 32, 64>(
+        query.data_ptr(), key.data_ptr(), value.data_ptr(),
+        block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+        seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+        output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+        query_stride_token, query_stride_head, query_stride_dim,
+        k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+        k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+        v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+        output.stride(0), output.stride(1), num_queries_per_kv,
+        max_input_length, batch_size, num_heads, query.size(0),
+        max_context_length, max_q_length);
+      break;
+    case 80:
+      vllm::context_attention_kernel_v2<sycl::half, 32, 80>(
+        query.data_ptr(), key.data_ptr(), value.data_ptr(),
+        block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+        seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+        output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+        query_stride_token, query_stride_head, query_stride_dim,
+        k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+        k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+        v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+        output.stride(0), output.stride(1), num_queries_per_kv,
+        max_input_length, batch_size, num_heads, query.size(0),
+        max_context_length, max_q_length);
+      break;
+    case 96:
+      vllm::context_attention_kernel_v2<sycl::half, 32, 96>(
+        query.data_ptr(), key.data_ptr(), value.data_ptr(),
+        block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+        seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+        output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+        query_stride_token, query_stride_head, query_stride_dim,
+        k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+        k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+        v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+        output.stride(0), output.stride(1), num_queries_per_kv,
+        max_input_length, batch_size, num_heads, query.size(0),
+        max_context_length, max_q_length);
+      break;
+    default: throw std::runtime_error("unsupported head_dim");
+  }
+    return output;
+}
+
+torch::Tensor context_attention_forward_v1(
+    torch::Tensor query,  // [num_tokens, num_kv_head, head_dim]
+    torch::Tensor key,    // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor value,  // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor block_tables, torch::Tensor query_start_loc,
+    torch::Tensor seq_lens, torch::Tensor context_lens, int max_input_length,
+    int max_context_length) {
+  // Currently, only support fp16
+  int64_t num_tokens = query.size(0);
+  int64_t num_heads = query.size(1);
+  int64_t head_dim = query.size(2);
+  int64_t batch_size = seq_lens.size(0);
+  int num_kv_heads = value.size(1);
+
+  int key_dimension = key.dim();
+  auto output = at::empty({query.size(0), query.size(1), query.size(2)},
+                          at::device(query.device()).dtype(query.dtype()));
+
+  // key should be in shape:
+  // 1. [num_blocks, num_heads, block_size, head_dim]
+  // 2. [num_blocks, num_heads, head_dim / x, block_size, x]
+  assert(key_dimension == 4 or key_dimension == 5);
+  assert(query.scalar_type() == key.scalar_type() &&
+         query.scalar_type() == value.scalar_type());
+  assert(query.scalar_type() == at::ScalarType::Half);
+
+  int query_stride_token = query.stride(0);
+  int query_stride_head = query.stride(1);
+  int query_stride_dim = query.stride(2);
+  const float attn_scale = 1 / std::sqrt((float)head_dim);
+
+  assert(num_heads % num_kv_heads == 0);
+  int num_queries_per_kv = num_heads / num_kv_heads;
+  int block_table_stride_bsz = block_tables.stride(0);
+  int block_table_stride_seq = block_tables.stride(1);
+  if (key_dimension == 4) {
+    // key/value: num_blocks, num_kv_heads, num_blocks, head_dim)
+    int block_size = value.size(2);
+    int k_cache_stride_0 = key.stride(0);
+    int k_cache_stride_1 = key.stride(1);
+    int k_cache_stride_2 = key.stride(2);
+    int k_cache_stride_3 = key.stride(3);
+
+    int v_cache_stride_0 = value.stride(0);
+    int v_cache_stride_1 = value.stride(1);
+    int v_cache_stride_2 = value.stride(2);
+    int v_cache_stride_3 = value.stride(3);
+    switch (head_dim) {
+      case 128:
+        vllm::context_attention_kernel_v1_reshaped<sycl::half, 32, 128>(
+            query.data_ptr(), key.data_ptr(), value.data_ptr(),
+            block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+            seq_lens.data_ptr(), context_lens.data_ptr(), block_size,
+            output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+            query_stride_token, query_stride_head, query_stride_dim,
+            k_cache_stride_0, k_cache_stride_1, k_cache_stride_2,
+            k_cache_stride_3, v_cache_stride_0, v_cache_stride_1,
+            v_cache_stride_2, v_cache_stride_3, output.stride(0),
+            output.stride(1), num_queries_per_kv, max_input_length, batch_size,
+            num_heads);
+        break;
+      case 64:
+        vllm::context_attention_kernel_v1_reshaped<sycl::half, 32, 64>(
+            query.data_ptr(), key.data_ptr(), value.data_ptr(),
+            block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+            seq_lens.data_ptr(), context_lens.data_ptr(), block_size,
+            output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+            query_stride_token, query_stride_head, query_stride_dim,
+            k_cache_stride_0, k_cache_stride_1, k_cache_stride_2,
+            k_cache_stride_3, v_cache_stride_0, v_cache_stride_1,
+            v_cache_stride_2, v_cache_stride_3, output.stride(0),
+            output.stride(1), num_queries_per_kv, max_input_length, batch_size,
+            num_heads);
+        break;
+      default:
+        throw std::runtime_error("unsupported head_dim");
+    }
+  } else {
+    int x = key.size(4);
+    int block_size = value.size(3);
+    int k_cache_stride_token = key.stride(0);
+    int k_cache_stride_head = key.stride(1);
+    int k_cache_stride_head_dim = key.stride(2);
+    int k_cache_stride_block = key.stride(3);
+    int k_cache_stride_x = key.stride(4);
+
+    int v_cache_stride_token = value.stride(0);
+    int v_cache_stride_head = value.stride(1);
+    int v_cache_stride_head_dim = value.stride(2);
+    int v_cache_stride_block = value.stride(3);
+    switch (head_dim) {
+      case 128:
+        vllm::context_attention_kernel_v1<sycl::half, 32, 128>(
+            query.data_ptr(), key.data_ptr(), value.data_ptr(),
+            block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+            seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+            output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+            query_stride_token, query_stride_head, query_stride_dim,
+            k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+            k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+            v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+            output.stride(0), output.stride(1), num_queries_per_kv,
+            max_input_length, batch_size, num_heads);
+        break;
+      case 64:
+        vllm::context_attention_kernel_v1<sycl::half, 32, 64>(
+            query.data_ptr(), key.data_ptr(), value.data_ptr(),
+            block_tables.data_ptr(), attn_scale, query_start_loc.data_ptr(),
+            seq_lens.data_ptr(), context_lens.data_ptr(), block_size, x,
+            output.data_ptr(), block_table_stride_bsz, block_table_stride_seq,
+            query_stride_token, query_stride_head, query_stride_dim,
+            k_cache_stride_token, k_cache_stride_head, k_cache_stride_head_dim,
+            k_cache_stride_block, k_cache_stride_x, v_cache_stride_token,
+            v_cache_stride_head, v_cache_stride_head_dim, v_cache_stride_block,
+            output.stride(0), output.stride(1), num_queries_per_kv,
+            max_input_length, batch_size, num_heads);
+        break;
+      default:
+        throw std::runtime_error("unsupported head_dim");
+    }
+  }
+  return output;
+}
+
+template<typename IT, const int VS, const int HD>
+void gqa_1_kernel(
+    const void * query, // [num_seqs, num_heads, head_size]
+    const void * key,   // [num_blocks, num_kv_heads, head_size, block_size]
+    const void * value, // [num_blocks, num_kv_heads, head_size, block_size]
+    const void* block_tables, // [num_seqs, max_num_blocks_per_seq]
+    const void* context_lens, // [num_seqs]
+    void * o_a_s,
+    void * o_accs,
+    const int64_t query_bsz_stride,
+    const int64_t query_head_stride,
+    const int64_t kv_token_stride,
+    const int64_t kv_head_stride,
+    const int64_t kv_block_stride,
+    const int64_t block_table_stride_batch,
+    const int64_t o_a_s_bsz_stride,
+    const int64_t o_a_s_head_stride,
+    const int64_t o_accs_bsz_stride,
+    const int64_t o_accs_head_stride,
+    const float scale,
+    const int block_size,
+    const int bsz,
+    const int num_heads,
+    const int num_kv_heads,
+    const int block_num,
+    const at::Device & device
+) {
+    const int group_size = num_heads / num_kv_heads;
+    const int sub_rows = VS / group_size;
+    const int rem_rows = VS % group_size;
+
+    const float attn_scale = scale;
+
+    sycl::range<3> global_size(bsz, num_heads, block_num);
+    sycl::range<3> local_size(1, group_size, 1);
+
+    auto cgf = [&](sycl::handler& handle) {
+        handle.parallel_for(
+            sycl::nd_range<3>(global_size, local_size),
+            [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+                slm_init<VS * HD * sizeof(IT)>();
+
+                const int bsz_idx = item.get_global_id(0);
+                const int head_idx = item.get_global_id(1);
+                const int kv_head_idx = item.get_group(1);
+                const int tid = item.get_local_id(1);
+                const int vid = item.get_global_id(2);
+
+                const IT * query_head = (const IT *)query + bsz_idx * query_bsz_stride
+                                                          + head_idx * query_head_stride;
+                
+                IT * o_accs_head = (IT *)o_accs + bsz_idx * o_accs_bsz_stride
+                                                + head_idx * o_accs_head_stride;
+                float * o_a_s_head = (float *)o_a_s + bsz_idx * o_a_s_bsz_stride
+                                                    + head_idx * o_a_s_head_stride;
+
+                const int* block_tables_ptr = (const int*)block_tables;
+                const int* block_table =
+                    block_tables_ptr + bsz_idx * block_table_stride_batch;
+
+                const int* context_lens_ptr = (const int*)context_lens;
+                const int context_length = context_lens_ptr[bsz_idx];
+
+                simd<IT, HD> query_row = block_load<IT, HD>(query_head) * attn_scale;
+
+                // copy k_cache to slm
+                int start_row = std::min(vid * VS + tid * sub_rows + std::min(tid, rem_rows), context_length);
+                int end_row = std::min(start_row + sub_rows + (tid < rem_rows), context_length);
+                for (int r = start_row; r < end_row; ++r) {
+                    int which_block = r / block_size;
+                    int which_slot = r % block_size;
+                    int physical_block_number = block_table[which_block];
+
+                    const IT * key_head = (const IT *)key + physical_block_number * kv_token_stride +
+                      kv_head_idx * kv_head_stride +
+                      which_slot * kv_block_stride;
+
+                    simd<IT, HD> key_row = block_load<IT, HD>(key_head);
+                    slm_block_store<IT, HD>((r - vid * VS) * HD * sizeof(IT), key_row);
+                }
+                barrier();
+
+                simd<float, VS> attns = -sycl::detail::max_v<float>();
+                int row_num = (vid + 1) * VS > context_length ? context_length % VS : VS;
+                // q @ k
+                for (int r = 0; r < row_num; ++r) {
+                    simd<IT, HD> key_row = slm_block_load<IT, HD>(r * HD * sizeof(IT));
+                    float attn = sycl::ext::intel::esimd::detail::sum<float, IT, HD>(query_row * key_row);
+                    attns[r] = attn;
+                }
+
+                float max_attn = hmax<float, float, VS>(attns);
+                const simd<IT, VS> attn_exp = exp(attns - max_attn);
+                barrier();
+
+                // copy v_cache to slm
+                for (int r = start_row; r < end_row; ++r) {
+                    int which_block = r / block_size;
+                    int which_slot = r % block_size;
+                    int physical_block_number = block_table[which_block];
+
+                    const IT * value_head = (const IT *)value + physical_block_number * kv_token_stride +
+                      kv_head_idx * kv_head_stride +
+                      which_slot * kv_block_stride;
+
+                    simd<IT, HD> value_row = block_load<IT, HD>(value_head);
+                    slm_block_store<IT, HD>((r - vid * VS) * HD * sizeof(IT), value_row);
+                }
+                barrier();
+
+                // attn @ v
+                simd<IT, HD> accs = 0;
+                for (int r = 0; r < row_num; ++r) {
+                    simd<IT, HD> value_row = slm_block_load<IT, HD>(r * HD * sizeof(IT));
+                    accs = accs + value_row * attn_exp[r];
+                }
+
+                float softmax = sycl::ext::intel::esimd::detail::sum<float, float, VS>(attn_exp);
+
+                block_store<IT, HD>(o_accs_head + vid * HD, accs);
+                block_store<float, 1>(o_a_s_head + vid * 2, max_attn);
+                block_store<float, 1>(o_a_s_head + vid * 2 + 1, softmax);
+            }
+        );
+    };
+
+    utils::submit_kernel(cgf, device, "gqa kernel 1/2");
+}
+
+template<typename IT, const int GS, const int HD>
+void gqa_2_kernel(
+    void * o_a_s,
+    void * o_accs,
+    void * output,
+    const void* context_lens, // [num_seqs]
+    const int64_t o_a_s_bsz_stride,
+    const int64_t o_a_s_head_stride,
+    const int64_t o_accs_bsz_stride,
+    const int64_t o_accs_head_stride,
+    const int64_t output_bsz_stride,
+    const int64_t output_head_stride,
+    const int bsz,
+    const int num_heads,
+    const int row_block_num,
+    const at::Device & device
+) {
+    constexpr int SUB_HD = 8;
+    static_assert(HD % SUB_HD == 0);
+    static_assert(HD / SUB_HD <= GS);
+
+    const int sub_rows = row_block_num / GS;
+    const int rem_rows = row_block_num % GS;
+
+    constexpr int accs_slm_offset = 0;
+    constexpr int attn_slm_offset = GS * HD * sizeof(float);
+    constexpr int softmax_slm_offset = attn_slm_offset + GS * sizeof(float);
+
+    sycl::range<3> global_size(bsz, num_heads, GS);
+    sycl::range<3> local_size(1, 1, GS);
+
+    auto cgf = [&](sycl::handler& handle) {
+        handle.parallel_for(
+            sycl::nd_range<3>(global_size, local_size),
+            [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+                slm_init<GS * HD * sizeof(float) + GS * 2 * sizeof(float)>();
+
+                const int bsz_idx = item.get_global_id(0);
+                const int head_idx = item.get_global_id(1);
+                const int tid = item.get_global_id(2);
+
+                const int* context_lens_ptr = (const int*)context_lens;
+                const int context_length = context_lens_ptr[bsz_idx];
+                constexpr int VS = 32;
+                const int cur_row_block_num = (context_length + VS - 1) / VS;
+                const int cur_sub_rows = cur_row_block_num / GS;
+                const int cur_rem_rows = cur_row_block_num % GS;
+
+                const float * o_a_s_head = (const float *)o_a_s + bsz_idx * o_a_s_bsz_stride
+                                                                + head_idx * o_a_s_head_stride;
+                const IT * o_accs_head = (const IT *)o_accs + bsz_idx * o_accs_bsz_stride
+                                                            + head_idx * o_accs_head_stride;
+                IT * output_head = (IT *)output + bsz_idx * output_bsz_stride
+                                                + head_idx * output_head_stride;
+
+                int start_row = std::min(tid * cur_sub_rows + std::min(tid, cur_rem_rows), cur_row_block_num);
+                int end_row = std::min(start_row + cur_sub_rows + (tid < cur_rem_rows), cur_row_block_num);
+
+                float max_attn = -sycl::detail::max_v<float>();
+                float softmax = 0;
+                simd<float, HD> accs = 0;
+                for (int r = start_row; r < end_row; ++r) {
+                    float sub_attn = o_a_s_head[2 * r];
+                    float sub_softmax = o_a_s_head[2 * r + 1];
+                    simd<float, HD> sub_accs = block_load<IT, HD>(o_accs_head + r * HD);
+                    float new_max_attn = std::max(max_attn, sub_attn);
+                    float exp1 = exp(max_attn - new_max_attn);
+                    float exp2 = exp(sub_attn - new_max_attn);
+                    accs = accs * exp1 + sub_accs * exp2;
+                    softmax = softmax * exp1 + sub_softmax * exp2;
+                    max_attn = new_max_attn;
+                }
+
+                slm_block_store<float, HD>(accs_slm_offset + tid * HD * sizeof(float), accs);
+                slm_block_store<float, 1>(attn_slm_offset + tid * sizeof(float), max_attn);
+                slm_block_store<float, 1>(softmax_slm_offset + tid * sizeof(float), softmax);
+                barrier();
+
+                if (tid < HD / SUB_HD) {
+                    simd<float, GS> max_attns = slm_block_load<float, GS>(attn_slm_offset);
+                    const simd<float, GS> scales = exp(max_attns - hmax<float, float, GS>(max_attns));
+                    simd<float, GS> softmaxs = slm_block_load<float, GS>(softmax_slm_offset);
+                    float softmax_sum = sycl::ext::intel::esimd::detail::sum<float, float, GS>(softmaxs * scales);
+
+                    simd<float, SUB_HD> result = 0;
+                    #pragma unroll
+                    for (int r = 0; r < GS; ++r) {
+                        simd<float, SUB_HD> sub_accs = slm_block_load<float, SUB_HD>(
+                            accs_slm_offset + (r * HD + tid * SUB_HD) * sizeof(float)
+                        );
+                        result = result + sub_accs * scales[r];
+                    }
+                    result = result / softmax_sum;
+                    block_store<IT, SUB_HD>(output_head + tid * SUB_HD, result);
+                }
+            }
+        );
+    };
+
+    utils::submit_kernel(cgf, device, "gqa kernel 2/2");
+}
+
+using AT = at::ScalarType;
+using fp16 = sycl::half;
+template<const int VS, const int GS, const int HD>
+auto dispatch_gqa_kernel(AT it) {
+    switch (it) {
+        case AT::Float: return std::make_tuple(gqa_1_kernel<float, VS, HD>, gqa_2_kernel<float, GS, HD>);
+        case AT::Half: return std::make_tuple(gqa_1_kernel<fp16, VS, HD>, gqa_2_kernel<fp16, GS, HD>);
+        default: throw std::runtime_error("unsupported dtype, only fp32 and fp16 are supported");
+    }
+}
+
+void paged_attention_gqa(
+    torch::Tensor output,
+    torch::Tensor query,
+    torch::Tensor key_cache,
+    torch::Tensor value_cache,
+    int64_t bsz,
+    int64_t num_heads,
+    int64_t num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int block_size,
+    int64_t head_dim,
+    int max_seq_len
+) {
+    constexpr int VS = 32;
+    constexpr int GS = 32;
+
+    const int row_block_num = (max_seq_len + VS - 1) / VS;
+    auto o_a_s = torch::empty({bsz, num_heads, 1, row_block_num * 2},
+                              torch::device(query.device()).dtype(torch::kFloat32));
+    auto o_accs = torch::empty({bsz, num_heads, 1, row_block_num * head_dim},
+                               torch::device(query.device()).dtype(query.dtype()));
+
+    auto [func1, func2] = [&](){
+        switch (head_dim) {
+            case 128: return dispatch_gqa_kernel<VS, GS, 128>(query.scalar_type());
+            case 96: return dispatch_gqa_kernel<VS, GS, 96>(query.scalar_type());
+            case 80: return dispatch_gqa_kernel<VS, GS, 80>(query.scalar_type());
+            case 64: return dispatch_gqa_kernel<VS, GS, 64>(query.scalar_type());
+            default: throw std::runtime_error("unsupported head_dim, only 128, 96, 80 and 64 are supported");
+        }
+    }();
+
+    func1(
+        query.data_ptr(), key_cache.data_ptr(), value_cache.data_ptr(),
+        block_tables.data_ptr(), context_lens.data_ptr(), o_a_s.data_ptr(), o_accs.data_ptr(),
+        query.stride(0), query.stride(1), key_cache.stride(0), key_cache.stride(1), key_cache.stride(2), block_tables.stride(0),
+        o_a_s.stride(0), o_a_s.stride(1), o_accs.stride(0), o_accs.stride(1),
+        scale, block_size, bsz, num_heads, num_kv_heads, row_block_num,
+        query.device()
+    );
+
+    func2(
+        o_a_s.data_ptr(), o_accs.data_ptr(), output.data_ptr(), context_lens.data_ptr(),
+        o_a_s.stride(0), o_a_s.stride(1),
+        o_accs.stride(0), o_accs.stride(1),
+        output.stride(0), output.stride(1),
+        bsz, num_heads, row_block_num,
+        query.device()
+    );
+}
diff --git a/csrc/xpu/attention_xpu_fp8.cpp b/csrc/xpu/attention_xpu_fp8.cpp
new file mode 100644
index 000000000..a2ea5819b
--- /dev/null
+++ b/csrc/xpu/attention_xpu_fp8.cpp
@@ -0,0 +1,324 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <ext/intel/esimd.hpp>
+#include "kv.h"
+
+// clang-format on
+#include <float.h>
+#include <torch/extension.h>
+#include <stdexcept>
+#include "utils.h"
+#include "xpu_types.h"
+// #include "dtype_bfloat16.dp.hpp"
+#include "dtype_float16.h"
+#include "dtype_float32.h"
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+#include <c10/xpu/XPUStream.h>
+#endif
+
+#include <functional>
+// #include <ipex.h>
+
+using namespace sycl::ext::intel::esimd;
+using AT = at::ScalarType;
+
+template <typename IT, const int VS, const int HD>
+void gqa_1_kernel_fp8(
+    const void* query,  // [num_seqs, num_heads, head_size]
+    const void* key,    // [num_blocks, num_kv_heads, head_size, block_size]
+    const void* value,  // [num_blocks, num_kv_heads, head_size, block_size]
+    const void* block_tables,  // [num_seqs, max_num_blocks_per_seq]
+    const void* context_lens,  // [num_seqs]
+    void* o_a_s, void* o_accs, const int64_t query_bsz_stride,
+    const int64_t query_head_stride, const int64_t kv_token_stride,
+    const int64_t kv_head_stride, const int64_t kv_block_stride,
+    const int64_t block_table_stride_batch, const int64_t o_a_s_bsz_stride,
+    const int64_t o_a_s_head_stride, const int64_t o_accs_bsz_stride,
+    const int64_t o_accs_head_stride, const float scale, const int block_size,
+    const int bsz, const int num_heads, const int num_kv_heads,
+    const int block_num, const at::Device& device) {
+  const int group_size = num_heads / num_kv_heads;
+  const int sub_rows = VS / group_size;
+  const int rem_rows = VS % group_size;
+
+  const float attn_scale = scale;
+
+  sycl::range<3> global_size(bsz, num_heads, block_num);
+  sycl::range<3> local_size(1, group_size, 1);
+
+  auto cgf = [&](sycl::handler& handle) {
+    handle.parallel_for(
+        sycl::nd_range<3>(global_size, local_size),
+        [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+          slm_init<VS * HD * sizeof(IT)>();
+
+          const int bsz_idx = item.get_global_id(0);
+          const int head_idx = item.get_global_id(1);
+          const int kv_head_idx = item.get_group(1);
+          const int tid = item.get_local_id(1);
+          const int vid = item.get_global_id(2);
+
+          const IT* query_head = (const IT*)query + bsz_idx * query_bsz_stride +
+                                 head_idx * query_head_stride;
+
+          IT* o_accs_head = (IT*)o_accs + bsz_idx * o_accs_bsz_stride +
+                            head_idx * o_accs_head_stride;
+          float* o_a_s_head = (float*)o_a_s + bsz_idx * o_a_s_bsz_stride +
+                              head_idx * o_a_s_head_stride;
+
+          const int* block_tables_ptr = (const int*)block_tables;
+          const int* block_table =
+              block_tables_ptr + bsz_idx * block_table_stride_batch;
+
+          const int* context_lens_ptr = (const int*)context_lens;
+          const int context_length = context_lens_ptr[bsz_idx];
+
+          simd<IT, HD> query_row = block_load<IT, HD>(query_head) * attn_scale;
+
+          // copy k_cache to slm
+          int start_row =
+              std::min(vid * VS + tid * sub_rows + std::min(tid, rem_rows),
+                       context_length);
+          int end_row =
+              std::min(start_row + sub_rows + (tid < rem_rows), context_length);
+          for (int r = start_row; r < end_row; ++r) {
+            int which_block = r / block_size;
+            int which_slot = r % block_size;
+            int physical_block_number = block_table[which_block];
+
+            // Load elements in uint8_t
+            const uint8_t* key_head =
+                (const uint8_t*)key + physical_block_number * kv_token_stride +
+                kv_head_idx * kv_head_stride + which_slot * kv_block_stride;
+
+            simd<uint8_t, HD> key_row = block_load<uint8_t, HD>(key_head);
+            simd<IT, HD> key_dequantized = dequantize_key_row<HD>(key_row);
+            slm_block_store<IT, HD>((r - vid * VS) * HD * sizeof(IT), key_dequantized);
+          }
+          barrier();
+
+          simd<float, VS> attns = -sycl::detail::max_v<float>();
+          int row_num =
+              (vid + 1) * VS > context_length ? context_length % VS : VS;
+          // q @ k
+          for (int r = 0; r < row_num; ++r) {
+            simd<IT, HD> key_row = slm_block_load<IT, HD>(r * HD * sizeof(IT));
+            float attn = sycl::ext::intel::esimd::detail::sum<float, IT, HD>(
+                query_row * key_row);
+            attns[r] = attn;
+          }
+
+          float max_attn = hmax<float, float, VS>(attns);
+          const simd<IT, VS> attn_exp = exp(attns - max_attn);
+          barrier();
+
+          // copy v_cache to slm
+          for (int r = start_row; r < end_row; ++r) {
+            int which_block = r / block_size;
+            int which_slot = r % block_size;
+            int physical_block_number = block_table[which_block];
+
+            const uint8_t* value_head =
+                (const uint8_t*)value + physical_block_number * kv_token_stride +
+                kv_head_idx * kv_head_stride + which_slot * kv_block_stride;
+
+            simd<uint8_t, HD> value_row = block_load<uint8_t, HD>(value_head);
+            simd<IT, HD> value_dequantized = dequantize_value_row<HD>(value_row);
+            slm_block_store<IT, HD>((r - vid * VS) * HD * sizeof(IT),
+                                    value_dequantized);
+          }
+          barrier();
+
+          // attn @ v
+          simd<IT, HD> accs = 0;
+          for (int r = 0; r < row_num; ++r) {
+            simd<IT, HD> value_row =
+                slm_block_load<IT, HD>(r * HD * sizeof(IT));
+            accs = accs + value_row * attn_exp[r];
+          }
+
+          float softmax =
+              sycl::ext::intel::esimd::detail::sum<float, float, VS>(attn_exp);
+
+          block_store<IT, HD>(o_accs_head + vid * HD, accs);
+          block_store<float, 1>(o_a_s_head + vid * 2, max_attn);
+          block_store<float, 1>(o_a_s_head + vid * 2 + 1, softmax);
+        });
+  };
+
+  utils::submit_kernel(cgf, device, "gqa kernel 1/2");
+}
+
+template <typename IT, const int GS, const int HD>
+void gqa_2_kernel_fp8(void* o_a_s, void* o_accs, void* output,
+                  const void* context_lens,  // [num_seqs]
+                  const int64_t o_a_s_bsz_stride,
+                  const int64_t o_a_s_head_stride,
+                  const int64_t o_accs_bsz_stride,
+                  const int64_t o_accs_head_stride,
+                  const int64_t output_bsz_stride,
+                  const int64_t output_head_stride, const int bsz,
+                  const int num_heads, const int row_block_num,
+                  const at::Device& device) {
+  constexpr int SUB_HD = 8;
+  static_assert(HD % SUB_HD == 0);
+  static_assert(HD / SUB_HD <= GS);
+
+  const int sub_rows = row_block_num / GS;
+  const int rem_rows = row_block_num % GS;
+
+  constexpr int accs_slm_offset = 0;
+  constexpr int attn_slm_offset = GS * HD * sizeof(float);
+  constexpr int softmax_slm_offset = attn_slm_offset + GS * sizeof(float);
+
+  sycl::range<3> global_size(bsz, num_heads, GS);
+  sycl::range<3> local_size(1, 1, GS);
+
+  auto cgf = [&](sycl::handler& handle) {
+    handle.parallel_for(
+        sycl::nd_range<3>(global_size, local_size),
+        [=](sycl::nd_item<3> item) SYCL_ESIMD_KERNEL {
+          slm_init<GS * HD * sizeof(float) + GS * 2 * sizeof(float)>();
+
+          const int bsz_idx = item.get_global_id(0);
+          const int head_idx = item.get_global_id(1);
+          const int tid = item.get_global_id(2);
+
+          const int* context_lens_ptr = (const int*)context_lens;
+          const int context_length = context_lens_ptr[bsz_idx];
+          constexpr int VS = 32;
+          const int cur_row_block_num = (context_length + VS - 1) / VS;
+          const int cur_sub_rows = cur_row_block_num / GS;
+          const int cur_rem_rows = cur_row_block_num % GS;
+
+          const float* o_a_s_head = (const float*)o_a_s +
+                                    bsz_idx * o_a_s_bsz_stride +
+                                    head_idx * o_a_s_head_stride;
+          const IT* o_accs_head = (const IT*)o_accs +
+                                  bsz_idx * o_accs_bsz_stride +
+                                  head_idx * o_accs_head_stride;
+          IT* output_head = (IT*)output + bsz_idx * output_bsz_stride +
+                            head_idx * output_head_stride;
+
+          int start_row =
+              std::min(tid * cur_sub_rows + std::min(tid, cur_rem_rows),
+                       cur_row_block_num);
+          int end_row =
+              std::min(start_row + cur_sub_rows + (tid < cur_rem_rows),
+                       cur_row_block_num);
+
+          float max_attn = -sycl::detail::max_v<float>();
+          float softmax = 0;
+          simd<float, HD> accs = 0;
+          for (int r = start_row; r < end_row; ++r) {
+            float sub_attn = o_a_s_head[2 * r];
+            float sub_softmax = o_a_s_head[2 * r + 1];
+            simd<float, HD> sub_accs = block_load<IT, HD>(o_accs_head + r * HD);
+            float new_max_attn = std::max(max_attn, sub_attn);
+            float exp1 = exp(max_attn - new_max_attn);
+            float exp2 = exp(sub_attn - new_max_attn);
+            accs = accs * exp1 + sub_accs * exp2;
+            softmax = softmax * exp1 + sub_softmax * exp2;
+            max_attn = new_max_attn;
+          }
+
+          slm_block_store<float, HD>(accs_slm_offset + tid * HD * sizeof(float),
+                                     accs);
+          slm_block_store<float, 1>(attn_slm_offset + tid * sizeof(float),
+                                    max_attn);
+          slm_block_store<float, 1>(softmax_slm_offset + tid * sizeof(float),
+                                    softmax);
+          barrier();
+
+          if (tid < HD / SUB_HD) {
+            simd<float, GS> max_attns =
+                slm_block_load<float, GS>(attn_slm_offset);
+            const simd<float, GS> scales =
+                exp(max_attns - hmax<float, float, GS>(max_attns));
+            simd<float, GS> softmaxs =
+                slm_block_load<float, GS>(softmax_slm_offset);
+            float softmax_sum =
+                sycl::ext::intel::esimd::detail::sum<float, float, GS>(
+                    softmaxs * scales);
+
+            simd<float, SUB_HD> result = 0;
+#pragma unroll
+            for (int r = 0; r < GS; ++r) {
+              simd<float, SUB_HD> sub_accs = slm_block_load<float, SUB_HD>(
+                  accs_slm_offset + (r * HD + tid * SUB_HD) * sizeof(float));
+              result = result + sub_accs * scales[r];
+            }
+            result = result / softmax_sum;
+            block_store<IT, SUB_HD>(output_head + tid * SUB_HD, result);
+          }
+        });
+  };
+
+  utils::submit_kernel(cgf, device, "gqa kernel 2/2");
+}
+
+template <const int VS, const int GS, const int HD>
+auto dispatch_gqa_kernel_fp8(AT it) {
+  switch (it) {
+    case AT::Float:
+      return std::make_tuple(gqa_1_kernel_fp8<float, VS, HD>,
+                             gqa_2_kernel_fp8<float, GS, HD>);
+    case AT::Half:
+      return std::make_tuple(gqa_1_kernel_fp8<fp16, VS, HD>,
+                             gqa_2_kernel_fp8<fp16, GS, HD>);
+    default:
+      throw std::runtime_error(
+          "unsupported dtype, only fp32 and fp16 are supported");
+  }
+}
+
+void paged_attention_gqa_fp8(torch::Tensor output, torch::Tensor query,
+                         torch::Tensor key_cache, torch::Tensor value_cache,
+                         int64_t bsz, int64_t num_heads, int64_t num_kv_heads,
+                         float scale, torch::Tensor& block_tables,
+                         torch::Tensor& context_lens, int block_size,
+                         int64_t head_dim, int max_seq_len) {
+  constexpr int VS = 32;
+  constexpr int GS = 32;
+
+  const int row_block_num = (max_seq_len + VS - 1) / VS;
+  auto o_a_s =
+      torch::empty({bsz, num_heads, 1, row_block_num * 2},
+                   torch::device(query.device()).dtype(torch::kFloat32));
+  auto o_accs =
+      torch::empty({bsz, num_heads, 1, row_block_num * head_dim},
+                   torch::device(query.device()).dtype(query.dtype()));
+
+  auto [func1, func2] = [&]() {
+    switch (head_dim) {
+      case 128:
+        return dispatch_gqa_kernel_fp8<VS, GS, 128>(query.scalar_type());
+      case 96:
+        return dispatch_gqa_kernel_fp8<VS, GS, 96>(query.scalar_type());
+      case 80:
+        return dispatch_gqa_kernel_fp8<VS, GS, 80>(query.scalar_type());
+      case 64:
+        return dispatch_gqa_kernel_fp8<VS, GS, 64>(query.scalar_type());
+      default:
+        throw std::runtime_error(
+            "unsupported head_dim, only 128, 96, 80 and 64 are supported");
+    }
+  }();
+
+  func1(query.data_ptr(), key_cache.data_ptr(), value_cache.data_ptr(),
+        block_tables.data_ptr(), context_lens.data_ptr(), o_a_s.data_ptr(),
+        o_accs.data_ptr(), query.stride(0), query.stride(1),
+        key_cache.stride(0), key_cache.stride(1), key_cache.stride(2),
+        block_tables.stride(0), o_a_s.stride(0), o_a_s.stride(1),
+        o_accs.stride(0), o_accs.stride(1), scale, block_size, bsz, num_heads,
+        num_kv_heads, row_block_num, query.device());
+
+  func2(o_a_s.data_ptr(), o_accs.data_ptr(), output.data_ptr(),
+        context_lens.data_ptr(), o_a_s.stride(0), o_a_s.stride(1),
+        o_accs.stride(0), o_accs.stride(1), output.stride(0), output.stride(1),
+        bsz, num_heads, row_block_num, query.device());
+}
diff --git a/csrc/xpu/base.hpp b/csrc/xpu/base.hpp
new file mode 100644
index 000000000..c364c62e6
--- /dev/null
+++ b/csrc/xpu/base.hpp
@@ -0,0 +1,118 @@
+#pragma once
+
+#include <sycl.hpp>
+#include <sycl/ext/intel/esimd.hpp>
+
+#include "common.h"
+
+using namespace sycl::ext::intel::esimd;
+using fp16 = sycl::half;
+
+constexpr int QK = 64;
+constexpr int SBS = 4;
+
+constexpr int BLOCK_SIZES[GGML_TYPE_COUNT] = {
+    [GGML_TYPE_Q4_0]     = QK / 2,
+    [GGML_TYPE_Q4_0_WOQ] = QK / 2,
+    [GGML_TYPE_FP8E5]  = QK,
+};
+
+constexpr int SCALE_SIZES[GGML_TYPE_COUNT] = {
+    [GGML_TYPE_Q4_0]     = sizeof(fp16),
+    [GGML_TYPE_Q4_0_WOQ] = sizeof(fp16),
+    [GGML_TYPE_FP8E5]  = 0,
+};
+
+template<int QTYPE>
+ESIMD_INLINE auto load_qblocks(const uint8_t * weight, const uint8_t * scale);
+
+template<>
+ESIMD_INLINE auto load_qblocks<GGML_TYPE_Q4_0>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_Q4_0];
+    simd<uint8_t, BLOCK_SIZE * SBS> ybytes = block_load<uint8_t, BLOCK_SIZE * SBS>(weight);
+    const simd<fp16, SBS> scales = block_load<fp16, SBS>((const fp16 *)scale);
+
+    simd<fp16, QK * SBS> yvs;
+    #pragma unroll
+    for (int i = 0; i < SBS; ++i) {
+        simd<uint8_t, QK> uyv;
+        uyv.select<QK / 2, 1>(0) = ybytes.template select<QK / 2, 1>(i * QK / 2) & (uint8_t)0xF;
+        uyv.select<QK / 2, 1>(QK / 2) = ybytes.template select<QK / 2, 1>(i * QK / 2) >> (uint8_t)4;
+        yvs.template select<QK, 1>(i * QK) = (uyv.bit_cast_view<int8_t>() - (int8_t)8) * scales[i];
+    }
+    return yvs;
+}
+
+template<>
+ESIMD_INLINE auto load_qblocks<GGML_TYPE_Q4_0_WOQ>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_Q4_0_WOQ];
+    simd<uint8_t, BLOCK_SIZE * SBS> ybytes = block_load<uint8_t, BLOCK_SIZE * SBS>(weight);
+    const simd<fp16, SBS> scales = block_load<fp16, SBS>((const fp16 *)scale);
+
+    simd<fp16, QK * SBS> yvs;
+    #pragma unroll
+    for (int i = 0; i < SBS; ++i) {
+        simd<uint8_t, QK> uyv;
+        uyv.select<QK / 2, 2>(0) = ybytes.template select<QK / 2, 1>(i * QK / 2) & (uint8_t)0xF;
+        uyv.select<QK / 2, 2>(1) = ybytes.template select<QK / 2, 1>(i * QK / 2) >> (uint8_t)4;
+        yvs.template select<QK, 1>(i * QK) = (uyv.bit_cast_view<int8_t>() - (int8_t)8) * scales[i];
+    }
+    return yvs;
+}
+
+
+template<>
+ESIMD_INLINE auto load_qblocks<GGML_TYPE_FP8E5>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_FP8E5];
+    simd<uint8_t, BLOCK_SIZE * SBS> ybytes = block_load<uint8_t, BLOCK_SIZE * SBS>(weight);
+
+    simd<fp16, QK * SBS> yvs;
+    yvs.template bit_cast_view<uint8_t>().template select<QK * SBS, 2>(0) = 0x80;
+    yvs.template bit_cast_view<uint8_t>().template select<QK * SBS, 2>(1) = ybytes;
+    return yvs;
+}
+
+
+// C++ doesn't support function template partial specialization, so write a new version for SBS=1
+template<int QTYPE>
+ESIMD_INLINE auto load_qblock(const uint8_t * weight, const uint8_t * scale);
+
+template<>
+ESIMD_INLINE auto load_qblock<GGML_TYPE_Q4_0>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_Q4_0];
+    simd<uint8_t, BLOCK_SIZE> ybytes = block_load<uint8_t, BLOCK_SIZE>(weight);
+    fp16 scales = *(const fp16 *)scale;
+
+    simd<uint8_t, QK> uyv;
+    uyv.select<QK / 2, 1>(0) = ybytes & (uint8_t)0xF;
+    uyv.select<QK / 2, 1>(QK / 2) = ybytes >> (uint8_t)4;
+    simd<fp16, QK> yv = (uyv.bit_cast_view<int8_t>() - (int8_t)8) * scales;
+
+    return yv;
+}
+
+template<>
+ESIMD_INLINE auto load_qblock<GGML_TYPE_Q4_0_WOQ>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_Q4_0_WOQ];
+    simd<uint8_t, BLOCK_SIZE> ybytes = block_load<uint8_t, BLOCK_SIZE>(weight);
+    fp16 scales = *(const fp16 *)scale;
+
+    simd<uint8_t, QK> uyv;
+    uyv.select<QK / 2, 2>(0) = ybytes & (uint8_t)0xF;
+    uyv.select<QK / 2, 2>(1) = ybytes >> (uint8_t)4;
+    simd<fp16, QK> yv = (uyv.bit_cast_view<int8_t>() - (int8_t)8) * scales;
+
+    return yv;
+}
+
+
+template<>
+ESIMD_INLINE auto load_qblock<GGML_TYPE_FP8E5>(const uint8_t * weight, const uint8_t * scale) {
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[GGML_TYPE_FP8E5];
+    simd<uint8_t, BLOCK_SIZE> ybytes = block_load<uint8_t, BLOCK_SIZE>(weight);
+
+    simd<fp16, QK> yvs;
+    yvs.template bit_cast_view<uint8_t>().template select<QK, 2>(0) = 0x80;
+    yvs.template bit_cast_view<uint8_t>().template select<QK, 2>(1) = ybytes;
+    return yvs;
+}
diff --git a/csrc/xpu/cache_ops_xpu.cpp b/csrc/xpu/cache_ops_xpu.cpp
new file mode 100644
index 000000000..a3451c0e7
--- /dev/null
+++ b/csrc/xpu/cache_ops_xpu.cpp
@@ -0,0 +1,579 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <ext/intel/esimd.hpp>
+// clang-format on
+#include "xpu_types.h"
+
+#include <torch/extension.h>
+#include "utils.h"
+
+using fp16 = sycl::half;
+using namespace sycl::ext::intel::esimd;
+
+template <typename scalar_t>
+void reshape_and_cache_kernel(
+    const scalar_t* __restrict__ key, // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value, // [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key_cache, // [num_blocks, num_heads, head_size/x,
+                                      // block_size, x]
+    scalar_t* __restrict__ value_cache, // [num_blocks, num_heads, head_size,
+                                        // block_size]
+    const int64_t* __restrict__ slot_mapping, // [num_tokens]
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  const int64_t slot_idx = slot_mapping[token_idx];
+  if (slot_idx < 0) {
+    // Padding token that should be ignored.
+    return;
+  }
+
+  const int64_t block_idx = slot_idx / block_size;
+  const int64_t block_offset = slot_idx % block_size;
+
+  const int n = num_heads * head_size;
+  for (int i = item_ct1.get_local_id(2); i < n;
+       i += item_ct1.get_local_range(2)) {
+    const int64_t src_key_idx = token_idx * key_stride + i;
+    const int64_t src_value_idx = token_idx * value_stride + i;
+
+    const int head_idx = i / head_size;
+    const int head_offset = i % head_size;
+    const int x_idx = head_offset / x;
+    const int x_offset = head_offset % x;
+
+    const int64_t tgt_key_idx =
+        block_idx * num_heads * (head_size / x) * block_size * x +
+        head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +
+        block_offset * x + x_offset;
+    const int64_t tgt_value_idx =
+        block_idx * num_heads * head_size * block_size +
+        head_idx * head_size * block_size + head_offset * block_size +
+        block_offset;
+    key_cache[tgt_key_idx] = key[src_key_idx];
+    value_cache[tgt_value_idx] = value[src_value_idx];
+  }
+}
+
+template <typename scalar_t>
+void call_reshape_and_cache_kernel(
+    const scalar_t* __restrict__ key,
+    const scalar_t* __restrict__ value,
+    scalar_t* __restrict__ key_cache,
+    scalar_t* __restrict__ value_cache,
+    const int64_t* __restrict__ slot_mapping,
+    const int num_tokens,
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * head_size, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          reshape_and_cache_kernel<sycl_t>(
+              (const sycl_t* __restrict__)key,
+              (const sycl_t* __restrict__)value,
+              (sycl_t* __restrict__)key_cache,
+              (sycl_t* __restrict__)value_cache,
+              slot_mapping,
+              key_stride,
+              value_stride,
+              num_heads,
+              head_size,
+              block_size,
+              x,
+              item_ct1);
+        });
+  });
+}
+
+void reshape_and_cache(
+    torch::Tensor& key,
+    torch::Tensor& value,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& slot_mapping,
+    const std::string& kv_cache_dtype,
+    const float kv_scale) {
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(3);
+  int x = key_cache.size(4);
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      key.scalar_type(), "call_reshape_and_cache_kernel", [&] {
+        call_reshape_and_cache_kernel<scalar_t>(
+            key.data_ptr<scalar_t>(),
+            value.data_ptr<scalar_t>(),
+            key_cache.data_ptr<scalar_t>(),
+            value_cache.data_ptr<scalar_t>(),
+            slot_mapping.data_ptr<int64_t>(),
+            num_tokens,
+            key_stride,
+            value_stride,
+            num_heads,
+            head_size,
+            block_size,
+            x);
+      });
+}
+
+template <typename scalar_t>
+void reshape_and_cache_ipexllm_kernel(
+    const scalar_t* __restrict__ key, // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value, // [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key_cache, // [num_blocks, num_kv_heads, block_size, head_size]
+    scalar_t* __restrict__ value_cache, // [num_blocks, num_kv_heads, block_size, head_size]
+    const int64_t* __restrict__ slot_mapping, // [num_tokens]
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  const int64_t slot_idx = slot_mapping[token_idx];
+  if (slot_idx < 0) {
+    // Padding token that should be ignored.
+    return;
+  }
+
+  const int64_t block_idx = slot_idx / block_size;
+  const int64_t block_offset = slot_idx % block_size;
+
+  const int n = num_heads * head_size;
+  for (int i = item_ct1.get_local_id(2); i < n;
+       i += item_ct1.get_local_range(2)) {
+    const int64_t src_key_idx = token_idx * key_stride + i;
+    const int64_t src_value_idx = token_idx * value_stride + i;
+
+    const int head_idx = i / head_size;
+    const int head_offset = i % head_size;
+
+    // const int64_t tgt_key_idx =
+    //     block_idx * num_heads * (head_size / x) * block_size * x +
+    //     head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +
+    //     block_offset * x + x_offset;
+
+    // const int64_t tgt_value_idx =
+    //     block_idx * num_heads * head_size * block_size +
+    //     head_idx * head_size * block_size + head_offset * block_size +
+    //     block_offset;
+
+    const int64_t tgt_value_idx =
+        block_idx * num_heads * head_size * block_size +
+        head_idx * head_size * block_size + 
+        block_offset * head_size + 
+        head_offset;
+    const int64_t tgt_key_idx = tgt_value_idx;
+    key_cache[tgt_key_idx] = key[src_key_idx];
+    value_cache[tgt_value_idx] = value[src_value_idx];
+  }
+}
+
+template <typename scalar_t>
+void call_reshape_and_cache_ipexllm_kernel(
+    const scalar_t* __restrict__ key,
+    const scalar_t* __restrict__ value,
+    scalar_t* __restrict__ key_cache,
+    scalar_t* __restrict__ value_cache,
+    const int64_t* __restrict__ slot_mapping,
+    const int num_tokens,
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * head_size, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          reshape_and_cache_ipexllm_kernel<sycl_t>(
+              (const sycl_t* __restrict__)key,
+              (const sycl_t* __restrict__)value,
+              (sycl_t* __restrict__)key_cache,
+              (sycl_t* __restrict__)value_cache,
+              slot_mapping,
+              key_stride,
+              value_stride,
+              num_heads,
+              head_size,
+              block_size,
+              x,
+              item_ct1);
+        });
+  });
+}
+
+void reshape_and_cache_ipexllm(
+    torch::Tensor& key,
+    torch::Tensor& value,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& slot_mapping,
+    const std::string& kv_cache_dtype,
+    const float kv_scale) {
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(2);
+  // int x = key_cache.size(4);
+  int x = 1;
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel", [&] {
+        call_reshape_and_cache_ipexllm_kernel<scalar_t>(
+            key.data_ptr<scalar_t>(),
+            value.data_ptr<scalar_t>(),
+            key_cache.data_ptr<scalar_t>(),
+            value_cache.data_ptr<scalar_t>(),
+            slot_mapping.data_ptr<int64_t>(),
+            num_tokens,
+            key_stride,
+            value_stride,
+            num_heads,
+            head_size,
+            block_size,
+            x);
+      });
+}
+
+
+template <typename scalar_t>
+void copy_blocks_kernel(
+    int64_t* key_cache_ptrs,
+    int64_t* value_cache_ptrs,
+    const int64_t* __restrict__ block_mapping,
+    const int numel_per_block,
+    const sycl::nd_item<3>& item_ct1) {
+  const int layer_idx = item_ct1.get_group(2);
+  const int pair_idx = item_ct1.get_group(1);
+
+  scalar_t* key_cache = reinterpret_cast<scalar_t*>(key_cache_ptrs[layer_idx]);
+  scalar_t* value_cache =
+      reinterpret_cast<scalar_t*>(value_cache_ptrs[layer_idx]);
+  int64_t src_block_number = block_mapping[2 * pair_idx];
+  int64_t dst_block_number = block_mapping[2 * pair_idx + 1];
+
+  const int64_t src_block_offset = src_block_number * numel_per_block;
+  const int64_t dst_block_offset = dst_block_number * numel_per_block;
+  for (int i = item_ct1.get_local_id(2); i < numel_per_block;
+       i += item_ct1.get_local_range(2)) {
+    int64_t src_offset = src_block_offset + i;
+    int64_t dst_offset = dst_block_offset + i;
+    key_cache[dst_offset] = key_cache[src_offset];
+  }
+  for (int i = item_ct1.get_local_id(2); i < numel_per_block;
+       i += item_ct1.get_local_range(2)) {
+    int64_t src_offset = src_block_offset + i;
+    int64_t dst_offset = dst_block_offset + i;
+    value_cache[dst_offset] = value_cache[src_offset];
+  }
+}
+
+template <typename scalar_t>
+void call_copy_blocks_kernel(
+    std::vector<torch::Tensor>& key_caches,
+    std::vector<torch::Tensor>& value_caches,
+    const std::map<int64_t, std::vector<int64_t>>& block_mapping) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int num_layers = key_caches.size();
+  TORCH_CHECK(num_layers == value_caches.size());
+  if (num_layers == 0) {
+    return;
+  }
+  torch::Device cache_device = key_caches[0].device();
+  TORCH_CHECK(cache_device.is_xpu());
+  // Create data structures for the kernel.
+  // Create an array of pointers to the key and value caches.
+  int64_t key_cache_ptrs[num_layers];
+  int64_t value_cache_ptrs[num_layers];
+  for (int layer_idx = 0; layer_idx < num_layers; ++layer_idx) {
+    key_cache_ptrs[layer_idx] =
+        reinterpret_cast<int64_t>(key_caches[layer_idx].data_ptr());
+    value_cache_ptrs[layer_idx] =
+        reinterpret_cast<int64_t>(value_caches[layer_idx].data_ptr());
+  }
+  // Create block mapping array.
+  std::vector<int64_t> block_mapping_vec;
+  for (const auto& pair : block_mapping) {
+    int64_t src_block_number = pair.first;
+    for (int64_t dst_block_number : pair.second) {
+      block_mapping_vec.push_back(src_block_number);
+      block_mapping_vec.push_back(dst_block_number);
+    }
+  }
+  int64_t* block_mapping_array = block_mapping_vec.data();
+  int num_pairs = block_mapping_vec.size() / 2;
+  // Move the data structures to the GPU.
+  // NOTE: This synchronizes the CPU and GPU.
+  torch::Tensor key_cache_ptrs_tensor =
+      torch::from_blob(key_cache_ptrs, {num_layers}, torch::kInt64)
+          .to(cache_device);
+  torch::Tensor value_cache_ptrs_tensor =
+      torch::from_blob(value_cache_ptrs, {num_layers}, torch::kInt64)
+          .to(cache_device);
+  torch::Tensor block_mapping_tensor =
+      torch::from_blob(block_mapping_array, {2 * num_pairs}, torch::kInt64)
+          .to(cache_device);
+  auto k_ptr = key_cache_ptrs_tensor.data_ptr<int64_t>();
+  auto v_ptr = value_cache_ptrs_tensor.data_ptr<int64_t>();
+  auto b_ptr = block_mapping_tensor.data_ptr<int64_t>();
+  // Launch the kernel.
+  const int numel_per_block = key_caches[0][0].numel();
+
+  sycl::range<3> grid(1, num_pairs, num_layers);
+  sycl::range<3> block(1, 1, std::min(1024, numel_per_block));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          copy_blocks_kernel<sycl_t>(
+              k_ptr, v_ptr, b_ptr, numel_per_block, item_ct1);
+        });
+  });
+}
+
+void copy_blocks(
+    std::vector<torch::Tensor>& key_caches,
+    std::vector<torch::Tensor>& value_caches,
+    const std::map<int64_t, std::vector<int64_t>>& block_mapping) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      key_caches[0].scalar_type(), "call_copy_blocks_kernel", [&] {
+        call_copy_blocks_kernel<scalar_t>(
+            key_caches, value_caches, block_mapping);
+      });
+}
+
+void swap_blocks(
+    torch::Tensor& src,
+    torch::Tensor& dst,
+    const std::map<int64_t, int64_t>& block_mapping) {
+  char* src_ptr = (char*)src.data_ptr();
+  char* dst_ptr = (char*)dst.data_ptr();
+
+  const int64_t block_size_in_bytes = src.element_size() * src[0].numel();
+  auto& queue = vllm::xpu::vllmGetQueue();
+
+  // NOTE(woosuk): This can be slow if the number of blocks is large.
+  for (const auto& pair : block_mapping) {
+    int64_t src_block_number = pair.first;
+    int64_t dst_block_number = pair.second;
+    int64_t src_offset = src_block_number * block_size_in_bytes;
+    int64_t dst_offset = dst_block_number * block_size_in_bytes;
+    queue.memcpy(
+        dst_ptr + dst_offset, src_ptr + src_offset, block_size_in_bytes);
+  }
+  queue.wait();
+}
+
+template <typename scalar_t>
+void gather_cached_kv_kernel(
+    scalar_t* __restrict__ key, // [num_tokens, [stride], num_heads, head_size]
+    scalar_t* __restrict__ value, // [num_tokens, [stride], num_heads,
+                                  // head_size]
+    const scalar_t* __restrict__ key_cache, // [num_blocks, num_heads,
+                                            // head_size/x, block_size, x]
+    const scalar_t* __restrict__ value_cache, // [num_blocks, num_heads,
+                                              // head_size, block_size]
+    const int* __restrict__ slot_mapping, // [num_tokens]
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x,
+    const sycl::nd_item<3>& item_ct1) {
+  const int token_idx = item_ct1.get_group(2);
+  const int slot_idx = slot_mapping[token_idx];
+  const int block_idx = slot_idx / block_size;
+  const int block_offset = slot_idx % block_size;
+
+  const int num_tokens = num_heads * head_size;
+  for (int i = item_ct1.get_local_id(2); i < num_tokens;
+       i += item_ct1.get_local_range(2)) {
+    const int tgt_key_idx = token_idx * key_stride + i;
+    const int tgt_value_idx = token_idx * value_stride + i;
+
+    const int head_idx = i / head_size;
+    const int head_offset = i % head_size;
+    const int x_idx =
+        head_offset / x; // the offset of the [head_size/x] dimension
+    const int x_offset = head_offset % x;
+
+    // const int src_key_idx =
+    //     block_idx * num_heads * (head_size / x) * block_size * x +
+    //     head_idx * (head_size / x) * block_size * x + x_idx * block_size * x +
+    //     block_offset * x + x_offset;
+    // const int src_value_idx = block_idx * num_heads * head_size * block_size +
+    //     head_idx * head_size * block_size + head_offset * block_size +
+    //     block_offset;
+
+    const int src_value_idx = 
+        block_idx * num_heads * head_size * block_size + 
+        head_idx * head_size * block_size + 
+        block_offset * head_size + 
+        head_offset;
+    const int src_key_idx = src_value_idx;
+
+    key[tgt_key_idx] = VLLM_LDG(&key_cache[src_key_idx]);
+    value[tgt_value_idx] = VLLM_LDG(&value_cache[src_value_idx]);
+  }
+}
+
+template <typename scalar_t>
+void gather_cached_kv_kernel_optimized(
+    scalar_t* __restrict__ key, // [num_tokens, [stride], num_heads, head_size]
+    scalar_t* __restrict__ value, // [num_tokens, [stride], num_heads,
+                                  // head_size]
+    const scalar_t* __restrict__ key_cache, // [num_blocks, num_heads,
+                                            // head_size/x, block_size, x]
+    const scalar_t* __restrict__ value_cache, // [num_blocks, num_heads,
+                                              // head_size, block_size]
+    const int* __restrict__ slot_mapping, // [num_tokens]
+    const int key_stride,
+    const int value_stride,
+    const int num_heads,
+    const int head_size,
+    const int block_size,
+    const int x,
+    const sycl::nd_item<3>& item_ct1) {
+  const int token_idx = item_ct1.get_group(2);
+  const int slot_idx = slot_mapping[token_idx];
+  const int block_idx = slot_idx / block_size;
+  const int block_offset = slot_idx % block_size;
+
+  const int dim = num_heads * head_size;
+  assert(dim % 4 == 0); // this is true for known use cases
+  const int unroll_factor = 4;
+  const int unrolled_dim = dim / unroll_factor;
+
+  for (int i = item_ct1.get_local_id(2); i < unrolled_dim;
+       i += item_ct1.get_local_range(2)) {
+    int tgt_key_indices[unroll_factor];
+    int tgt_value_indices[unroll_factor];
+    int src_key_indices[unroll_factor];
+    int src_value_indices[unroll_factor];
+    scalar_t keys_to_store[unroll_factor];
+    scalar_t values_to_store[unroll_factor];
+
+#pragma unroll
+    for (int j = 0; j < unroll_factor; ++j) {
+      int index = i + j * unrolled_dim;
+
+      const int tgt_key_idx = token_idx * key_stride + index;
+      const int tgt_value_idx = token_idx * value_stride + index;
+
+      const int head_idx = index / head_size;
+      const int head_offset = index % head_size;
+
+      const int src_value_idx = 
+        block_idx * num_heads * head_size * block_size + 
+        head_idx * head_size * block_size + 
+        block_offset * head_size + 
+        head_offset;
+      const int src_key_idx = src_value_idx;
+
+      tgt_key_indices[j] = tgt_key_idx;
+      tgt_value_indices[j] = tgt_value_idx;
+      src_key_indices[j] = src_key_idx;
+      src_value_indices[j] = src_value_idx;
+
+      keys_to_store[j] = VLLM_LDG(&key_cache[src_key_idx]);
+      values_to_store[j] = VLLM_LDG(&value_cache[src_value_idx]);
+    }
+
+#pragma unroll
+    for (int j = 0; j < unroll_factor; ++j) {
+      key[tgt_key_indices[j]] = keys_to_store[j];
+      value[tgt_value_indices[j]] = values_to_store[j];
+    }
+  }
+}
+
+template <typename scalar_t>
+void call_gather_cached_kv_kernel_optimized(
+    torch::Tensor& key,
+    torch::Tensor& value,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& slot_mapping) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(2);
+  // int x = key_cache.size(4);
+  int x = 1;
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+  auto key_ptr = key.data_ptr<scalar_t>();
+  auto value_ptr = value.data_ptr<scalar_t>();
+  auto key_cache_ptr = key_cache.data_ptr<scalar_t>();
+  auto value_cache_ptr = value_cache.data_ptr<scalar_t>();
+  auto slot_mapping_ptr = slot_mapping.data_ptr<int>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * head_size, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          gather_cached_kv_kernel_optimized<sycl_t>(
+              (sycl_t* __restrict__)key_ptr,
+              (sycl_t* __restrict__)value_ptr,
+              (const sycl_t* __restrict__)key_cache_ptr,
+              (const sycl_t* __restrict__)value_cache_ptr,
+              slot_mapping_ptr,
+              key_stride,
+              value_stride,
+              num_heads,
+              head_size,
+              block_size,
+              x,
+              item_ct1);
+        });
+  });
+}
+
+void gather_cached_kv(
+    torch::Tensor& key,
+    torch::Tensor& value,
+    torch::Tensor& key_cache,
+    torch::Tensor& value_cache,
+    torch::Tensor& slot_mapping) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      key_cache[0].scalar_type(),
+      "call_gather_cached_kv_kernel_optimized",
+      [&] {
+        call_gather_cached_kv_kernel_optimized<scalar_t>(
+            key, value, key_cache, value_cache, slot_mapping);
+      });
+}
diff --git a/csrc/xpu/cache_ops_xpu_fp8.cpp b/csrc/xpu/cache_ops_xpu_fp8.cpp
new file mode 100644
index 000000000..e4a0001fe
--- /dev/null
+++ b/csrc/xpu/cache_ops_xpu_fp8.cpp
@@ -0,0 +1,170 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <ext/intel/esimd.hpp>
+// clang-format on
+#include "xpu_types.h"
+
+#include <torch/extension.h>
+#include "utils.h"
+#include "kv.h"
+
+using fp16 = sycl::half;
+using namespace sycl::ext::intel::esimd;
+
+// scalar_t is key.scalar_type() -> half
+template <typename scalar_t, const int HD>
+void reshape_and_cache_ipexllm_kernel_fp8(
+    const scalar_t* __restrict__ key,    // [num_tokens, num_heads, head_size]
+    const scalar_t* __restrict__ value,  // [num_tokens, num_heads, head_size]
+    uint8_t * __restrict__ key_cache,  // [num_blocks, num_kv_heads, block_size,
+                                       // head_size]
+    uint8_t * __restrict__ value_cache,        // [num_blocks, num_kv_heads,
+                                               // block_size, head_size]
+    const int64_t* __restrict__ slot_mapping,  // [num_tokens]
+    const int key_stride, const int value_stride,
+    const int key_head_stride, const int value_head_stride,
+    const int num_heads,
+    const int head_size, const int block_size, const int x,
+    const sycl::nd_item<3>& item_ct1) {
+
+  //                      New Implementation                      //
+  const size_t token_idx = item_ct1.get_global_id(0);
+  const size_t head_idx = item_ct1.get_global_id(1);
+  const int64_t slot_idx = slot_mapping[token_idx];
+  if (slot_idx < 0) {
+    return;
+  }
+  const int64_t block_idx = slot_idx / block_size;
+  const int64_t block_offset = slot_idx % block_size;
+  // The thread is responsible for the HD elements within key/value
+  const scalar_t * key_head = key + token_idx * key_stride + head_idx * key_head_stride;
+
+  const scalar_t * value_head = value + token_idx * value_stride + head_idx * value_head_stride;
+
+  uint8_t * key_output_head = key_cache + block_idx * num_heads * head_size * block_size +
+      head_idx * head_size * block_size + block_offset * head_size;
+  uint8_t * value_output_head = value_cache + block_idx * num_heads * head_size * block_size +
+      head_idx * head_size * block_size + block_offset * head_size;
+
+  simd<fp16, HD> key_row = block_load<scalar_t, HD>(key_head);
+  simd<uint8_t, HD> key_result = quantize_key_row<HD>(key_row);
+  block_store<uint8_t, HD>(key_output_head, key_result);
+
+  simd<fp16, HD> value_row = block_load<scalar_t, HD>(value_head);
+  simd<uint8_t, HD> value_result = quantize_value_row<HD>(value_row);
+  block_store<uint8_t, HD>(value_output_head, value_result);
+}
+
+
+template <typename scalar_t, const int HD>
+void call_reshape_and_cache_ipexllm_kernel_fp8(
+    const scalar_t* __restrict__ key, const scalar_t* __restrict__ value,
+    uint8_t* __restrict__ key_cache, uint8_t* __restrict__ value_cache,
+    const int64_t* __restrict__ slot_mapping, const int num_tokens,
+    const int key_stride, const int value_stride,
+    const int key_head_stride, const int value_head_stride,
+    const int num_heads,
+    const int head_size, const int block_size, const int x) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(num_tokens, num_heads, 1);
+  sycl::range<3> block(1, 1, 1);
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) SYCL_ESIMD_KERNEL {
+          reshape_and_cache_ipexllm_kernel_fp8<sycl_t, HD>(
+              (const sycl_t* __restrict__)key,
+              (const sycl_t* __restrict__)value,
+              (uint8_t* __restrict__)key_cache,
+              (uint8_t* __restrict__)value_cache, slot_mapping, key_stride,
+              value_stride, key_head_stride, value_head_stride,
+              num_heads, head_size, block_size, x, item_ct1);
+        });
+  });
+}
+
+void reshape_and_cache_ipexllm_fp8(torch::Tensor& key, torch::Tensor& value,
+                               torch::Tensor& key_cache,
+                               torch::Tensor& value_cache,
+                               torch::Tensor& slot_mapping,
+                               const std::string& kv_cache_dtype,
+                               const float kv_scale) {
+  int num_tokens = key.size(0);
+  int num_heads = key.size(1);
+  int head_size = key.size(2);
+  int block_size = key_cache.size(2);
+  // int x = key_cache.size(4);
+  int x = 1;
+
+  int key_stride = key.stride(0);
+  int value_stride = value.stride(0);
+
+  int key_head_stride = key.stride(1);
+  int value_head_stride = value.stride(1);
+
+  // This actually dispatches on scalar_type, we will then need to dispatch on Head Dim...
+switch (head_size) {
+  case 64:
+    VLLM_XPU_DISPATCH_FLOATING_TYPES(
+        key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+          call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 64>(
+              key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+              key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+              slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+              value_stride, key_head_stride, value_head_stride, num_heads,
+              head_size, block_size, x);
+        });
+    break;
+  case 128:
+    VLLM_XPU_DISPATCH_FLOATING_TYPES(
+        key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+          call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 128>(
+              key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+              key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+              slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+              value_stride, key_head_stride, value_head_stride, num_heads,
+              head_size, block_size, x);
+        });
+    break;
+  case 96:
+    VLLM_XPU_DISPATCH_FLOATING_TYPES(
+        key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+          call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 96>(
+              key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+              key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+              slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+              value_stride, key_head_stride, value_head_stride, num_heads,
+              head_size, block_size, x);
+        });
+    break;
+  case 80:
+    VLLM_XPU_DISPATCH_FLOATING_TYPES(
+        key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+          call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 80>(
+              key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+              key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+              slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+              value_stride, key_head_stride, value_head_stride, num_heads,
+              head_size, block_size, x);
+        });
+    break;
+  default:
+    TORCH_CHECK(false, "Unsupported head_dim: ", head_size);
+}
+  // VLLM_XPU_DISPATCH_FLOATING_TYPES(
+  //     key.scalar_type(), "call_reshape_and_cache_ipexllm_kernel_fp8", [&] {
+  //       call_reshape_and_cache_ipexllm_kernel_fp8<scalar_t, 128>(
+  //           key.data_ptr<scalar_t>(), value.data_ptr<scalar_t>(),
+  //           key_cache.data_ptr<uint8_t>(), value_cache.data_ptr<uint8_t>(),
+  //           slot_mapping.data_ptr<int64_t>(), num_tokens, key_stride,
+  //           value_stride, key_head_stride, value_head_stride,
+  //           num_heads, head_size, block_size, x);
+  //     });
+}
+
+
+
diff --git a/csrc/xpu/common.h b/csrc/xpu/common.h
new file mode 100644
index 000000000..17d6ef643
--- /dev/null
+++ b/csrc/xpu/common.h
@@ -0,0 +1,312 @@
+#pragma once
+
+#include <sycl.hpp>
+#include <torch/extension.h>
+
+typedef union half_t {
+    uint16_t u;
+    sycl::half f;
+} __half_t;
+
+typedef union ufloat32 {
+    unsigned u;
+    float f;
+} __float_t;
+
+#define QK4_0 64
+#define QR4_0 2
+#define QK4_1 64
+#define QR4_1 2
+#define QK5_0 64
+#define QR5_0 2
+#define QK5_1 64
+#define QR5_1 2
+#define QK8_0 64
+#define QR8_0 1
+#define QK8_1 32
+#define QR8_1 1
+#define QI8_1 (QK8_1 / (4 * QR8_1)) // 8
+#define QKFP8 64
+#define QRFP8 1
+#define QKFP6 64
+// for iq2 quantization
+#define WARP_SIZE 32
+#define QK_K 256
+#define QK4_K 32
+#define QR4_K 2
+#define QK6_K 16
+#define QKFP6_K 16
+#define QR2_XXS 8
+#define QI2_XXS (QK_K / (4*QR2_XXS)) // 8
+#define QR2_XS 8
+#define QI2_XS (QK_K / (4*QR2_XS)) // 8
+#define QR2_K 4
+#define QI2_K (QK_K / (4*QR2_K)) // 16
+#define QR1_S 8
+#define QI1_S (QK_K / (4*QR1_S)) // 8
+
+typedef struct {
+    sycl::half d;          // delta
+    uint8_t qs[QK4_0 / 2];    // nibbles / quants
+} block_q4_0;
+
+typedef struct {
+    uint8_t qs[QK4_0 / 2];    // nibbles / quants
+} block_q4_0_qs;
+
+typedef struct {
+    uint8_t qs[QK4_1 / 2];    // nibbles / quants
+} block_q4_1_qs;
+
+typedef struct {
+    sycl::half d;              // delta
+    sycl::half m;              // min
+    uint8_t qs[QK4_1 / 2];     // nibbles / quants
+} block_q4_1;
+
+typedef struct {
+    sycl::half d;
+    uint8_t qh[8];
+    uint8_t qs[QK5_0 / 2];
+} block_q5_0;
+
+typedef struct {
+    sycl::half d;          // delta
+    sycl::half m;          // min
+    uint8_t qh[8];         // 5-th bit of quants
+    uint8_t qs[QK5_1 / 2]; // nibbles / quants
+} block_q5_1;
+
+typedef struct {
+    sycl::half d;           // delta
+    uint8_t qh[8];          // 3-th bit of quants
+    uint8_t qs[QK4_0 / 4];  // nibbles / quants
+} block_nf3;
+
+typedef struct {
+    uint8_t qh[8];          // 3-th bit of quants
+    uint8_t qs[QK4_0 / 4];  // nibbles / quants
+} block_nf3_qs;
+
+typedef struct {
+    float d;       // delta
+    int8_t qs[QK8_0];   // quants
+} block_q8_0;
+
+typedef struct {
+    int8_t qs[QK8_0];   // quants
+} block_q8_0_qs;
+
+typedef struct {
+    sycl::half d;
+    sycl::half sum;
+    int8_t  qs[QK8_1];      // quants
+} block_q8_1;
+
+typedef struct {
+    uint8_t qs[QKFP8];
+} block_fp8_qs;
+
+typedef struct {
+    float d;
+    uint8_t qs[QKFP8];
+} block_fp8;
+
+typedef struct {
+    sycl::half d;
+    uint16_t qs[QK_K/8]; // 32
+} block_iq2_xxs;
+
+typedef struct {
+    sycl::half d;
+    uint16_t qs[QK_K/8]; // 32
+    uint8_t  scales[QK_K/32]; // 8
+} block_iq2_xs;
+
+typedef struct {
+    uint8_t scales[QK_K/16]; // scales and mins, quantized with 4 bits
+    uint8_t qs[QK_K/4];      // quants
+    sycl::half d;            // super-block scale for quantized scales
+    sycl::half min;          // super-block min for quantized mins
+} block_q2_K;
+
+typedef struct {
+    sycl::half d;                 // super-block scale for quantized scales
+    sycl::half dmin;              // super-block scale for quantized mins
+    uint8_t scales[16];           // scales and mins, quantized with 8 bits
+    uint8_t qs[QK_K/2];           // 4--bit quants
+} block_q4_K;
+
+typedef struct {
+    uint8_t qs[QK_K/2];            // 4-bit quants
+} block_q4_K_qs;
+
+typedef struct {
+    uint8_t qs[QK4_K/2];            // 4-bit quants
+} block_q4_K_qs_block;
+
+typedef struct {
+    uint8_t scales[16];            // scales and mins, quantized with 8 bits
+} block_q4_K_scales;
+
+typedef struct {
+    sycl::half d;               // super-block scale for quantized scales
+    sycl::half dmin;            // super-block scale for quantized mins
+    uint8_t scales[12];         // scales and mins, quantized with 6 bits
+    uint8_t qh[QK_K/8];          // quants, high bit
+    uint8_t qs[QK_K/2];          // quants, low 4 bits
+} block_q5_K;
+
+typedef struct {
+    uint8_t ql[QK_K/2];   // quants, lower 4 bits
+    uint8_t qh[QK_K/4];   // quants, upper 2 bits
+    int8_t  scales[QK_K/16]; // scales
+    sycl::half d;            // delta
+} block_q6_K;
+
+typedef struct {
+    uint32_t qh[QK_K/16];      // quants, upper 2 bits
+} block_q6_K_qh;
+
+typedef struct {
+    uint32_t ql[QK_K/8];      // quants, lower 4 bits
+} block_q6_K_ql;
+
+typedef struct {
+    int8_t  scales[QK_K/16]; // scales, quantized with 8 bits
+} block_q6_K_scales;
+
+typedef struct {
+    uint8_t ql[QK_K/2];       // quants, lower 4 bits
+    uint8_t qh[QK_K/4];       // quants, upper 2 bits
+    int8_t  scales[QK_K/16];  // scales, quantized with 8 bits
+    sycl::half d;            // super-block scale
+} block_fp6_K;
+static_assert(sizeof(block_fp6_K) == sizeof(sycl::half) + QK_K / 16 + 3*QK_K/4, "wrong fp6_K block size/padding");
+
+typedef struct {
+    uint32_t ql[QK_K/8];      // quants, lower 4 bits
+} block_fp6_k_ql;
+
+typedef struct {
+    uint32_t qh[QK_K/16];     // quants, upper 2 bits
+} block_fp6_k_qh;
+
+typedef struct {
+    int8_t scales[QK_K/16];  // scales, quantized with 8 bits, 16
+} block_fp6_k_scales;
+
+typedef struct {
+    uint32_t ql[QKFP6_K/8];     // upper 2 bits, 2
+} block_base_fp6_k_ql;
+
+typedef struct {
+    uint32_t qh[QKFP6_K/16];     // upper 2 bits, 1
+} block_base_fp6_k_qh;
+
+#define NGRID_IQ1S 2048
+#define IQ1S_DELTA 0.125f
+#define IQ1M_DELTA 0.125f
+
+typedef struct {
+    sycl::half d;
+    uint8_t  qs[QK_K/8];
+    uint16_t qh[QK_K/32];
+} block_iq1_s;
+
+// 1.8125 bpw
+typedef struct {
+    uint8_t  qs[QK_K/8];      // grid index, low 8 bits
+    uint8_t  qh[QK_K/16];     // grid index, high 3 bits + grid shift bit (for two groups of 8)
+    uint8_t  scales[QK_K/32]; // 4-bit block scales
+} block_iq1_m;
+
+typedef struct {
+    uint8_t ql[QKFP6/2];      // lower 4 bits, 32
+    uint8_t qh[QKFP6/4];      // upper 2 bits, 16
+    sycl::half  d;            // delta
+} block_fp6;
+
+typedef struct {
+    uint32_t qh[QKFP6/16];     // upper 2 bits, 4
+} block_fp6_32_qh;
+
+typedef struct {
+    uint32_t ql[QKFP6/8];      // lower 4 bits, 8
+} block_fp6_32_ql;
+
+enum ggml_type {
+    GGML_TYPE_Q4_0 = 2,
+    GGML_TYPE_Q4_1 = 3,
+    GGML_TYPE_Q5_0 = 6,
+    GGML_TYPE_Q5_1 = 7,
+    GGML_TYPE_Q8_0 = 8,
+    GGML_TYPE_Q8_1 = 9,
+    GGML_TYPE_NF4 = 10,
+    GGML_TYPE_NF3 = 11,
+    GGML_TYPE_FP8E4 = 15,
+    GGML_TYPE_FP4 = 16,
+    GGML_TYPE_FP8E5 = 19,
+    GGML_TYPE_IQ2_XXS = 21,
+    GGML_TYPE_IQ2_XS = 22,
+    GGML_TYPE_Q2_K = 23,
+    GGML_TYPE_IQ1_S = 24,
+    GGML_TYPE_IQ1_M = 25,
+    GGML_TYPE_Q6_K = 26,
+    GGML_TYPE_Q4_K = 27,
+    GGML_TYPE_Q5_K = 28,
+    GGML_TYPE_FP6 = 29,
+    GGML_TYPE_FP6_K = 30,
+    GGML_TYPE_Q4_0_WOQ = 34,
+    GGML_TYPE_COUNT
+};
+
+static const int GGML_BLCK_SIZE[GGML_TYPE_COUNT] = {
+    [GGML_TYPE_Q4_0] = QK4_0,
+    [GGML_TYPE_Q4_1] = QK4_1,
+    [GGML_TYPE_Q5_0] = QK5_0,
+    [GGML_TYPE_Q5_1] = QK5_1,
+    [GGML_TYPE_NF4]  = QK4_0,
+    [GGML_TYPE_NF3]  = QK4_0,
+    [GGML_TYPE_Q8_0] = QK8_0,
+    [GGML_TYPE_Q8_1] = QK8_1,
+    [GGML_TYPE_FP8E4]  = QKFP8,
+    [GGML_TYPE_FP4]  = QK4_0,
+    [GGML_TYPE_FP6]  = QKFP6,
+    [GGML_TYPE_FP8E5]  = QKFP8,
+    [GGML_TYPE_IQ2_XXS] = QK_K,
+    [GGML_TYPE_IQ2_XS] = QK_K,
+    [GGML_TYPE_Q2_K] = QK_K,
+    [GGML_TYPE_IQ1_S] = QK_K,
+    [GGML_TYPE_IQ1_M] = QK_K,
+    [GGML_TYPE_Q6_K] = QK_K,
+    [GGML_TYPE_Q4_K] = QK_K,
+    [GGML_TYPE_Q5_K] = QK_K,
+    [GGML_TYPE_FP6_K] = QK_K,
+    [GGML_TYPE_Q4_0_WOQ] = QK4_0,
+};
+
+static const size_t GGML_TYPE_SIZE[GGML_TYPE_COUNT] = {
+    [GGML_TYPE_Q4_0] = sizeof(block_q4_0),
+    [GGML_TYPE_Q4_1] = sizeof(block_q4_1),
+    [GGML_TYPE_Q5_0] = sizeof(block_q5_1),
+    [GGML_TYPE_Q5_1] = sizeof(block_q5_1),
+    [GGML_TYPE_NF4]  = sizeof(block_q4_0),
+    [GGML_TYPE_NF3]  = sizeof(block_nf3),
+    [GGML_TYPE_Q8_0] = sizeof(block_q8_0),
+    [GGML_TYPE_Q8_1] = sizeof(block_q8_1),
+    [GGML_TYPE_FP8E4]= sizeof(block_fp8),
+    [GGML_TYPE_FP4]  = sizeof(block_q4_0),
+    [GGML_TYPE_FP6]  = sizeof(block_fp6),
+    [GGML_TYPE_FP8E5]  = sizeof(block_fp8),
+    [GGML_TYPE_IQ2_XXS] = sizeof(block_iq2_xxs),
+    [GGML_TYPE_IQ2_XS] = sizeof(block_iq2_xs),
+    [GGML_TYPE_Q2_K] = sizeof(block_q2_K),
+    [GGML_TYPE_IQ1_S] = sizeof(block_iq1_s),
+    [GGML_TYPE_IQ1_M] = sizeof(block_iq1_m),
+    [GGML_TYPE_Q6_K] = sizeof(block_q6_K),
+    [GGML_TYPE_Q4_K] = sizeof(block_q4_K),
+    [GGML_TYPE_Q5_K] = sizeof(block_q5_K),
+    [GGML_TYPE_FP6_K] = sizeof(block_fp6_K),
+    [GGML_TYPE_Q4_0_WOQ] = sizeof(block_q4_0),
+};
diff --git a/csrc/xpu/dequantize.h b/csrc/xpu/dequantize.h
new file mode 100644
index 000000000..9a967312e
--- /dev/null
+++ b/csrc/xpu/dequantize.h
@@ -0,0 +1,74 @@
+#include <dpct/dpct.hpp>
+#include <sycl/sycl.hpp>
+#include "utils.h"
+/*
+Adapted from https://github.com/mit-han-lab/llm-awq
+Modified from NVIDIA FasterTransformer:
+https://github.com/NVIDIA/FasterTransformer/blob/main/src/fastertransformer/cutlass_extensions/include/cutlass_extensions/interleaved_numeric_conversion.h
+@article{lin2023awq,
+  title={AWQ: Activation-aware Weight Quantization for LLM Compression and
+Acceleration}, author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang,
+Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}
+}
+*/
+
+#pragma once
+
+namespace vllm {
+namespace awq {
+
+sycl::uint4 dequantize_s4_to_fp16x2(uint32_t const& source) {
+  sycl::uint4 result;
+
+  uint32_t* h = reinterpret_cast<uint32_t*>(&result);
+  uint32_t const i4s = reinterpret_cast<uint32_t const&>(source);
+
+  // First, we extract the i4s and construct an intermediate fp16 number.
+  static constexpr uint32_t immLut = (0xf0 & 0xcc) | 0xaa;
+  static constexpr uint32_t BOTTOM_MASK = 0x000f000f;
+  static constexpr uint32_t TOP_MASK = 0x00f000f0;
+  static constexpr uint32_t I4s_TO_F16s_MAGIC_NUM = 0x64006400;
+
+  // Note that the entire sequence only requires 1 shift instruction. This is
+  // thanks to the register packing format and the fact that we force our
+  // integers to be unsigned, and account for this in the fp16 subtractions. In
+  // addition, I exploit the fact that sub and fma have the same throughput in
+  // order to convert elt_23 and elt_67 to fp16 without having to shift them to
+  // the bottom bits before hand.
+
+  // Shift right by 8 to now consider elt_45 and elt_67. Issue first to hide RAW
+  // dependency if we issue immediately before required.
+  const uint32_t top_i4s = i4s >> 8;
+  h[0] = (i4s & BOTTOM_MASK) | I4s_TO_F16s_MAGIC_NUM;
+  h[1] = (i4s & TOP_MASK) | I4s_TO_F16s_MAGIC_NUM;
+  h[2] = (top_i4s & BOTTOM_MASK) | I4s_TO_F16s_MAGIC_NUM;
+  h[3] = (top_i4s & TOP_MASK) | I4s_TO_F16s_MAGIC_NUM;
+
+  // This is the half2 {1032, 1032} represented as an integer.
+  // static constexpr uint32_t FP16_TOP_MAGIC_NUM = 0x64086408;
+  // Haotian: subtract {1024, 1024} instead, we do not need to map to [-8, 7]
+  static constexpr uint32_t FP16_TOP_MAGIC_NUM = 0x64006400;
+  // This is the half2 {1 / 16, 1 / 16} represented as an integer.
+  static constexpr uint32_t ONE_SIXTEENTH = 0x2c002c00;
+  // This is the half2 {-72, -72} represented as an integer.
+  // static constexpr uint32_t NEG_72 = 0xd480d480;
+  // Haotian: Let's use {-64, -64}.
+  static constexpr uint32_t NEG_64 = 0xd400d400;
+  *(sycl::half2*)(&h[0]) = sycl_half_sub2(
+      *(sycl::half2*)(&h[0]), *(sycl::half2*)(&FP16_TOP_MAGIC_NUM));
+  *(sycl::half2*)(&h[1]) = sycl_half_fma2(
+      *(sycl::half2*)(&h[1]),
+      *(sycl::half2*)(&ONE_SIXTEENTH),
+      *(sycl::half2*)(&NEG_64));
+  *(sycl::half2*)(&h[2]) = sycl_half_sub2(
+      *(sycl::half2*)(&h[2]), *(sycl::half2*)(&FP16_TOP_MAGIC_NUM));
+  *(sycl::half2*)(&h[3]) = sycl_half_fma2(
+      *(sycl::half2*)(&h[3]),
+      *(sycl::half2*)(&ONE_SIXTEENTH),
+      *(sycl::half2*)(&NEG_64));
+
+  return result;
+}
+
+} // namespace awq
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/dtype_float16.h b/csrc/xpu/dtype_float16.h
new file mode 100644
index 000000000..1b9c1f248
--- /dev/null
+++ b/csrc/xpu/dtype_float16.h
@@ -0,0 +1,458 @@
+/*
+ * Adapted from
+ * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
+ * and
+ * https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h
+ * Copyright (c) 2023, The vLLM team.
+ * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <dpct/dpct.hpp>
+#include <sycl/sycl.hpp>
+#include "attention_generic.h"
+#include "dtype_float32.h"
+#include "utils.h"
+
+#include <stdint.h>
+
+namespace vllm {
+
+// FP16 vector types for Q, K, V.
+template <>
+struct Vec<sycl::half, 1> {
+  using Type = sycl::half;
+};
+template <>
+struct Vec<sycl::half, 2> {
+  using Type = sycl::half2;
+};
+template <>
+struct Vec<sycl::half, 4> {
+  using Type = sycl::half4;
+};
+template <>
+struct Vec<sycl::half, 8> {
+  using Type = sycl::half8;
+};
+
+template <>
+struct FloatVec<sycl::half> {
+  using Type = float;
+};
+template <>
+struct FloatVec<sycl::half2> {
+  using Type = sycl::float2;
+};
+
+template <>
+struct FloatVec<sycl::half4> {
+  using Type = Float4_;
+};
+template <>
+struct FloatVec<sycl::half8> {
+  using Type = Float8_;
+};
+
+// Utility functions for type conversions.
+inline sycl::half2 h0_h0(sycl::half a) {
+  return sycl::half2{a, a};
+}
+
+inline float half_to_float(sycl::half h) {
+  return float(h);
+}
+
+inline sycl::float2 half2_to_float2(sycl::half2 v) {
+
+  return sycl::float2(half_to_float(v.x()), half_to_float(v.y()));
+}
+
+inline sycl::half float_to_half(float f) {
+  return sycl::half(f);
+}
+
+inline sycl::half2 float2_to_half2(sycl::float2 f) {
+  return sycl::half2{float_to_half(f.x()), float_to_half(f.y())};
+}
+
+// Vector addition.
+inline sycl::half add(sycl::half a, sycl::half b) {
+  return sycl_half_add(a,b);
+}
+
+inline sycl::half2 add(sycl::half2 a, sycl::half2 b) {
+  auto val = sycl_half_add2(a, b);
+  return (val);
+}
+
+inline sycl::half4 add(sycl::half4 a, sycl::half4 b) {
+  sycl::half4 c;
+  c.x() = add(a.x(), b.x());
+  c.y() = add(a.y(), b.y());
+  c.z() = add(a.z(), b.z());
+  c.w() = add(a.w(), b.w());
+  return c;
+}
+
+inline sycl::half8 add(sycl::half8 a, sycl::half8 b) {
+  sycl::half8 c;
+  c.s0() = add(a.s0(), b.s0());
+  c.s1() = add(a.s1(), b.s1());
+  c.s2() = add(a.s2(), b.s2());
+  c.s3() = add(a.s3(), b.s3());
+  c.s4() = add(a.s4(), b.s4());
+  c.s5() = add(a.s5(), b.s5());
+  c.s6() = add(a.s6(), b.s6());
+  c.s7() = add(a.s7(), b.s7());
+  return c;
+}
+
+inline sycl::float2 add(sycl::half2 a, sycl::float2 fb) {
+  sycl::float2 fa = half2_to_float2(a);
+  return add(fa, fb);
+}
+
+inline Float4_ add(sycl::half4 a, Float4_ fb) {
+  Float4_ fc;
+  fc.x = add(sycl::half2{a.x(), a.y()}, fb.x);
+  fc.y = add(sycl::half2{a.z(), a.w()}, fb.y);
+  return fc;
+}
+
+inline Float8_ add(sycl::half8 a, Float8_ fb) {
+  Float8_ fc;
+  fc.x = add(sycl::half2{a.s0(), a.s1()}, fb.x);
+  fc.y = add(sycl::half2{a.s2(), a.s3()}, fb.y);
+  fc.z = add(sycl::half2{a.s4(), a.s5()}, fb.z);
+  fc.w = add(sycl::half2{a.s6(), a.s7()}, fb.w);
+  return fc;
+}
+
+// Vector multiplication.
+template <>
+inline sycl::half mul(sycl::half a, sycl::half b) {
+  auto val = sycl_half_mul((a), (b));
+  return (val);
+}
+
+template <>
+inline sycl::half2 mul(sycl::half2 a, sycl::half2 b) {
+  auto val = sycl_half_mul2((a), (b));
+  return (val);
+}
+
+template <>
+inline sycl::half2 mul(sycl::half a, sycl::half2 b) {
+  return mul<sycl::half2, sycl::half2, sycl::half2>(h0_h0(a), b);
+}
+
+
+template <>
+inline sycl::half4 mul(sycl::half4 a, sycl::half4 b) {
+  sycl::half4 c;
+  c.x() = mul<sycl::half, sycl::half, sycl::half>(a.x(), b.x());
+  c.y() = mul<sycl::half, sycl::half, sycl::half>(a.y(), b.y());
+  c.z() = mul<sycl::half, sycl::half, sycl::half>(a.z(), b.z());
+  c.w() = mul<sycl::half, sycl::half, sycl::half>(a.w(), b.w());
+  return c;
+}
+
+template <>
+inline sycl::half4 mul(sycl::half a, sycl::half4 b) {
+  sycl::half4 c;
+  c.x() = mul<sycl::half, sycl::half, sycl::half>(a, b.x());
+  c.y() = mul<sycl::half, sycl::half, sycl::half>(a, b.y());
+  c.z() = mul<sycl::half, sycl::half, sycl::half>(a, b.z());
+  c.w() = mul<sycl::half, sycl::half, sycl::half>(a, b.w());
+  return c;
+}
+
+template <>
+inline sycl::half8 mul(sycl::half8 a, sycl::half8 b) {
+  sycl::half8 c;
+  c.s0() = mul<sycl::half, sycl::half, sycl::half>(a.s0(), b.s0());
+  c.s1() = mul<sycl::half, sycl::half, sycl::half>(a.s1(), b.s1());
+  c.s2() = mul<sycl::half, sycl::half, sycl::half>(a.s2(), b.s2());
+  c.s3() = mul<sycl::half, sycl::half, sycl::half>(a.s3(), b.s3());
+  c.s4() = mul<sycl::half, sycl::half, sycl::half>(a.s4(), b.s4());
+  c.s5() = mul<sycl::half, sycl::half, sycl::half>(a.s5(), b.s5());
+  c.s6() = mul<sycl::half, sycl::half, sycl::half>(a.s6(), b.s6());
+  c.s7() = mul<sycl::half, sycl::half, sycl::half>(a.s7(), b.s7());
+  return c;
+}
+
+template <>
+inline sycl::half8 mul(sycl::half a, sycl::half8 b) {
+  sycl::half8 c;
+  c.s0() = mul<sycl::half, sycl::half, sycl::half>(a, b.s0());
+  c.s1() = mul<sycl::half, sycl::half, sycl::half>(a, b.s1());
+  c.s2() = mul<sycl::half, sycl::half, sycl::half>(a, b.s2());
+  c.s3() = mul<sycl::half, sycl::half, sycl::half>(a, b.s3());
+  c.s4() = mul<sycl::half, sycl::half, sycl::half>(a, b.s4());
+  c.s5() = mul<sycl::half, sycl::half, sycl::half>(a, b.s5());
+  c.s6() = mul<sycl::half, sycl::half, sycl::half>(a, b.s6());
+  c.s7() = mul<sycl::half, sycl::half, sycl::half>(a, b.s7());
+  return c;
+}
+
+template <>
+inline float mul(sycl::half a, sycl::half b) {
+  float fa = half_to_float(a);
+  float fb = half_to_float(b);
+  return fa * fb;
+}
+
+template <>
+inline sycl::float2 mul(sycl::half2 a, sycl::half2 b) {
+  sycl::float2 fa = half2_to_float2(a);
+  sycl::float2 fb = half2_to_float2(b);
+  return mul<sycl::float2, sycl::float2, sycl::float2>(fa, fb);
+}
+
+template <>
+inline sycl::float2 mul(sycl::half a, sycl::half2 b) {
+  return mul<sycl::float2, sycl::half2, sycl::half2>(h0_h0(a), b);
+}
+
+template <>
+inline Float4_ mul(sycl::half4 a, sycl::half4 b) {
+  Float4_ fc;
+  fc.x = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.x(), a.y()}, sycl::half2{b.x(), b.y()});
+  fc.y = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.z(), a.w()}, sycl::half2{b.z(), b.w()});
+  return fc;
+}
+
+template <>
+inline Float4_ mul(sycl::half a, sycl::half4 b) {
+  sycl::half2 s = h0_h0(a);
+  Float4_ fc;
+
+  fc.x =
+      mul<sycl::float2, sycl::half2, sycl::half2>(s, sycl::half2{b.x(), b.y()});
+  fc.y =
+      mul<sycl::float2, sycl::half2, sycl::half2>(s, sycl::half2{b.z(), b.w()});
+  return fc;
+}
+
+template <>
+inline Float8_ mul(sycl::half8 a, sycl::half8 b) {
+  Float8_ fc;
+  fc.x = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.s0(), a.s1()}, sycl::half2{b.s0(), b.s1()});
+  fc.y = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.s2(), a.s3()}, sycl::half2{b.s2(), b.s3()});
+  fc.z = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.s4(), a.s5()}, sycl::half2{b.s4(), b.s5()});
+  fc.w = mul<sycl::float2, sycl::half2, sycl::half2>(
+      sycl::half2{a.s6(), a.s7()}, sycl::half2{b.s6(), b.s7()});
+  return fc;
+}
+
+template <>
+inline Float8_ mul(sycl::half a, sycl::half8 b) {
+  sycl::half2 s = h0_h0(a);
+  Float8_ fc;
+  fc.x = mul<sycl::float2, sycl::half2, sycl::half2>(
+      s, sycl::half2{b.s0(), b.s1()});
+  fc.y = mul<sycl::float2, sycl::half2, sycl::half2>(
+      s, sycl::half2{b.s2(), b.s3()});
+  fc.z = mul<sycl::float2, sycl::half2, sycl::half2>(
+      s, sycl::half2{b.s4(), b.s5()});
+  fc.w = mul<sycl::float2, sycl::half2, sycl::half2>(
+      s, sycl::half2{b.s6(), b.s7()});
+  return fc;
+}
+
+// Vector fused multiply-add.
+inline sycl::half2 fma(sycl::half2 a, sycl::half2 b, sycl::half2 c) {
+  auto val = sycl_half_fma2((a), (b), (c));
+  return (val);
+}
+
+inline sycl::half2 fma(sycl::half a, sycl::half2 b, sycl::half2 c) {
+  return fma(h0_h0(a), b, c);
+}
+
+inline sycl::half4 fma(sycl::half4 a, sycl::half4 b, sycl::half4 c) {
+  sycl::half4 d;
+  d.x() = fma(a.x(), b.x(), c.x());
+  d.y() = fma(a.y(), b.y(), c.y());
+  d.z() = fma(a.z(), b.z(), c.z());
+  d.w() = fma(a.w(), b.w(), c.w());
+  return d;
+}
+
+inline sycl::half4 fma(sycl::half a, sycl::half4 b, sycl::half4 c) {
+  sycl::half4 s = sycl::half4{a, a, a, a};
+  return fma(s, b, c);
+}
+
+inline sycl::half8 fma(sycl::half8 a, sycl::half8 b, sycl::half8 c) {
+  sycl::half8 d;
+  d.s0() = fma(a.s0(), b.s0(), c.s0());
+  d.s1() = fma(a.s1(), b.s1(), c.s1());
+  d.s2() = fma(a.s2(), b.s2(), c.s2());
+  d.s3() = fma(a.s3(), b.s3(), c.s3());
+  d.s4() = fma(a.s4(), b.s4(), c.s4());
+  d.s5() = fma(a.s5(), b.s5(), c.s5());
+  d.s6() = fma(a.s6(), b.s6(), c.s6());
+  d.s7() = fma(a.s7(), b.s7(), c.s7());
+  return d;
+}
+
+inline sycl::half8 fma(sycl::half a, sycl::half8 b, sycl::half8 c) {
+  sycl::half8 d;
+  d.s0() = fma(a, b.s0(), c.s0());
+  d.s1() = fma(a, b.s1(), c.s1());
+  d.s2() = fma(a, b.s2(), c.s2());
+  d.s3() = fma(a, b.s3(), c.s3());
+  d.s4() = fma(a, b.s4(), c.s4());
+  d.s5() = fma(a, b.s5(), c.s5());
+  d.s6() = fma(a, b.s6(), c.s6());
+  d.s7() = fma(a, b.s7(), c.s7());
+  return d;
+}
+
+inline float fma(sycl::half a, sycl::half b, float fc) {
+  float fa = half_to_float(a);
+  float fb = half_to_float(b);
+  return sycl::fma(fa, fb, fc);
+}
+
+inline sycl::float2 fma(sycl::half2 a, sycl::half2 b, sycl::float2 fc) {
+  sycl::float2 fa = half2_to_float2(a);
+  sycl::float2 fb = half2_to_float2(b);
+  return fma(fa, fb, fc);
+}
+
+inline sycl::float2 fma(sycl::half a, sycl::half2 b, sycl::float2 fc) {
+  return fma(h0_h0(a), b, fc);
+}
+
+inline Float4_ fma(sycl::half4 a, sycl::half4 b, Float4_ fc) {
+  Float4_ fd;
+  fd.x = fma(sycl::half2{a.x(), a.y()}, sycl::half2{b.x(), b.y()}, fc.x);
+  fd.y = fma(sycl::half2{a.z(), a.w()}, sycl::half2{b.z(), b.w()}, fc.y);
+  return fd;
+}
+
+inline Float4_ fma(sycl::half a, sycl::half4 b, Float4_ fc) {
+  sycl::half4 s = sycl::half4{a, a, a, a};
+
+  return fma(s, b, fc);
+}
+
+inline Float8_ fma(sycl::half8 a, sycl::half8 b, Float8_ fc) {
+  Float8_ fd;
+  fd.x = fma(sycl::half2{a.s0(), a.s1()}, sycl::half2{b.s0(), b.s1()}, fc.x);
+  fd.y = fma(sycl::half2{a.s2(), a.s3()}, sycl::half2{b.s2(), b.s3()}, fc.y);
+  fd.z = fma(sycl::half2{a.s4(), a.s5()}, sycl::half2{b.s4(), b.s5()}, fc.z);
+  fd.w = fma(sycl::half2{a.s6(), a.s7()}, sycl::half2{b.s6(), b.s7()}, fc.w);
+  return fd;
+}
+
+inline Float8_ fma(sycl::half a, sycl::half8 b, Float8_ fc) {
+  sycl::half8 s = sycl::half8{a, a, a, a, a, a, a, a};
+
+  return fma(s, b, fc);
+}
+
+// Vector sum.
+template <>
+inline float sum(sycl::half v) {
+  return half_to_float(v);
+}
+
+template <>
+inline float sum(sycl::half2 v) {
+  sycl::float2 tmp = half2_to_float2(v);
+  return tmp.x() + tmp.y();
+}
+
+template <>
+inline float sum(sycl::half4 v) {
+  sycl::half2 c = add(sycl::half2{v.x(), v.y()}, sycl::half2{v.z(), v.w()});
+  return sum(c);
+}
+
+template <>
+inline float sum(sycl::half8 v) {
+  return add(
+      sum(sycl::half4{v.s0(), v.s1(), v.s2(), v.s3()}),
+      sum(sycl::half4{v.s4(), v.s5(), v.s6(), v.s7()}));
+}
+
+inline void from_float(sycl::half& dst, float src) {
+  dst = sycl::half(src);
+}
+
+inline void from_float(sycl::half2& dst, sycl::float2 src) {
+  dst = float2_to_half2(src);
+}
+
+inline void from_float(sycl::half4& dst, Float4_ src) {
+  sycl::half2 h0 = float2_to_half2(src.x);
+  sycl::half2 h1 = float2_to_half2(src.y);
+  dst.x() = h0.x();
+  dst.y() = h0.y();
+  dst.z() = h1.x();
+  dst.w() = h1.y();
+}
+
+inline void from_float(sycl::half8& dst, Float8_ src) {
+  dst.s0() = float2_to_half2(src.x).x();
+  dst.s1() = float2_to_half2(src.x).y();
+  dst.s2() = float2_to_half2(src.y).x();
+  dst.s3() = float2_to_half2(src.y).y();
+  dst.s4() = float2_to_half2(src.z).x();
+  dst.s5() = float2_to_half2(src.z).y();
+  dst.s6() = float2_to_half2(src.w).x();
+  dst.s7() = float2_to_half2(src.w).y();
+}
+
+// From float16 to float32.
+inline float to_float(sycl::half u) {
+  return half_to_float(u);
+}
+
+inline sycl::float2 to_float(sycl::half2 u) {
+  return half2_to_float2(u);
+}
+
+inline Float4_ to_float(sycl::half4 u) {
+  Float4_ tmp;
+  tmp.x = half2_to_float2(sycl::half2{u.x(), u.y()});
+  tmp.y = half2_to_float2(sycl::half2{u.z(), u.w()});
+  return tmp;
+}
+
+inline Float8_ to_float(sycl::half8 u) {
+  Float8_ tmp;
+  tmp.x = half2_to_float2(sycl::half2{u.s0(), u.s1()});
+  tmp.y = half2_to_float2(sycl::half2{u.s2(), u.s3()});
+  tmp.z = half2_to_float2(sycl::half2{u.s4(), u.s5()});
+  tmp.w = half2_to_float2(sycl::half2{u.s6(), u.s7()});
+  return tmp;
+}
+
+// Zero-out a variable.
+inline void zero(sycl::half& dst) {
+  dst = sycl::half(0);
+}
+
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/dtype_float32.h b/csrc/xpu/dtype_float32.h
new file mode 100644
index 000000000..7b70e4efc
--- /dev/null
+++ b/csrc/xpu/dtype_float32.h
@@ -0,0 +1,268 @@
+/*
+ * Adapted from https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention/decoder_masked_multihead_attention_template.hpp
+ * and https://github.com/NVIDIA/FasterTransformer/blob/release/v5.3_tag/src/fastertransformer/kernels/decoder_masked_multihead_attention_utils.h
+ * Copyright (c) 2023, The vLLM team.
+ * Copyright (c) 2020-2023, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "attention_generic.h"
+
+#include <stdint.h>
+
+namespace vllm {
+
+// Define custom FP32 vector data types.
+struct Float4_ {
+  sycl::float2 x;
+  sycl::float2 y;
+};
+
+struct Float8_ {
+  sycl::float2 x;
+  sycl::float2 y;
+  sycl::float2 z;
+  sycl::float2 w;
+};
+
+// FP32 vector types for Q, K, V.
+template<>
+struct Vec<float, 1> {
+  using Type = float;
+};
+template<>
+struct Vec<float, 2> {
+  using Type = sycl::float2;
+};
+template<>
+struct Vec<float, 4> {
+  using Type = sycl::float4;
+};
+
+// FP32 accumulator vector types corresponding to Vec.
+template<>
+struct FloatVec<float> {
+  using Type = float;
+};
+template <> struct FloatVec<sycl::float2> {
+  using Type = sycl::float2;
+};
+template <> struct FloatVec<sycl::float4> {
+  using Type = sycl::float4;
+};
+
+// Vector addition.
+inline float add(float a, float b) {
+  return a + b;
+}
+
+inline sycl::float2 add(sycl::float2 a, sycl::float2 b) {
+  sycl::float2 c;
+  c.x() = add(a.x(), b.x());
+  c.y() = add(a.y(), b.y());
+  return c;
+}
+
+inline sycl::float4 add(sycl::float4 a, sycl::float4 b) {
+  sycl::float4 c;
+  c.x() = add(a.x(), b.x());
+  c.y() = add(a.y(), b.y());
+  c.z() = add(a.z(), b.z());
+  c.w() = add(a.w(), b.w());
+  return c;
+}
+
+// Vector multiplication.
+template<>
+inline float mul<float, float>(float a, float b) {
+  return a * b;
+}
+
+template <> inline sycl::float2 mul(sycl::float2 a, sycl::float2 b) {
+  sycl::float2 c;
+  c.x() = a.x() * b.x();
+  c.y() = a.y() * b.y();
+  return c;
+}
+
+template <> inline sycl::float2 mul(float a, sycl::float2 b) {
+  sycl::float2 c;
+  c.x() = a * b.x();
+  c.y() = a * b.y();
+  return c;
+}
+
+template <> inline sycl::float4 mul(sycl::float4 a, sycl::float4 b) {
+  sycl::float4 c;
+  c.x() = a.x() * b.x();
+  c.y() = a.y() * b.y();
+  c.z() = a.z() * b.z();
+  c.w() = a.w() * b.w();
+  return c;
+}
+
+template <> inline sycl::float4 mul(float a, sycl::float4 b) {
+  sycl::float4 c;
+  c.x() = a * b.x();
+  c.y() = a * b.y();
+  c.z() = a * b.z();
+  c.w() = a * b.w();
+  return c;
+}
+
+// Vector fused multiply-add.
+inline float fma(float a, float b, float c) {
+  return a * b + c;
+}
+
+inline sycl::float2 fma(sycl::float2 a, sycl::float2 b, sycl::float2 c) {
+  sycl::float2 d;
+  d.x() = fma(a.x(), b.x(), c.x());
+  d.y() = fma(a.y(), b.y(), c.y());
+  return d;
+}
+
+inline sycl::float2 fma(float a, sycl::float2 b, sycl::float2 c) {
+  sycl::float2 d;
+  d.x() = fma(a, b.x(), c.x());
+  d.y() = fma(a, b.y(), c.y());
+  return d;
+}
+
+inline sycl::float4 fma(sycl::float4 a, sycl::float4 b, sycl::float4 c) {
+  sycl::float4 d;
+  d.x() = fma(a.x(), b.x(), c.x());
+  d.y() = fma(a.y(), b.y(), c.y());
+  d.z() = fma(a.z(), b.z(), c.z());
+  d.w() = fma(a.w(), b.w(), c.w());
+  return d;
+}
+
+inline sycl::float4 fma(float a, sycl::float4 b, sycl::float4 c) {
+  sycl::float4 d;
+  d.x() = fma(a, b.x(), c.x());
+  d.y() = fma(a, b.y(), c.y());
+  d.z() = fma(a, b.z(), c.z());
+  d.w() = fma(a, b.w(), c.w());
+  return d;
+}
+
+inline Float4_ fma(float a, Float4_ b, Float4_ c) {
+  Float4_ d;
+  d.x = fma(a, b.x, c.x);
+  d.y = fma(a, b.y, c.y);
+  return d;
+}
+
+inline Float8_ fma(float a, Float8_ b, Float8_ c) {
+  Float8_ d;
+  d.x = fma(a, b.x, c.x);
+  d.y = fma(a, b.y, c.y);
+  d.z = fma(a, b.z, c.z);
+  d.w = fma(a, b.w, c.w);
+  return d;
+}
+
+// Vector sum.
+template<>
+inline float sum(float v) {
+  return v;
+}
+
+template <> inline float sum(sycl::float2 v) {
+  return v.x() + v.y();
+}
+
+template <> inline float sum(sycl::float4 v) {
+  return v.x() + v.y() + v.z() + v.w();
+}
+
+template<>
+inline float sum(Float4_ v) {
+  return v.x.x() + v.x.y() + v.y.x() + v.y.y();
+}
+
+template<>
+inline float sum(Float8_ v) {
+  return v.x.x() + v.x.y() + v.y.x() + v.y.y() + v.z.x() + v.z.y() + v.w.x() +
+         v.w.y();
+}
+
+// Vector dot product.
+inline float dot(float a, float b) {
+  return a * b;
+}
+
+inline float dot(sycl::float2 a, sycl::float2 b) {
+  sycl::float2 c = mul<sycl::float2, sycl::float2, sycl::float2>(a, b);
+  return c.x() + c.y();
+}
+
+inline float dot(Float4_ a, Float4_ b) {
+  sycl::float2 acc = mul<sycl::float2, sycl::float2, sycl::float2>(a.x, b.x);
+  acc = fma(a.y, b.y, acc);
+  return acc.x() + acc.y();
+}
+
+inline float dot(Float8_ a, Float8_ b) {
+  sycl::float2 acc = mul<sycl::float2, sycl::float2, sycl::float2>(a.x, b.x);
+  acc = fma(a.y, b.y, acc);
+  acc = fma(a.z, b.z, acc);
+  acc = fma(a.w, b.w, acc);
+  return acc.x() + acc.y();
+}
+
+// From float to float.
+inline void from_float(float& dst, float src) {
+  dst = src;
+}
+
+inline void from_float(sycl::float2 &dst, sycl::float2 src) {
+  dst = src;
+}
+
+inline void from_float(sycl::float4 &dst, sycl::float4 src) {
+  dst = src;
+}
+
+// From float to float.
+inline float to_float(float u) {
+  return u;
+}
+
+inline sycl::float2 to_float(sycl::float2 u) {
+  return u;
+}
+
+inline sycl::float4 to_float(sycl::float4 u) {
+  return u;
+}
+
+inline Float4_ to_float(Float4_ u) {
+  return u;
+}
+
+inline Float8_ to_float(Float8_ u) {
+  return u;
+}
+
+// Zero-out a variable.
+inline void zero(float& dst) {
+  dst = 0.f;
+}
+
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/fused_moe.cpp b/csrc/xpu/fused_moe.cpp
new file mode 100644
index 000000000..3a39d0e13
--- /dev/null
+++ b/csrc/xpu/fused_moe.cpp
@@ -0,0 +1,269 @@
+#include "utils.h"
+#include "base.hpp"
+
+using ST = at::ScalarType;
+
+#include <sycl/sycl.hpp>
+#include "xpu_types.h"
+#include <torch/extension.h>
+
+template <typename T>
+__inline__ T silu_xpu(const T& x) {
+  // x * sigmoid(x)
+  return (T)(((float)x) / (1.0f + sycl::exp((float)-x)));
+}
+
+template <typename scalar_t>
+void silu_and_mul_kernel(
+    scalar_t* __restrict__ out, // [..., d]
+    const scalar_t* __restrict__ input, // [..., 2, d]
+    const int d,
+    const sycl::nd_item<3>& item_ct1) {
+  const int64_t token_idx = item_ct1.get_group(2);
+  for (int64_t idx = item_ct1.get_local_id(2); idx < d;
+       idx += item_ct1.get_local_range(2)) {
+    const scalar_t x = input[token_idx * 2 * d + idx];
+    const scalar_t y = input[token_idx * 2 * d + d + idx];
+    out[token_idx * d + idx] = silu_xpu(x) * y;
+  }
+}
+
+template <typename scalar_t>
+void call_silu_and_mul_kernel(
+    int num_tokens,
+    int d,
+    const scalar_t* __restrict__ input,
+    scalar_t* __restrict__ output) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(d, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+          silu_and_mul_kernel<sycl_t>(
+              (sycl_t*)output, (const sycl_t*)input, d, item_ct1);
+        });
+  });
+}
+
+void _silu_and_mul(torch::Tensor& out, torch::Tensor& input) {
+  int num_tokens = input.numel() / input.size(-1);
+  int d = input.size(-1) / 2;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_silu_and_mul_kernel", [&] {
+        call_silu_and_mul_kernel(
+            num_tokens,
+            d,
+            input.data_ptr<scalar_t>(),
+            out.data_ptr<scalar_t>());
+      });
+}
+
+template <typename IT, const int VS, const int GS, const int ES, const int QTYPE>
+static void moe_forward_kernel(
+    const void* input_ptr,
+    const int64_t* indexs,
+    const uint64_t* qweights,
+    void * output_ptr,
+    const int num_tokens,
+    const int state_size,
+    const int output_size,
+    at::Device device
+) {
+    static_assert(ES == 8 || ES == 16 || ES == 32);
+    assert(output_size % VS == 0);
+
+    const int nb = state_size / QK;
+    const int nsb = nb / SBS;
+
+    constexpr int BLOCK_SIZE = BLOCK_SIZES[QTYPE];
+    constexpr int SCALE_SIZE = SCALE_SIZES[QTYPE];
+
+    sycl::range<2> global_size(num_tokens, output_size / VS * GS);
+    sycl::range<2> local_size(1, GS);
+
+    auto cgf = [&](sycl::handler& handle) {
+        handle.parallel_for(
+            sycl::nd_range<2>(global_size, local_size),
+            [=](sycl::nd_item<2> item) SYCL_ESIMD_KERNEL {
+                slm_init<GS * VS * sizeof(float)>();
+
+                const int eid = item.get_global_id(0);
+                const int tid = item.get_local_id(1);
+                const int vid = item.get_group(1) * VS;
+
+                if (indexs[eid] >= 0) {
+                    const uint8_t* weight = (const uint8_t *)(qweights[indexs[eid]]);
+                    const uint8_t* scales = weight + (int64_t)output_size * nb * BLOCK_SIZE;
+                    const IT* input = static_cast<const IT *>(input_ptr) + eid * state_size;
+                    IT* output = static_cast<IT *>(output_ptr) + eid * output_size;
+
+                    const uint8_t * weight_base = weight + nb * BLOCK_SIZE * vid;
+                    const uint8_t * scale_base = scales + nb * SCALE_SIZE * vid;
+
+                    simd<IT, VS * ES> accvs{};
+
+                    for (int s = tid; s < nsb; s += GS) {
+                        simd<IT, SBS * QK> xvs = block_load<IT, SBS * QK>(input + s * SBS * QK);
+
+                        #pragma unroll
+                        for (int v = 0; v < VS; ++v) {
+                            simd<fp16, SBS * QK> yvs = load_qblocks<QTYPE>(
+                                weight_base + v * nb * BLOCK_SIZE + s * SBS * BLOCK_SIZE,
+                                scale_base + v * nb * SCALE_SIZE + s * SBS * SCALE_SIZE
+                            );
+
+                            #pragma unroll
+                            for (int i = 0; i < SBS * QK; i += ES) {
+                                accvs.template select<ES, 1>(v * ES) +=
+                                    xvs.template select<ES, 1>(i) *
+                                    yvs.template select<ES, 1>(i);
+                            }
+                        }
+                    }
+
+                    for (int b = nsb * SBS + tid; b < nb; b += GS) {
+                        simd<IT, QK> xv = block_load<IT, QK>(input + b * QK);
+
+                        #pragma unroll
+                        for (int v = 0; v < VS; ++v) {
+                            simd<fp16, QK> yv = load_qblock<QTYPE>(
+                                weight_base + v * nb * BLOCK_SIZE + b * BLOCK_SIZE,
+                                scale_base + v * nb * SCALE_SIZE + b * SCALE_SIZE
+                            );
+
+                            #pragma unroll
+                            for (int i = 0; i < QK; i += ES) {
+                                accvs.template select<ES, 1>(v * ES) +=
+                                    xv.template select<ES, 1>(i) *
+                                    yv.template select<ES, 1>(i);
+                            }
+                        }
+                    }
+
+                    simd<float, VS> accs;
+                    #pragma unroll
+                    for(int v = 0; v < VS; ++v) {
+                        accs[v] = sycl::ext::intel::esimd::detail::sum<float, IT, ES>(
+                            accvs.template select<ES, 1>(v * ES)
+                        );
+                    }
+
+                    slm_block_store<float, VS>(tid * VS * sizeof(float), accs);
+
+                    barrier();
+
+                    if (tid == 0) {
+                        #pragma unroll
+                        for (int i = 1; i < GS; ++i) {
+                            accs += slm_block_load<float, VS>(i * VS * sizeof(float));
+                        }
+
+                        block_store<IT, VS>(output + vid, accs);
+                    }
+                }
+
+                
+            }
+        );
+    };
+
+    utils::submit_kernel(cgf, device, "moe forward down kernel");
+}
+
+
+template <int QTYPE>
+static auto dispatch_moe_forward(ST scalar_t) {
+    switch (scalar_t) {
+        case ST::Float: return std::make_tuple(moe_forward_kernel<float, 4, 4, 16, QTYPE>);
+        case ST::Half: return std::make_tuple(moe_forward_kernel<fp16, 4, 4, 32, QTYPE>);
+        default: throw std::runtime_error("unsupported dtype, only fp32 and fp16 are supported");
+    }
+}
+
+
+torch::Tensor moe_forward(
+    torch::Tensor input,
+    torch::Tensor indexs,
+    torch::Tensor qweights_attr,
+    int64_t state_size,
+    int64_t output_size,
+    int64_t qtype
+) {
+    auto [func] = [&] () {
+        switch (qtype) {
+            case GGML_TYPE_Q4_0:
+                return dispatch_moe_forward<GGML_TYPE_Q4_0>(input.scalar_type());
+            case GGML_TYPE_Q4_0_WOQ:
+                return dispatch_moe_forward<GGML_TYPE_Q4_0_WOQ>(input.scalar_type());
+            case GGML_TYPE_FP8E5:
+                return dispatch_moe_forward<GGML_TYPE_FP8E5>(input.scalar_type());
+            default: throw std::runtime_error("unsupported qtype: " + std::to_string(qtype));
+        }
+    } ();
+
+    int64_t num_tokens = indexs.numel();
+
+    torch::Tensor output = torch::zeros({num_tokens, output_size},
+                                    torch::device(input.device()).dtype(input.dtype()));
+
+    func(
+        input.data_ptr(), indexs.data_ptr<int64_t>(),
+        qweights_attr.data_ptr<uint64_t>(), output.data_ptr(),
+        num_tokens, state_size, output_size, input.device()
+    );
+
+    return output;
+}
+
+
+torch::Tensor fused_moe_forward(
+    torch::Tensor input,
+    torch::Tensor indexs,
+    torch::Tensor qweights1_attr,
+    torch::Tensor qweights2_attr,
+    int64_t hidden_size,
+    int64_t intermediate_size,
+    int64_t qtype
+) {
+    auto [gmm_func] = [&] () {
+        switch (qtype) {
+            case GGML_TYPE_Q4_0:
+                return dispatch_moe_forward<GGML_TYPE_Q4_0>(input.scalar_type());
+            case GGML_TYPE_Q4_0_WOQ:
+                return dispatch_moe_forward<GGML_TYPE_Q4_0_WOQ>(input.scalar_type());
+            case GGML_TYPE_FP8E5:
+                return dispatch_moe_forward<GGML_TYPE_FP8E5>(input.scalar_type());
+            default: throw std::runtime_error("unsupported qtype: " + std::to_string(qtype));
+        }
+    } ();
+
+    int64_t num_tokens = indexs.numel();
+
+    torch::Tensor w1_output = torch::zeros({num_tokens, intermediate_size * 2},
+                                    torch::device(input.device()).dtype(input.dtype()));
+    
+    torch::Tensor tmp = torch::zeros({num_tokens, intermediate_size},
+                                    torch::device(input.device()).dtype(input.dtype()));
+    
+    torch::Tensor w2_output = torch::zeros({num_tokens, hidden_size},
+                                    torch::device(input.device()).dtype(input.dtype()));
+
+    gmm_func(
+        input.data_ptr(), indexs.data_ptr<int64_t>(),
+        qweights1_attr.data_ptr<uint64_t>(), w1_output.data_ptr(),
+        num_tokens, hidden_size, intermediate_size * 2, input.device()
+    );
+
+    _silu_and_mul(tmp, w1_output);
+
+    gmm_func(
+        tmp.data_ptr(), indexs.data_ptr<int64_t>(),
+        qweights2_attr.data_ptr<uint64_t>(), w2_output.data_ptr(),
+        num_tokens, intermediate_size, hidden_size, input.device()
+    );
+
+    return w2_output;
+}
diff --git a/csrc/xpu/gemm_kernels_xpu.cpp b/csrc/xpu/gemm_kernels_xpu.cpp
new file mode 100644
index 000000000..d96aa5880
--- /dev/null
+++ b/csrc/xpu/gemm_kernels_xpu.cpp
@@ -0,0 +1,125 @@
+/*
+Adapted from https://github.com/mit-han-lab/llm-awq
+@article{lin2023awq,
+  title={AWQ: Activation-aware Weight Quantization for LLM Compression and
+Acceleration}, author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang,
+Shang and Dang, Xingyu and Han, Song}, journal={arXiv}, year={2023}
+}
+ */
+
+#include <dpct/dpct.hpp>
+#include <sycl/sycl.hpp>
+#include <torch/extension.h>
+//#include <c10/cuda/CUDAGuard.h>
+#include "dequantize.h"
+#include "utils.h"
+#include "xpu_types.h"
+
+void awq_dequantize_impl(
+    int* __restrict__ input,
+    sycl::half* __restrict__ scaling_factors,
+    int* __restrict__ zeros,
+    sycl::half* __restrict__ output,
+    int G,
+    sycl::nd_item<3> item_ct1) {
+  int j_factors1 = 4;
+  int row_stride2 = 4;
+  int split_k_iters = 1;
+  sycl::half2 ZERO_HALF2{0, 0};
+  sycl::half input_shared[8];
+
+  int N = item_ct1.get_local_range(2) * item_ct1.get_group_range(2);
+  int col = item_ct1.get_group(2) * item_ct1.get_local_range(2) +
+      item_ct1.get_local_id(2);
+  int row = item_ct1.get_group(1) * item_ct1.get_local_range(1) +
+      item_ct1.get_local_id(1);
+  int index1 = 8 * col + 8 * row * N;
+  sycl::half* output_ptr2 = output + index1;
+
+  int index2 = col + row * N;
+  int* input_ptr2 = input + index2;
+
+  int index3 = col + (int)(row / G) * N;
+  int* zeros_ptr2 = zeros + index3;
+  int index4 = 8 * col + (int)(row / G) * N * 8;
+  sycl::half* scale_loaded = scaling_factors + index4;
+
+  uint32_t zeros_loaded = *(uint32_t*)(zeros_ptr2);
+  sycl::uint4 zero_loaded_u4 = vllm::awq::dequantize_s4_to_fp16x2(zeros_loaded);
+  // sycl::uint4 scale_loaded_u4 = *(sycl::uint4*)(scaling_factors_ptr2);
+  // int j = 0;
+
+  uint32_t input_loaded = *(uint32_t*)(input_ptr2);
+  sycl::uint4 input_loaded_fp16 =
+      vllm::awq::dequantize_s4_to_fp16x2(input_loaded);
+
+  sycl::half2* input_loaded_h2 = (sycl::half2*)(&input_loaded_fp16);
+  sycl::half2* zero_loaded_h2 = (sycl::half2*)(&zero_loaded_u4);
+  sycl::half2* scale_loaded_h2 = (sycl::half2*)scale_loaded;
+  for (int i = 0; i < 4; i++) {
+    input_loaded_h2[i] = sycl_half_sub2(input_loaded_h2[i], zero_loaded_h2[i]);
+    input_loaded_h2[i] =
+        sycl_half_fma2(input_loaded_h2[i], scale_loaded_h2[i], ZERO_HALF2);
+  }
+  *(sycl::uint4*)(input_shared) = input_loaded_fp16;
+
+  for (int i = 0; i < 8; ++i) {
+    *(output_ptr2 + i) = input_shared[i];
+  }
+}
+
+torch::Tensor awq_dequantize(
+    torch::Tensor _kernel,
+    torch::Tensor _scaling_factors,
+    torch::Tensor _zeros,
+    int split_k_iters,
+    int thx,
+    int thy) {
+  int in_c = _kernel.size(0);
+  int qout_c = _kernel.size(1);
+  int out_c = qout_c * 8;
+  int G = in_c / _scaling_factors.size(0);
+
+  int x_thread = thx;
+  int y_thread = thy;
+
+  int x_blocks = 1;
+  int y_blocks = 1;
+  if (thx == 0) {
+    x_thread = qout_c;
+  }
+  if (thy == 0) {
+    y_thread = in_c;
+  }
+  if (thx == 0 && thy == 0) {
+    x_thread = 8;
+    y_thread = 8;
+    x_blocks = (int)(qout_c / 8);
+    y_blocks = (int)(in_c / 8);
+  }
+
+  auto options = torch::TensorOptions()
+                     .dtype(_scaling_factors.dtype())
+                     .device(_scaling_factors.device());
+  at::Tensor _de_kernel = torch::empty({in_c, out_c}, options);
+  auto kernel = reinterpret_cast<int*>(_kernel.data_ptr<int>());
+  auto de_kernel =
+      reinterpret_cast<sycl::half*>(_de_kernel.data_ptr<at::Half>());
+  auto scaling_factors =
+      reinterpret_cast<sycl::half*>(_scaling_factors.data_ptr<at::Half>());
+  auto zeros = reinterpret_cast<int*>(_zeros.data_ptr<int>());
+
+  sycl::range<3> num_blocks(1, y_blocks, x_blocks);
+  sycl::range<3> threads_per_block(1, y_thread, x_thread);
+  auto& queue = vllm::xpu::vllmGetQueue();
+
+  queue.submit([&](sycl::handler& cgh) {
+    cgh.parallel_for(
+        sycl::nd_range<3>(num_blocks * threads_per_block, threads_per_block),
+        [=](sycl::nd_item<3> item_ct1) {
+          awq_dequantize_impl(
+              kernel, scaling_factors, zeros, de_kernel, G, item_ct1);
+        });
+  });
+  return _de_kernel;
+}
\ No newline at end of file
diff --git a/csrc/xpu/kv.h b/csrc/xpu/kv.h
new file mode 100644
index 000000000..9616ad7ef
--- /dev/null
+++ b/csrc/xpu/kv.h
@@ -0,0 +1,76 @@
+#pragma once
+
+#include <torch/extension.h>
+#include <ext/intel/esimd.hpp>
+
+using fp16 = sycl::half;
+
+constexpr uint8_t FP16_EXP_OFFSET = 15;
+constexpr uint8_t K_EXP_OFFSET = 9;
+constexpr uint8_t V_EXP_OFFSET = 12;
+constexpr uint8_t K_OFFSET = (FP16_EXP_OFFSET - K_EXP_OFFSET) << 3;
+constexpr uint8_t V_OFFSET = (FP16_EXP_OFFSET - V_EXP_OFFSET) << 3;
+constexpr uint16_t K_MAX =
+    (uint16_t)0x3FC0 + ((uint16_t)(FP16_EXP_OFFSET - K_EXP_OFFSET) << 10);
+constexpr uint16_t K_MIN =
+    (uint16_t)0x0040 + ((uint16_t)(FP16_EXP_OFFSET - K_EXP_OFFSET) << 10);
+constexpr uint16_t V_MAX =
+    (uint16_t)0x3FC0 + ((uint16_t)(FP16_EXP_OFFSET - V_EXP_OFFSET) << 10);
+constexpr uint16_t V_MIN =
+    (uint16_t)0x0040 + ((uint16_t)(FP16_EXP_OFFSET - V_EXP_OFFSET) << 10);
+
+template <const int HD>
+ESIMD_INLINE __ESIMD_NS::simd<uint8_t, HD> quantize_key_row(
+    __ESIMD_NS::simd<fp16, HD> key_row) {
+  const __ESIMD_NS::simd<fp16, HD> kmax = sycl::bit_cast<fp16, uint16_t>(K_MAX);
+  const __ESIMD_NS::simd<fp16, HD> kmin = sycl::bit_cast<fp16, uint16_t>(K_MIN);
+  __ESIMD_NS::simd<fp16, HD> key =
+      __ESIMD_NS::max(__ESIMD_NS::min(__ESIMD_NS::abs(key_row), kmax), kmin);
+  key.template bit_cast_view<uint16_t>() <<= 1;
+  __ESIMD_NS::simd<uint8_t, HD> sign =
+      key_row.template bit_cast_view<uint8_t>().template select<HD, 2>(1) &
+      (uint8_t)0x80;
+  return (key.template bit_cast_view<uint8_t>().template select<HD, 2>(1) -
+          K_OFFSET) |
+         sign;
+}
+
+template <const int HD>
+ESIMD_INLINE __ESIMD_NS::simd<uint8_t, HD> quantize_value_row(
+    __ESIMD_NS::simd<fp16, HD> value_row) {
+  const __ESIMD_NS::simd<fp16, HD> vmax = sycl::bit_cast<fp16, uint16_t>(V_MAX);
+  const __ESIMD_NS::simd<fp16, HD> vmin = sycl::bit_cast<fp16, uint16_t>(V_MIN);
+  __ESIMD_NS::simd<fp16, HD> value =
+      __ESIMD_NS::max(__ESIMD_NS::min(__ESIMD_NS::abs(value_row), vmax), vmin);
+  value.template bit_cast_view<uint16_t>() <<= 1;
+  __ESIMD_NS::simd<uint8_t, HD> sign =
+      value_row.template bit_cast_view<uint8_t>().template select<HD, 2>(1) &
+      (uint8_t)0x80;
+  return (value.template bit_cast_view<uint8_t>().template select<HD, 2>(1) -
+          V_OFFSET) |
+         sign;
+}
+
+template <const int HD>
+ESIMD_INLINE __ESIMD_NS::simd<fp16, HD> dequantize_key_row(
+    const __ESIMD_NS::simd<uint8_t, HD>& key_row) {
+  __ESIMD_NS::simd<uint16_t, HD> result = 0x80;
+  result.template bit_cast_view<uint8_t>().template select<HD, 2>(1) =
+      (key_row & (uint8_t)0x7F) + K_OFFSET;
+  result >>= 1;
+  __ESIMD_NS::simd<uint8_t, HD> sign = key_row & (uint8_t)0x80;
+  result.template bit_cast_view<uint8_t>().template select<HD, 2>(1) |= sign;
+  return result.template bit_cast_view<fp16>();
+}
+
+template <const int HD>
+ESIMD_INLINE __ESIMD_NS::simd<fp16, HD> dequantize_value_row(
+    const __ESIMD_NS::simd<uint8_t, HD>& value_row) {
+  __ESIMD_NS::simd<uint16_t, HD> result = 0x80;
+  result.template bit_cast_view<uint8_t>().template select<HD, 2>(1) =
+      (value_row & (uint8_t)0x7F) + V_OFFSET;
+  result >>= 1;
+  __ESIMD_NS::simd<uint8_t, HD> sign = value_row & (uint8_t)0x80;
+  result.template bit_cast_view<uint8_t>().template select<HD, 2>(1) |= sign;
+  return result.template bit_cast_view<fp16>();
+}
\ No newline at end of file
diff --git a/csrc/xpu/layernorm_xpu.cpp b/csrc/xpu/layernorm_xpu.cpp
new file mode 100644
index 000000000..9a6a2af0a
--- /dev/null
+++ b/csrc/xpu/layernorm_xpu.cpp
@@ -0,0 +1,188 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+
+#include <torch/extension.h>
+#include <algorithm>
+#include "utils.h"
+#include "xpu_types.h"
+#include "reduction_utils.h"
+
+namespace vllm {
+
+template <typename scalar_t>
+void rms_norm_kernel(
+    scalar_t* __restrict__ out, // [..., hidden_size]
+    const scalar_t* __restrict__ input, // [..., hidden_size]
+    const scalar_t* __restrict__ weight, // [hidden_size]
+    const float epsilon,
+    const int num_tokens,
+    const int hidden_size,
+    const sycl::nd_item<3>& item_ct1,
+    float* s_variance,
+    float* shared_vals) {
+  float variance = 0.0f;
+
+  for (int idx = item_ct1.get_local_id(2); idx < hidden_size;
+       idx += item_ct1.get_local_range(2)) {
+    const float x = (float)input[item_ct1.get_group(2) * hidden_size + idx];
+    variance += x * x;
+  }
+
+  variance = blockReduceSum<float>(variance, item_ct1, shared_vals);
+  if (item_ct1.get_local_id(2) == 0) {
+    *s_variance = sycl::rsqrt(variance / hidden_size + epsilon);
+  }
+
+  // item_ct1.barrier();
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  for (int idx = item_ct1.get_local_id(2); idx < hidden_size;
+       idx += item_ct1.get_local_range(2)) {
+    float x = (float)input[item_ct1.get_group(2) * hidden_size + idx];
+    out[item_ct1.get_group(2) * hidden_size + idx] =
+        ((scalar_t)(x * (*s_variance))) * weight[idx];
+  }
+}
+
+template <typename scalar_t>
+void call_rms_norm_kernel(
+    torch::Tensor& out,
+    torch::Tensor& input,
+    torch::Tensor& weight,
+    float epsilon) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int hidden_size = input.size(-1);
+  int num_tokens = input.numel() / hidden_size;
+  auto out_ptr = out.data_ptr<scalar_t>();
+  auto input_ptr = input.data_ptr<scalar_t>();
+  auto weight_ptr = weight.data_ptr<scalar_t>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(hidden_size, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    sycl::local_accessor<float, 1> shared_vals( sycl::range<1>(32), cgh);
+    sycl::local_accessor<float, 1> s_variance( sycl::range<1>(1), cgh);
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block),
+        [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {
+          rms_norm_kernel<sycl_t>(
+              (sycl_t*)out_ptr,
+              (const sycl_t*)input_ptr,
+              (const sycl_t*)weight_ptr,
+              epsilon,
+              num_tokens,
+              hidden_size,
+              item_ct1,
+              s_variance.get_pointer(),
+              shared_vals.get_pointer());
+        });
+  });
+}
+
+
+template <typename scalar_t>
+void fused_add_rms_norm_kernel(
+    scalar_t* __restrict__ input,   // [..., hidden_size]
+    scalar_t* __restrict__ residual,        // [..., hidden_size]
+    const scalar_t* __restrict__ weight, // [hidden_size]
+    const float epsilon,
+    const int num_tokens,
+    const int hidden_size,
+    const sycl::nd_item<3>& item_ct1,
+    float* s_variance,
+    float* shared_vals) {
+  float variance = 0.0f;
+
+  for (int idx = item_ct1.get_local_id(2); idx < hidden_size;
+       idx += item_ct1.get_local_range(2)) {
+    float x = (float)input[item_ct1.get_group(2) * hidden_size + idx];
+    x+=(float)residual[item_ct1.get_group(2) * hidden_size + idx];
+    variance += x * x;
+    residual[item_ct1.get_group(2) * hidden_size + idx] = (scalar_t)x;
+  }
+
+  variance = blockReduceSum<float>(variance, item_ct1, shared_vals);
+  if (item_ct1.get_local_id(2) == 0) {
+    *s_variance = sycl::rsqrt(variance / hidden_size + epsilon);
+  }
+
+  // item_ct1.barrier();
+  item_ct1.barrier(sycl::access::fence_space::local_space);
+
+  for (int idx = item_ct1.get_local_id(2); idx < hidden_size;
+       idx += item_ct1.get_local_range(2)) {
+    float x = (float)residual[item_ct1.get_group(2) * hidden_size + idx];
+    input[item_ct1.get_group(2) * hidden_size + idx] =
+        ((scalar_t)(x * (*s_variance))) * weight[idx];
+  }
+}
+
+template <typename scalar_t>
+void call_fused_add_rms_norm_kernel(
+    torch::Tensor& input,
+    torch::Tensor& residual,
+    torch::Tensor& weight,
+    float epsilon){
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  int hidden_size = input.size(-1);
+  int num_tokens = input.numel() / hidden_size;
+  auto input_ptr = input.data_ptr<scalar_t>();
+  auto residual_ptr = residual.data_ptr<scalar_t>();
+  auto weight_ptr = weight.data_ptr<scalar_t>();
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(hidden_size, 1024));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  queue.submit([&](sycl::handler& cgh) {
+    sycl::local_accessor<float, 1> shared_vals( sycl::range<1>(32), cgh);
+    sycl::local_accessor<float, 1> s_variance( sycl::range<1>(1), cgh);
+    cgh.parallel_for(
+        sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1)[[intel::reqd_sub_group_size(32)]] {
+          fused_add_rms_norm_kernel<sycl_t>(
+              (sycl_t*)input_ptr,
+              (sycl_t*)residual_ptr,
+              (const sycl_t*)weight_ptr,
+              epsilon,
+              num_tokens,
+              hidden_size,
+              item_ct1,
+              s_variance.get_pointer(),
+              shared_vals.get_pointer());
+        });
+  });
+}
+
+} // namespace vllm
+
+void rms_norm(
+    torch::Tensor& out,
+    torch::Tensor& input,
+    torch::Tensor& weight,
+    float epsilon) {
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_rms_norm_kernel", [&] {
+        vllm::call_rms_norm_kernel<scalar_t>(out, input, weight, epsilon);
+      });
+}
+
+void fused_add_rms_norm(
+    torch::Tensor& input,
+    torch::Tensor& residual,
+    torch::Tensor& weight,
+    float epsilon) {
+  int hidden_size = input.size(-1);
+  int num_tokens = input.numel() / hidden_size;
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      input.scalar_type(), "call_fused_add_rms_norm_kernel", [&] {
+        vllm::call_fused_add_rms_norm_kernel<scalar_t>(
+            input,
+            residual,
+            weight,
+               epsilon);
+      });
+}
+
diff --git a/csrc/xpu/pos_encoding_xpu.cpp b/csrc/xpu/pos_encoding_xpu.cpp
new file mode 100644
index 000000000..3232cacbc
--- /dev/null
+++ b/csrc/xpu/pos_encoding_xpu.cpp
@@ -0,0 +1,333 @@
+// clang-format off
+#ifdef VLLM_DEV
+#undef __SYCL_DEVICE_ONLY__
+#endif
+#include <sycl/sycl.hpp>
+// clang-format on
+#include "xpu_types.h"
+
+#include <torch/extension.h>
+#include "utils.h"
+
+template <typename scalar_t, bool IS_NEOX>
+inline void apply_rotary_embedding(
+    scalar_t* __restrict__ arr,
+    const scalar_t* __restrict__ cos_ptr,
+    const scalar_t* __restrict__ sin_ptr,
+    int rot_offset,
+    int embed_dim) {
+  int x_index, y_index;
+  scalar_t cos, sin;
+  if (IS_NEOX) {
+    // GPT-NeoX style rotary embedding.
+    x_index = rot_offset;
+    y_index = embed_dim + rot_offset;
+    cos = VLLM_LDG(cos_ptr + x_index);
+    sin = VLLM_LDG(sin_ptr + x_index);
+  } else {
+    // GPT-J style rotary embedding.
+    x_index = 2 * rot_offset;
+    y_index = 2 * rot_offset + 1;
+    cos = VLLM_LDG(cos_ptr + x_index / 2);
+    sin = VLLM_LDG(sin_ptr + x_index / 2);
+  }
+
+  const scalar_t x = arr[x_index];
+  const scalar_t y = arr[y_index];
+  arr[x_index] = x * cos - y * sin;
+  arr[y_index] = y * cos + x * sin;
+}
+
+template <typename scalar_t, bool IS_NEOX>
+void rotary_embedding_kernel(
+    const int64_t* __restrict__ positions, // [batch_size, seq_len] or
+                                           // [num_tokens]
+    scalar_t* __restrict__ query, // [batch_size, seq_len, num_heads, head_size]
+                                  // or [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key, // [batch_size, seq_len, num_kv_heads,
+                                // head_size] or [num_tokens, num_kv_heads,
+                                // head_size]
+    const scalar_t* __restrict__ cos_sin_cache, // [max_position, 2, rot_dim //
+                                                // 2]
+    const int rot_dim,
+    const int query_stride,
+    const int key_stride,
+    const int num_heads,
+    const int num_kv_heads,
+    const int head_size,
+    const sycl::nd_item<3>& item_ct1) {
+  // Each thread block is responsible for one token.
+  const int token_idx = item_ct1.get_group(2);
+  int64_t pos = positions[token_idx];
+  const scalar_t* cache_ptr = cos_sin_cache + pos * rot_dim;
+
+  const int embed_dim = rot_dim / 2;
+  const scalar_t* cos_ptr = cache_ptr;
+  const scalar_t* sin_ptr = cache_ptr + embed_dim;
+
+  const int nq = num_heads * embed_dim;
+  for (int i = item_ct1.get_local_id(2); i < nq;
+       i += item_ct1.get_local_range(2)) {
+    const int head_idx = i / embed_dim;
+    const int token_head = token_idx * query_stride + head_idx * head_size;
+    const int rot_offset = i % embed_dim;
+    apply_rotary_embedding<scalar_t, IS_NEOX>(
+        query + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);
+  }
+
+  const int nk = num_kv_heads * embed_dim;
+  for (int i = item_ct1.get_local_id(2); i < nk;
+       i += item_ct1.get_local_range(2)) {
+    const int head_idx = i / embed_dim;
+    const int token_head = token_idx * key_stride + head_idx * head_size;
+    const int rot_offset = i % embed_dim;
+    apply_rotary_embedding<scalar_t, IS_NEOX>(
+        key + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);
+  }
+}
+
+template <typename scalar_t, bool IS_NEOX>
+void batched_rotary_embedding_kernel(
+    const int64_t* __restrict__ positions, // [batch_size, seq_len] or
+                                           // [num_tokens]
+    scalar_t* __restrict__ query, // [batch_size, seq_len, num_heads, head_size]
+                                  // or [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key, // [batch_size, seq_len, num_kv_heads,
+                                // head_size] or [num_tokens, num_kv_heads,
+                                // head_size]
+    const scalar_t* __restrict__ cos_sin_cache, // [max_position, 2, rot_dim //
+                                                // 2]
+    const int64_t* __restrict__ cos_sin_cache_offsets,  // [batch_size, seq_len] or [num_tokens]
+    const int rot_dim,
+    const int query_stride,
+    const int key_stride,
+    const int num_heads,
+    const int num_kv_heads,
+    const int head_size,
+    const sycl::nd_item<3>& item_ct1) {
+  // Each thread block is responsible for one token.
+  const int token_idx = item_ct1.get_group(2);
+  int64_t cos_sin_cache_offset = cos_sin_cache_offsets[token_idx];
+  int64_t pos = positions[token_idx];
+  const scalar_t* cache_ptr = cos_sin_cache + (cos_sin_cache_offset + pos) * rot_dim;
+
+  const int embed_dim = rot_dim / 2;
+  const scalar_t* cos_ptr = cache_ptr;
+  const scalar_t* sin_ptr = cache_ptr + embed_dim;
+
+  const int nq = num_heads * embed_dim;
+  for (int i = item_ct1.get_local_id(2); i < nq;
+       i += item_ct1.get_local_range(2)) {
+    const int head_idx = i / embed_dim;
+    const int token_head = token_idx * query_stride + head_idx * head_size;
+    const int rot_offset = i % embed_dim;
+    apply_rotary_embedding<scalar_t, IS_NEOX>(
+        query + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);
+  }
+
+  const int nk = num_kv_heads * embed_dim;
+  for (int i = item_ct1.get_local_id(2); i < nk;
+       i += item_ct1.get_local_range(2)) {
+    const int head_idx = i / embed_dim;
+    const int token_head = token_idx * key_stride + head_idx * head_size;
+    const int rot_offset = i % embed_dim;
+    apply_rotary_embedding<scalar_t, IS_NEOX>(
+        key + token_head, cos_ptr, sin_ptr, rot_offset, embed_dim);
+  }
+}
+
+template <typename scalar_t>
+void call_rotary_embedding_kernel(
+    const int64_t* __restrict__ positions, // [num_tokens]
+    scalar_t* __restrict__ query, // [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key, // [num_tokens, num_kv_heads, head_size]
+    const scalar_t* __restrict__ cos_sin_cache, // [max_position, 2, rot_dim //
+                                                // 2]
+    const int rot_dim,
+    const int query_stride,
+    const int key_stride,
+    const int num_heads,
+    const int num_kv_heads,
+    const int head_size,
+    const int num_tokens,
+    const int sin_cos_dim,
+    bool is_neox) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * rot_dim / 2, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  if (is_neox) {
+    queue.submit([&](sycl::handler& cgh) {
+      cgh.parallel_for(
+          sycl::nd_range<3>(grid * block, block),
+          [=](sycl::nd_item<3> item_ct1) {
+            rotary_embedding_kernel<sycl_t, true>(
+                positions,
+                (sycl_t* __restrict__)query,
+                (sycl_t* __restrict__)key,
+                (const sycl_t* __restrict__)cos_sin_cache,
+                rot_dim,
+                query_stride,
+                key_stride,
+                num_heads,
+                num_kv_heads,
+                head_size,
+                item_ct1);
+          });
+    });
+  } else {
+    queue.submit([&](sycl::handler& cgh) {
+      cgh.parallel_for(
+          sycl::nd_range<3>(grid * block, block),
+          [=](sycl::nd_item<3> item_ct1) {
+            rotary_embedding_kernel<sycl_t, false>(
+                positions,
+                (sycl_t* __restrict__)query,
+                (sycl_t* __restrict__)key,
+                (const sycl_t* __restrict__)cos_sin_cache,
+                rot_dim,
+                query_stride,
+                key_stride,
+                num_heads,
+                num_kv_heads,
+                head_size,
+                item_ct1);
+          });
+    });
+  }
+}
+
+template <typename scalar_t>
+void call_batched_rotary_embedding_kernel(
+    const int64_t* __restrict__ positions, // [num_tokens]
+    scalar_t* __restrict__ query, // [num_tokens, num_heads, head_size]
+    scalar_t* __restrict__ key, // [num_tokens, num_kv_heads, head_size]
+    const scalar_t* __restrict__ cos_sin_cache, // [max_position, 2, rot_dim //
+                                                // 2]
+    const int64_t* __restrict__ cos_sin_cache_offsets,  // [batch_size, seq_len] or [num_tokens]
+    const int rot_dim,
+    const int query_stride,
+    const int key_stride,
+    const int num_heads,
+    const int num_kv_heads,
+    const int head_size,
+    const int num_tokens,
+    const int sin_cos_dim,
+    bool is_neox) {
+  using sycl_t = vllm::xpu::SyclTypeTrait<scalar_t>::Type;
+  sycl::range<3> grid(1, 1, num_tokens);
+  sycl::range<3> block(1, 1, std::min(num_heads * rot_dim / 2, 512));
+  auto& queue = vllm::xpu::vllmGetQueue();
+  if (is_neox) {
+    queue.submit([&](sycl::handler& cgh) {
+      cgh.parallel_for(
+          sycl::nd_range<3>(grid * block, block),
+          [=](sycl::nd_item<3> item_ct1) {
+            batched_rotary_embedding_kernel<sycl_t, true>(
+                positions,
+                (sycl_t* __restrict__)query,
+                (sycl_t* __restrict__)key,
+                (const sycl_t* __restrict__)cos_sin_cache,
+                cos_sin_cache_offsets,
+                rot_dim,
+                query_stride,
+                key_stride,
+                num_heads,
+                num_kv_heads,
+                head_size,
+                item_ct1);
+          });
+    });
+  } else {
+    queue.submit([&](sycl::handler& cgh) {
+      cgh.parallel_for(
+          sycl::nd_range<3>(grid * block, block),
+          [=](sycl::nd_item<3> item_ct1) {
+            batched_rotary_embedding_kernel<sycl_t, false>(
+                positions,
+                (sycl_t* __restrict__)query,
+                (sycl_t* __restrict__)key,
+                (const sycl_t* __restrict__)cos_sin_cache,
+                cos_sin_cache_offsets,
+                rot_dim,
+                query_stride,
+                key_stride,
+                num_heads,
+                num_kv_heads,
+                head_size,
+                item_ct1);
+          });
+    });
+  }
+}
+
+void rotary_embedding(
+    torch::Tensor& positions,
+    torch::Tensor& query,
+    torch::Tensor& key,
+    int head_size,
+    torch::Tensor& cos_sin_cache,
+    bool is_neox) {
+
+  int num_tokens = query.numel() / query.size(-1);
+  int rot_dim = cos_sin_cache.size(1);
+  int num_heads = query.size(-1) / head_size;
+  int num_kv_heads = key.size(-1) / head_size;
+  int key_stride = key.stride(-2);
+  int query_stride = query.stride(-2);
+  int cos_sin_dim = cos_sin_cache.size(0);
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+      query.scalar_type(), "call_rotary_embedding_kernel", [&] {
+        call_rotary_embedding_kernel<scalar_t>(
+            positions.data_ptr<int64_t>(),
+            query.data_ptr<scalar_t>(),
+            key.data_ptr<scalar_t>(),
+            cos_sin_cache.data_ptr<scalar_t>(),
+            rot_dim,
+            query_stride,
+            key_stride,
+            num_heads,
+            num_kv_heads,
+            head_size,
+            num_tokens,
+            cos_sin_dim,
+            is_neox);
+      });
+}
+
+void batched_rotary_embedding(
+  torch::Tensor& positions,
+  torch::Tensor& query,
+  torch::Tensor& key,
+  int head_size,
+  torch::Tensor& cos_sin_cache,
+  bool is_neox,
+  int rot_dim,
+  torch::Tensor& cos_sin_cache_offsets) {
+  int64_t num_tokens = cos_sin_cache_offsets.size(0);
+  int num_heads = query.size(-1) / head_size;
+  int num_kv_heads = key.size(-1) / head_size;
+  int key_stride = key.stride(-2);
+  int query_stride = query.stride(-2);
+  int cos_sin_dim = cos_sin_cache.size(0);
+
+  VLLM_XPU_DISPATCH_FLOATING_TYPES(
+    query.scalar_type(), "call_batched_rotary_embedding_kernel", [&] {
+      call_batched_rotary_embedding_kernel<scalar_t>(
+          positions.data_ptr<int64_t>(),
+          query.data_ptr<scalar_t>(),
+          key.data_ptr<scalar_t>(),
+          cos_sin_cache.data_ptr<scalar_t>(),
+          cos_sin_cache_offsets.data_ptr<int64_t>(),
+          rot_dim,
+          query_stride,
+          key_stride,
+          num_heads,
+          num_kv_heads,
+          head_size,
+          num_tokens,
+          cos_sin_dim,
+          is_neox);
+    });
+}
\ No newline at end of file
diff --git a/csrc/xpu/pybind.cpp b/csrc/xpu/pybind.cpp
new file mode 100644
index 000000000..bf9e94612
--- /dev/null
+++ b/csrc/xpu/pybind.cpp
@@ -0,0 +1,112 @@
+// #include "cache.h"
+#include "xpu_ops.h"
+#include <torch/extension.h>
+
+PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
+  // vLLM custom ops
+  pybind11::module ops = m.def_submodule("ops", "vLLM custom operators");
+
+  // Attention ops
+  ops.def(
+    "paged_attention_v1",
+    &paged_attention_v1,
+    "Compute the attention between an input query and the cached keys/values using PagedAttention.");
+  ops.def(
+    "paged_attention_v2",
+    &paged_attention_v2,
+    "PagedAttention V2.");
+
+  ops.def("context_attention_forward_v1", &context_attention_forward_v1,
+          "Context attention forward_v1");
+
+  ops.def("context_attention_forward_v2", &context_attention_forward_v2,
+          "Context attention forward_v2");
+
+  ops.def(
+    "paged_attention_gqa",
+    &paged_attention_gqa,
+    "PagedAttention GQA.");
+
+  ops.def("paged_attention_gqa_fp8", &paged_attention_gqa_fp8, "PagedAttention GQA fp8.");
+
+  // Activation ops
+  ops.def(
+    "silu_and_mul",
+    &silu_and_mul,
+    "Activation function used in SwiGLU.");
+  ops.def(
+    "gelu_and_mul",
+    &gelu_and_mul,
+    "Activation function used in GeGLU with `none` approximation.");
+  ops.def(
+    "gelu_tanh_and_mul",
+    &gelu_tanh_and_mul,
+    "Activation function used in GeGLU with `tanh` approximation.");
+  ops.def(
+    "gelu_new",
+    &gelu_new,
+    "GELU implementation used in GPT-2.");
+  ops.def(
+    "gelu_fast",
+    &gelu_fast,
+    "Approximate GELU implementation.");
+
+  // Layernorm
+  ops.def(
+    "rms_norm",
+    &rms_norm,
+    "Apply Root Mean Square (RMS) Normalization to the input tensor.");
+
+  ops.def(
+    "fused_add_rms_norm",
+    &fused_add_rms_norm,
+    "In-place fused Add and RMS Normalization");
+
+  // Rotary embedding
+  ops.def(
+    "rotary_embedding",
+    &rotary_embedding,
+    "Apply GPT-NeoX or GPT-J style rotary embedding to query and key");
+
+  // Cache ops
+  pybind11::module cache_ops = m.def_submodule("cache_ops", "vLLM cache ops");
+  cache_ops.def(
+    "swap_blocks",
+    &swap_blocks,
+    "Swap in (out) the cache blocks from src to dst");
+  cache_ops.def(
+    "copy_blocks",
+    &copy_blocks,
+    "Copy the cache blocks from src to dst");
+  cache_ops.def(
+    "reshape_and_cache",
+    &reshape_and_cache,
+    "Reshape the key and value tensors and cache them");
+  cache_ops.def(
+    "reshape_and_cache_ipexllm",
+    &reshape_and_cache_ipexllm,
+    "Reshape the key and value tensors and cache them for ipex_llm");
+
+  cache_ops.def(
+    "reshape_and_cache_ipexllm_fp8",
+    &reshape_and_cache_ipexllm_fp8,
+    "Reshape the key and value tensors and cache them for ipex_llm with fp8");
+
+  // Quant
+  ops.def(
+    "awq_dequantize",
+    &awq_dequantize,
+    "dequant method for awq");
+
+
+  ops.def(
+    "moe_forward",
+    &moe_forward,
+    "PagedAttention GQA.");
+
+  ops.def(
+    "fused_moe_forward",
+    &fused_moe_forward,
+    "PagedAttention GQA.");
+
+}
diff --git a/csrc/xpu/reduction_utils.h b/csrc/xpu/reduction_utils.h
new file mode 100644
index 000000000..93c64d759
--- /dev/null
+++ b/csrc/xpu/reduction_utils.h
@@ -0,0 +1,56 @@
+/*
+ * Copyright (c) 2023, The vLLM team.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <dpct/dpct.hpp>
+#include <stdint.h>
+#include <sycl/sycl.hpp>
+
+namespace vllm {
+
+template <typename T>
+__inline__ T warpReduceSum(T val, const sycl::nd_item<3>& item_ct1) {
+#pragma unroll
+  for (int mask = 16; mask > 0; mask >>= 1)
+    val += dpct::permute_sub_group_by_xor(
+        item_ct1.get_sub_group(), val, mask, 32);
+  return val;
+}
+
+/* Calculate the sum of all elements in a block */
+template<typename T>
+__inline__ T blockReduceSum(T val, const sycl::nd_item<3> &item_ct1, T *shared) {
+
+  int lane = item_ct1.get_local_id(2) & 0x1f;
+  int wid = item_ct1.get_local_id(2) >> 5;
+
+  val = warpReduceSum<T>(val, item_ct1);
+
+  if (lane == 0) {
+    shared[wid] = val;
+  }
+  item_ct1.barrier();
+
+  // Modify from blockDim.x << 5 to blockDim.x / 32. to prevent
+  // blockDim.x is not divided by 32
+  val = (item_ct1.get_local_id(2) < (item_ct1.get_local_range(2) / 32.f))
+            ? shared[lane]
+            : (T)(0.0f);
+  val = warpReduceSum<T>(val, item_ct1);
+  return val;
+}
+
+} // namespace vllm
\ No newline at end of file
diff --git a/csrc/xpu/utils.cpp b/csrc/xpu/utils.cpp
new file mode 100644
index 000000000..5f613af55
--- /dev/null
+++ b/csrc/xpu/utils.cpp
@@ -0,0 +1,34 @@
+#include "utils.h"
+#include <sycl/ext/intel/math.hpp>
+
+sycl::half sycl_half_mul(sycl::half a, sycl::half b) {
+  return sycl::ext::intel::math::hmul(a, b);
+}
+sycl::half sycl_half_add(sycl::half a, sycl::half b) {
+  return sycl::ext::intel::math::hadd(a, b);
+}
+sycl::half sycl_half_sub(sycl::half a, sycl::half b) {
+  return sycl::ext::intel::math::hsub(a, b);
+}
+sycl::half sycl_half_fma(sycl::half a, sycl::half b, sycl::half c) {
+  return sycl::ext::intel::math::hfma(a, b, c);
+}
+
+sycl::half2 sycl_half_mul2(sycl::half2 a, sycl::half2 b) {
+  return sycl::ext::intel::math::hmul2(a, b);
+}
+sycl::half2 sycl_half_add2(sycl::half2 a, sycl::half2 b) {
+  return sycl::ext::intel::math::hadd2(a, b);
+}
+sycl::half2 sycl_half_sub2(sycl::half2 a, sycl::half2 b) {
+  return sycl::ext::intel::math::hsub2(a, b);
+}
+
+sycl::half2 sycl_half_fma2(sycl::half2 a, sycl::half2 b, sycl::half2 c) {
+  return sycl::ext::intel::math::hfma2(a, b, c);
+}
+
+int get_max_shared_memory_per_block_device_attribute(int device_id) {
+  const sycl::device& device = vllm::xpu::vllmGetQueue().get_device();
+  return device.get_info<sycl::info::device::local_mem_size>();
+}
diff --git a/csrc/xpu/utils.h b/csrc/xpu/utils.h
new file mode 100644
index 000000000..fa3ead51c
--- /dev/null
+++ b/csrc/xpu/utils.h
@@ -0,0 +1,82 @@
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <functional>
+#include <memory>
+// #include <ipex.h>
+#include <ATen/ATen.h>
+#include <torch/torch.h>
+
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+#include <c10/xpu/XPUStream.h>
+#endif
+
+
+#define VLLM_LDG(arg) *(arg)
+namespace vllm {
+namespace xpu {
+
+static inline sycl::queue& vllmGetQueue() {
+  auto device_type = c10::DeviceType::XPU;
+  c10::impl::VirtualGuardImpl impl(device_type);
+  c10::Stream c10_stream = impl.getStream(c10::Device(device_type));
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+  return at::xpu::XPUStream(c10_stream).queue();
+#else
+  return ::xpu::get_queue_from_stream(c10_stream);
+#endif
+}
+template <typename T>
+struct SyclTypeTrait{
+  using Type = T;
+};
+
+template <>
+struct SyclTypeTrait<c10::Half>{
+  using Type = sycl::half;
+};
+
+template <>
+struct SyclTypeTrait<c10::BFloat16>{
+  using Type = sycl::ext::oneapi::bfloat16;
+};
+
+
+} // namespace xpu
+
+} // namespace vllm
+
+SYCL_EXTERNAL sycl::half sycl_half_mul(sycl::half a, sycl::half b);
+SYCL_EXTERNAL sycl::half sycl_half_add(sycl::half a, sycl::half b);
+SYCL_EXTERNAL sycl::half sycl_half_sub(sycl::half a, sycl::half b);
+SYCL_EXTERNAL sycl::half sycl_half_fma(sycl::half a, sycl::half b, sycl::half c);
+
+SYCL_EXTERNAL sycl::half2 sycl_half_mul2(sycl::half2 a, sycl::half2 b);
+SYCL_EXTERNAL sycl::half2 sycl_half_add2(sycl::half2 a, sycl::half2 b);
+SYCL_EXTERNAL sycl::half2 sycl_half_sub2(sycl::half2 a, sycl::half2 b);
+SYCL_EXTERNAL sycl::half2 sycl_half_fma2(sycl::half2 a, sycl::half2 b, sycl::half2 c);
+
+int get_max_shared_memory_per_block_device_attribute(int device_id);
+
+namespace utils {
+    static inline sycl::queue& get_queue(const at::Device& device) {
+        c10::impl::VirtualGuardImpl impl(device.type());
+        c10::Stream c10_stream = impl.getStream(c10::Device(device));
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+        return at::xpu::XPUStream(c10_stream).queue();
+#else
+        return ::xpu::get_queue_from_stream(c10_stream);
+#endif
+    }
+
+    static inline sycl::event submit_kernel(std::function<void(sycl::handler&)> kernel, const at::Device& device, const char * desc) {
+        sycl::queue& queue = get_queue(device);
+        sycl::event event = queue.submit(kernel);
+#if TORCH_VERSION_MAJOR >= 2 && TORCH_VERSION_MINOR >= 3
+        // xpu::profiler_record(desc, event);
+#else
+        ::xpu::profiler_record(desc, event);
+#endif
+        return event;
+    }
+}
diff --git a/csrc/xpu/xpu_ops.h b/csrc/xpu/xpu_ops.h
new file mode 100644
index 000000000..603d4f23d
--- /dev/null
+++ b/csrc/xpu/xpu_ops.h
@@ -0,0 +1,194 @@
+#pragma once
+#include <torch/extension.h>
+
+void rotary_embedding(torch::Tensor &positions, torch::Tensor &query,
+                          torch::Tensor &key, int head_size,
+                          torch::Tensor &cos_sin_cache, bool is_neox);
+void batched_rotary_embedding(
+  torch::Tensor& positions,
+  torch::Tensor& query,
+  torch::Tensor& key,
+  int head_size,
+  torch::Tensor& cos_sin_cache,
+  bool is_neox,
+  int rot_dim,
+  torch::Tensor& cos_sin_cache_offsets);
+
+void silu_and_mul(torch::Tensor &out, torch::Tensor &input);
+void gelu_and_mul(torch::Tensor &out, torch::Tensor &input);
+
+void gelu_new(torch::Tensor &out, torch::Tensor &input);
+
+void gelu_fast(torch::Tensor &out, torch::Tensor &input);
+
+
+void gelu_tanh_and_mul(
+  torch::Tensor& out,
+  torch::Tensor& input);
+
+void paged_attention_v1(
+    torch::Tensor &out, torch::Tensor &query, torch::Tensor &key_cache,
+    torch::Tensor &value_cache, int num_kv_heads, float scale,
+    torch::Tensor &block_tables, torch::Tensor &context_lens, int block_size,
+    int max_context_len, const c10::optional<torch::Tensor> &alibi_slopes,
+    const std::string& kv_cache_dtype, const float kv_scale, const float attn_logit_softcapping);
+
+void paged_attention_v2(
+    torch::Tensor &out, torch::Tensor &exp_sums, torch::Tensor &max_logits,
+    torch::Tensor &tmp_out, torch::Tensor &query, torch::Tensor &key_cache,
+    torch::Tensor &value_cache, int num_kv_heads, float scale,
+    torch::Tensor &block_tables, torch::Tensor &context_lens, int block_size,
+    int max_context_len, const c10::optional<torch::Tensor> &alibi_slopes,
+    const std::string& kv_cache_dtype, const float kv_scale, const float attn_logit_softcapping);
+
+torch::Tensor context_attention_forward_v1(
+    torch::Tensor query,  // [num_tokens, num_kv_head, head_dim]
+    torch::Tensor key,    // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor value,  // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor block_tables, torch::Tensor query_start_loc,
+    torch::Tensor seq_lens, torch::Tensor context_lens, int max_input_length,
+    int max_context_length);
+
+torch::Tensor context_attention_forward_v2(
+    torch::Tensor query,  // [num_tokens, num_kv_head, head_dim]
+    torch::Tensor key,    // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor value,  // [num_tokens, num_kv_heads * head_size]
+    torch::Tensor block_tables, torch::Tensor query_start_loc,
+    torch::Tensor seq_lens, torch::Tensor context_lens, int max_input_length,
+    int max_context_length, int max_q_length);
+
+void copy_blocks(
+    std::vector<torch::Tensor> &key_caches,
+    std::vector<torch::Tensor> &value_caches,
+    const std::map<int64_t, std::vector<int64_t>> &block_mapping);
+
+void reshape_and_cache(torch::Tensor &key, torch::Tensor &value,
+                           torch::Tensor &key_cache, torch::Tensor &value_cache,
+                           torch::Tensor &slot_mapping,
+                           const std::string& kv_cache_dtype, const float kv_scale);
+void reshape_and_cache_ipexllm(torch::Tensor &key, torch::Tensor &value,
+                           torch::Tensor &key_cache, torch::Tensor &value_cache,
+                           torch::Tensor &slot_mapping,
+                           const std::string& kv_cache_dtype, const float kv_scale);
+
+void reshape_and_cache_ipexllm_fp8(torch::Tensor& key, torch::Tensor& value,
+                                   torch::Tensor& key_cache,
+                                   torch::Tensor& value_cache,
+                                   torch::Tensor& slot_mapping,
+                                   const std::string& kv_cache_dtype,
+                                   const float kv_scale);
+
+void moe_align_block_size(
+  torch::Tensor topk_ids,
+  int num_experts,
+  int block_size,
+  torch::Tensor sorted_token_ids,
+  torch::Tensor experts_ids,
+  torch::Tensor num_tokens_post_pad) {
+  TORCH_CHECK(false, "moe_align_block_size is not supported on XPU.");
+}
+void swap_blocks(torch::Tensor &src, torch::Tensor &dst,
+                     const std::map<int64_t, int64_t> &block_mapping);
+
+void gather_cached_kv(torch::Tensor &key, torch::Tensor &value,
+                          torch::Tensor &key_cache, torch::Tensor &value_cache,
+                          torch::Tensor &slot_mapping);
+
+void convert_fp8_e5m2(torch::Tensor& src_cache, torch::Tensor& dst_cache) {
+  TORCH_CHECK(false, "Quantization is not supported on XPU.");
+}
+
+void rms_norm(torch::Tensor &out, torch::Tensor &input,
+                  torch::Tensor &weight, float epsilon);
+
+void fused_add_rms_norm(torch::Tensor &input, torch::Tensor &residual,
+                            torch::Tensor &weight, float epsilon);
+
+torch::Tensor awq_gemm(torch::Tensor _in_feats, torch::Tensor _kernel,
+                       torch::Tensor _scaling_factors, torch::Tensor _zeros,
+                       int split_k_iters) {
+  TORCH_CHECK(false, "awq_gemm is not supported on XPU.");                            
+}
+
+torch::Tensor marlin_gemm(
+    torch::Tensor& a, 
+    torch::Tensor& b_q_weight,
+    torch::Tensor& b_scales, 
+    torch::Tensor& workspace,
+    int64_t size_m, 
+    int64_t size_n, 
+    int64_t size_k) {
+  TORCH_CHECK(false, "marlin_gemm is not supported on XPU.");                            
+}
+
+torch::Tensor awq_dequantize(torch::Tensor _kernel, 
+    torch::Tensor _scaling_factors,
+    torch::Tensor _zeros,
+    int split_k_iters,
+    int thx,
+    int thy);
+
+void squeezellm_gemm(torch::Tensor vec, torch::Tensor mat,
+                         torch::Tensor mul, torch::Tensor lookup_table) {
+  TORCH_CHECK(false, "squeezellm_gemm is not supported on XPU.");
+}
+
+torch::Tensor gptq_gemm(
+  torch::Tensor a,
+  torch::Tensor b_q_weight,
+  torch::Tensor b_gptq_qzeros,
+  torch::Tensor b_gptq_scales,
+  torch::Tensor b_g_idx,
+  bool use_exllama,
+  int bit) {
+  TORCH_CHECK(false, "gptq_gemm is not supported on XPU.");
+}
+
+void gptq_shuffle(
+  torch::Tensor q_weight,
+  torch::Tensor q_perm,
+  int bit) {
+  TORCH_CHECK(false, "gptq_shuffle is not supported on XPU.");
+}
+
+void paged_attention_gqa(
+    torch::Tensor output,
+    torch::Tensor query,
+    torch::Tensor key_cache,
+    torch::Tensor value_cache,
+    int64_t bsz,
+    int64_t num_heads,
+    int64_t num_kv_heads,
+    float scale,
+    torch::Tensor& block_tables,
+    torch::Tensor& context_lens,
+    int block_size,
+    int64_t head_dim,
+    int max_seq_len
+);
+
+
+torch::Tensor moe_forward(
+    torch::Tensor input,
+    torch::Tensor indexs,
+    torch::Tensor qweights_attr,
+    int64_t state_size,
+    int64_t output_size,
+    int64_t qtype
+);
+
+torch::Tensor fused_moe_forward(
+    torch::Tensor input,
+    torch::Tensor indexs,
+    torch::Tensor qweights1_attr,
+    torch::Tensor qweights2_attr,
+    int64_t hidden_size,
+    int64_t intermediate_size,
+    int64_t qtype
+);
+void paged_attention_gqa_fp8(torch::Tensor output, torch::Tensor query,
+                         torch::Tensor key_cache, torch::Tensor value_cache,
+                         int64_t bsz, int64_t num_heads, int64_t num_kv_heads,
+                         float scale, torch::Tensor& block_tables,
+                         torch::Tensor& context_lens, int block_size,
+                         int64_t head_dim, int max_seq_len);
diff --git a/csrc/xpu/xpu_types.h b/csrc/xpu/xpu_types.h
new file mode 100644
index 000000000..23f5b805c
--- /dev/null
+++ b/csrc/xpu/xpu_types.h
@@ -0,0 +1,25 @@
+
+#ifndef XPU_TYPES_H
+#define XPU_TYPES_H
+
+#include <torch/extension.h>
+
+// FIXME: FP16 is not fully supported in Torch-CPU
+#define VLLM_XPU_DISPATCH_CASE_FLOATING_TYPES(...)     \
+  AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__) \
+  AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)  \
+  AT_DISPATCH_CASE(at::ScalarType::BFloat16, __VA_ARGS__)
+
+#define VLLM_XPU_DISPATCH_CASE_FLOATING_TYPES_FLOAT_ONLY(...) \
+  AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)        \
+  AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)
+
+#define VLLM_XPU_DISPATCH_FLOATING_TYPES(TYPE, NAME, ...) \
+  AT_DISPATCH_SWITCH(                                     \
+      TYPE, NAME, VLLM_XPU_DISPATCH_CASE_FLOATING_TYPES(__VA_ARGS__))
+
+#define VLLM_XPU_DISPATCH_FLOATING_TYPES_FLOAT_ONLY(TYPE, NAME, ...) \
+  AT_DISPATCH_SWITCH(                                     \
+      TYPE, NAME, VLLM_XPU_DISPATCH_CASE_FLOATING_TYPES_FLOAT_ONLY(__VA_ARGS__))
+
+#endif
\ No newline at end of file
diff --git a/docs/source/models/supported_models.md b/docs/source/models/supported_models.md
index bb318be98..833d3a44b 100644
--- a/docs/source/models/supported_models.md
+++ b/docs/source/models/supported_models.md
@@ -478,6 +478,16 @@ See [this page](#generative-models) for more information on how to use generativ
   * `Qwen/Qwen1.5-MoE-A2.7B`, `Qwen/Qwen1.5-MoE-A2.7B-Chat`, etc.
   *
   * ✅︎
+- * `Qwen3ForCausalLM`
+  * Qwen3
+  * `Qwen/Qwen3-8B`, etc.
+  * ✅︎
+  * ✅︎
+- * `Qwen3MoeForCausalLM`
+  * Qwen3MoE
+  * `Qwen/Qwen3-MoE-15B-A2B`, etc.
+  * ✅︎
+  * ✅︎
 - * `StableLmForCausalLM`
   * StableLM
   * `stabilityai/stablelm-3b-4e1t`, `stabilityai/stablelm-base-alpha-7b-v2`, etc.
diff --git a/examples/offline_inference/basic/basic.py b/examples/offline_inference/basic/basic.py
index 2ba5ec119..12268d3f0 100644
--- a/examples/offline_inference/basic/basic.py
+++ b/examples/offline_inference/basic/basic.py
@@ -13,7 +13,13 @@ prompts = [
 sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
 
 # Create an LLM.
-llm = LLM(model="facebook/opt-125m")
+llm = LLM(model="/llm/models/Llama-2-7b-chat-hf",
+          device="xpu",
+          dtype="float16",
+          enforce_eager=True,
+          tensor_parallel_size=1,
+          gpu_memory_utilization=0.9,
+          max_model_len=2048)
 # Generate texts from the prompts. The output is a list of RequestOutput objects
 # that contain the prompt, generated text, and other information.
 outputs = llm.generate(prompts, sampling_params)
diff --git a/setup.py b/setup.py
index b0cc2f481..bd8556b41 100755
--- a/setup.py
+++ b/setup.py
@@ -26,7 +26,7 @@ def load_module_from_path(module_name, path):
     spec.loader.exec_module(module)
     return module
 
-
+os.environ["SETUPTOOLS_SCM_PRETEND_VERSION"] = "0.8.3+ipexllm"
 ROOT_DIR = Path(__file__).parent
 logger = logging.getLogger(__name__)
 
@@ -147,6 +147,7 @@ class cmake_build_ext(build_ext):
         cmake_args = [
             '-DCMAKE_BUILD_TYPE={}'.format(cfg),
             '-DVLLM_TARGET_DEVICE={}'.format(VLLM_TARGET_DEVICE),
+            "-DCMAKE_CXX_STANDARD=17",
         ]
 
         verbose = envs.VERBOSE
@@ -432,7 +433,7 @@ def _no_device() -> bool:
 def _is_cuda() -> bool:
     has_cuda = torch.version.cuda is not None
     return (VLLM_TARGET_DEVICE == "cuda" and has_cuda
-            and not (_is_neuron() or _is_tpu() or _is_hpu()))
+            and not (_is_neuron() or _is_tpu() or _is_xpu()))
 
 
 def _is_hip() -> bool:
@@ -457,7 +458,11 @@ def _is_xpu() -> bool:
 
 
 def _build_custom_ops() -> bool:
-    return _is_cuda() or _is_hip() or _is_cpu()
+    return _is_cuda() or _is_hip() or _is_cpu() or _is_xpu()
+
+
+def _build_core_ext() -> bool:
+    return not (_is_neuron() or _is_tpu() or _is_xpu())
 
 
 def get_rocm_version():
@@ -634,6 +639,9 @@ def get_requirements() -> list[str]:
 
 ext_modules = []
 
+if _build_core_ext():
+    ext_modules.append(CMakeExtension(name="vllm._core_C"))
+
 if _is_cuda() or _is_hip():
     ext_modules.append(CMakeExtension(name="vllm._moe_C"))
 
diff --git a/tests/distributed/test_basic_distributed_correctness_enc_dec.py b/tests/distributed/test_basic_distributed_correctness_enc_dec.py
new file mode 100644
index 000000000..f00d5ef58
--- /dev/null
+++ b/tests/distributed/test_basic_distributed_correctness_enc_dec.py
@@ -0,0 +1,102 @@
+"""For encoder/decoder models only:
+Compare the outputs of HF and distributed vLLM when using greedy sampling.
+
+Run:
+```sh
+cd $VLLM_PATH/tests
+
+pytest distributed/test_basic_distributed_correctness_enc_dec.py
+```
+"""
+
+import pytest
+from transformers import AutoModelForSeq2SeqLM
+
+from vllm.utils import cuda_device_count_stateless
+
+from ..conftest import DecoderPromptType
+from ..models.utils import check_logprobs_close
+from ..utils import fork_new_process_for_each_test
+
+
+@pytest.mark.skipif(cuda_device_count_stateless() < 2,
+                    reason="Need at least 2 GPUs to run the test.")
+@pytest.mark.parametrize("model, distributed_executor_backend", [
+    ("facebook/bart-large-cnn", "ray"),
+    ("facebook/bart-large-cnn", "mp"),
+])
+@fork_new_process_for_each_test
+def test_models(
+    model: str,
+    distributed_executor_backend: str,
+    hf_runner,
+    vllm_runner,
+    example_encoder_decoder_prompts,
+) -> None:
+    '''
+    Test vLLM BART inference on more than one GPU, comparing
+    outputs against HF as a baseline.
+
+    Fork a new process for each test, to prevent CUDA from
+    being re-initialized by successive tests within the same
+    process.
+
+    Arguments:
+
+    * model: the HF ID of the specific BART variant under test
+    * distributed_executor_backend
+    * hf_runner: HuggingFace (HF) test model runner
+    * vllm_runner: vLLM test model runner
+    * example_encoder_decoder_prompts: test fixture which provides a 
+                                        dictionary of dummy prompts
+    '''
+
+    dtype = "float"
+    max_tokens = 64
+    num_logprobs = 5
+
+    # Example inputs with non-trivial (i.e. not None/empty) encoder &
+    # decoder prompts.
+    test_prompts = example_encoder_decoder_prompts[DecoderPromptType.CUSTOM]
+
+    # NOTE: take care of the order. run vLLM first, and then run HF.
+    # vLLM needs a fresh new process without cuda initialization.
+    # if we run HF first, the cuda initialization will be done and it
+    # will hurt multiprocessing backend with fork method (the default method).
+    with vllm_runner(
+            model,
+            dtype=dtype,
+            tensor_parallel_size=2,
+            distributed_executor_backend=distributed_executor_backend,
+            enforce_eager=True,
+    ) as vllm_model:
+        vllm_outputs = vllm_model.generate_encoder_decoder_greedy_logprobs(
+            test_prompts, max_tokens, num_logprobs)
+
+    # Configuration settings for HF baseline
+    hf_kwargs = {
+        "top_k": None,
+        "num_beams": 1,
+        "repetition_penalty": 1.0,
+        "top_p": 1.0,
+        "length_penalty": 1.0,
+        "early_stopping": False,
+        "no_repeat_ngram_size": None,
+        "min_length": 0
+    }
+
+    with hf_runner(model, dtype=dtype,
+                   auto_cls=AutoModelForSeq2SeqLM) as hf_model:
+        hf_outputs = (hf_model.generate_encoder_decoder_greedy_logprobs_limit(
+            test_prompts,
+            max_tokens,
+            num_logprobs,
+            **hf_kwargs,
+        ))
+
+    check_logprobs_close(
+        outputs_0_lst=hf_outputs,
+        outputs_1_lst=vllm_outputs,
+        name_0="hf",
+        name_1="vllm",
+    )
diff --git a/tests/models/decoder_only/language/test_granite.py b/tests/models/decoder_only/language/test_granite.py
index 119b79d64..ff8ba7b76 100644
--- a/tests/models/decoder_only/language/test_granite.py
+++ b/tests/models/decoder_only/language/test_granite.py
@@ -31,9 +31,11 @@ def test_models(
         hf_outputs = hf_model.generate_greedy_logprobs_limit(
             example_prompts, max_tokens, num_logprobs)
 
-    with vllm_runner(model, dtype=dtype) as vllm_model:
+    with vllm_runner(model, dtype=dtype,
+                     tokenizer_mode="mistral") as vllm_model:
         vllm_outputs = vllm_model.generate_greedy_logprobs(
             example_prompts, max_tokens, num_logprobs)
+
     check_logprobs_close(
         outputs_0_lst=hf_outputs,
         outputs_1_lst=vllm_outputs,
diff --git a/tests/models/registry.py b/tests/models/registry.py
index d508c5f44..e07f88311 100644
--- a/tests/models/registry.py
+++ b/tests/models/registry.py
@@ -202,6 +202,16 @@ _TEXT_GENERATION_EXAMPLE_MODELS = {
     "Qwen2ForCausalLM": _HfExamplesInfo("Qwen/Qwen2-7B-Instruct",
                                         extras={"2.5": "Qwen/Qwen2.5-7B-Instruct"}), # noqa: E501
     "Qwen2MoeForCausalLM": _HfExamplesInfo("Qwen/Qwen1.5-MoE-A2.7B-Chat"),
+    "Qwen3ForCausalLM": _HfExamplesInfo(
+        "Qwen/Qwen3-8B",
+        is_available_online=False,
+        min_transformers_version="4.51"
+    ),
+    "Qwen3MoeForCausalLM": _HfExamplesInfo(
+        "Qwen/Qwen3-MoE-15B-A2B",
+        is_available_online=False,
+        min_transformers_version="4.51"
+    ),
     "RWForCausalLM": _HfExamplesInfo("tiiuae/falcon-40b",
                                      is_available_online=False),
     "StableLMEpochForCausalLM": _HfExamplesInfo("stabilityai/stablelm-zephyr-3b",  # noqa: E501
diff --git a/tests/models/test_bart.py b/tests/models/test_bart.py
new file mode 100644
index 000000000..660b61d1a
--- /dev/null
+++ b/tests/models/test_bart.py
@@ -0,0 +1,170 @@
+"""Compare the outputs of HF and vLLM for BART models using greedy sampling.
+
+Run `pytest tests/models/test_bart.py`.
+"""
+from typing import List, Optional, Tuple
+
+from vllm.utils import is_cpu
+
+if not is_cpu():
+    # CPU backend is not currently supported with encoder/decoder models
+    # skip test definitions entirely to avoid importing GPU kernel libs
+    # (xFormers, etc.)
+
+    import pytest
+    from transformers import AutoModelForSeq2SeqLM
+
+    from vllm.sequence import SampleLogprobs
+
+    from ..conftest import DecoderPromptType
+    from .utils import check_logprobs_close
+
+    MODELS = ["facebook/bart-base", "facebook/bart-large-cnn"]
+
+    def vllm_to_hf_output(
+        vllm_output: Tuple[List[int], str, Optional[SampleLogprobs]],
+        decoder_prompt_type: DecoderPromptType,
+    ):
+        """Sanitize vllm output to be comparable with hf output."""
+        output_ids, output_str, out_logprobs = vllm_output
+
+        hf_output_str = output_str + "</s>"
+        if decoder_prompt_type == DecoderPromptType.NONE:
+            hf_output_str = "<s>" + hf_output_str
+
+        return output_ids, hf_output_str, out_logprobs
+
+    @pytest.mark.parametrize("model", MODELS)
+    @pytest.mark.parametrize("dtype", ["float", "bfloat16"])
+    @pytest.mark.parametrize("max_tokens", [64])
+    @pytest.mark.parametrize("num_logprobs", [5])
+    @pytest.mark.parametrize("decoder_prompt_type", list(DecoderPromptType))
+    def test_models(
+        hf_runner,
+        vllm_runner,
+        example_encoder_decoder_prompts,
+        model: str,
+        dtype: str,
+        max_tokens: int,
+        num_logprobs: int,
+        decoder_prompt_type: DecoderPromptType,
+    ) -> None:
+        '''
+        Test the vLLM BART model for a variety of encoder/decoder input prompts,
+        by validating it against HuggingFace (HF) BART.
+
+        Arguments:
+
+        * hf_runner: HuggingFace (HF) test model runner
+        * vllm_runner: vLLM test model runner
+        * example_encoder_decoder_prompts: test fixture which provides a 
+                                           dictionary of dummy prompts
+        * model: the HF ID of the specific BART variant under test
+        * dtype: the tensor datatype to employ
+        * max_tokens
+        * num_logprobs
+        * decoder_prompt_type: key into the example_encoder_decoder_prompts
+                               dictionary; selects specific encoder/decoder
+                               prompt scenarios to test
+
+        A note on using HF BART as a baseline for validating vLLM BART,
+        specifically when the decoder prompt is None. 
+        
+        The HF GenerationMixin's default behavior is to force the first
+        decoded token to be <BOS> if the prompt does not already contain
+        <BOS> (this is accomplished using a logit
+        processor setting.)
+        
+        So when we use HF BART as our baseline for comparison, note that
+        when the user provides a request with a None decoder prompt
+        (i.e. a singleton encoder prompt, or else an explicit encoder/
+        decoder prompt with the decoder sub-prompt set to None), HF and
+        vLLM handle this in different ways:
+        
+        * HF will (1) tokenize the None prompt as an empty token-list, 
+          (2) append <decoder-start-token> to the beginning, yielding
+          [<decoder-start-token>], (3) pass this token list to the model, and
+          then (4) after computing logits during prefill, override the model
+          logits & force <BOS> to be the first generated token.
+        
+        * vLLM will (1) tokenize the None prompt as [<BOS>], (2) append decoder-
+          start-token to the beginning, yielding [<decoder-start-token><BOS>],
+          (3) pass these tokens to the model & proceed with generation.
+        
+        The net effect is that compared to vLLM, the list of HF *decoded* tokens
+        will contain one more initial <BOS> than the vLLM generated tokens,
+        because vLLM's <BOS> token is injected into the prompt rather than into
+        the generated output. This is in spite of the fact that overall, the
+        complete sequences (prompt + decoded tokens) produced by vLLM will match
+        HF.
+        
+        So when we use HF decoded token output to validate vLLM's decoded token
+        output, the testing process must account for the difference in decoded
+        token sequences between vLLM and HF specifically in the
+        decoder-prompt-is-None case. 
+        
+        One option is to disable the logit processor feature that forces the
+        <BOS> token to be decoded (forced_bos_token_id = None), eliminating
+        the problem entirely. However this is not "normal" BART usage.
+        
+        The other option is - only in the decoder-prompt-is-None case - to
+        discard the first decoded token from the HF output before comparing it
+        to vLLM.
+
+        To that end, when testing the scenario where the decoder prompt is None
+        (and only in that one scenario), this test skips the first HF decoded
+        token during the process of validating the vLLM decoded output.
+        '''
+
+        test_case_prompts = example_encoder_decoder_prompts[
+            decoder_prompt_type]
+
+        # Configuration settings for HF baseline
+        hf_kwargs = {
+            "top_k": None,
+            "num_beams": 1,
+            "repetition_penalty": 1.0,
+            "top_p": 1.0,
+            "length_penalty": 1.0,
+            "early_stopping": False,
+            "no_repeat_ngram_size": None,
+            "min_length": 0
+        }
+
+        with hf_runner(model, dtype=dtype,
+                       auto_cls=AutoModelForSeq2SeqLM) as hf_model:
+            hf_outputs = (
+                hf_model.generate_encoder_decoder_greedy_logprobs_limit(
+                    test_case_prompts,
+                    max_tokens,
+                    num_logprobs,
+                    **hf_kwargs,
+                ))
+
+        # Note: currently encoder/decoder models are only compatible with
+        # enforce_eager=True. Normally this is not a problem because
+        # for encoder/decoder models vLLM will
+        # default to enforce_eager=True if enforce_eager
+        # is left unspecified. However, the
+        # VllmRunner test fixture (which wraps around the LLM class) defaults to
+        # enforce_eager=False (a behavior which a number of already-exisitng
+        # decoder-only unit tests expect), so when testing an encoder/decoder
+        # model we must explicitly specify enforce_eager=True in the VllmRunner
+        # constructor.
+        with vllm_runner(model, dtype=dtype, enforce_eager=True) as vllm_model:
+            vllm_outputs = vllm_model.generate_encoder_decoder_greedy_logprobs(
+                test_case_prompts, max_tokens, num_logprobs)
+
+        hf_skip_tokens = (1 if decoder_prompt_type == DecoderPromptType.NONE
+                          else 0)
+
+        check_logprobs_close(
+            outputs_0_lst=hf_outputs,
+            outputs_1_lst=[
+                vllm_to_hf_output(vllm_output, decoder_prompt_type)
+                for vllm_output in vllm_outputs
+            ],
+            name_0="hf",
+            name_1="vllm",
+            num_outputs_0_skip_tokens=hf_skip_tokens,
+        )
diff --git a/tests/models/test_gguf.py b/tests/models/test_gguf.py
new file mode 100644
index 000000000..196cd88e0
--- /dev/null
+++ b/tests/models/test_gguf.py
@@ -0,0 +1,90 @@
+"""
+Tests gguf models against unquantized models generations
+Note: To pass the test, quantization higher than Q4 should be used
+"""
+
+import os
+
+import pytest
+from huggingface_hub import hf_hub_download
+from transformers import AutoTokenizer
+
+from tests.quantization.utils import is_quant_method_supported
+
+from .utils import check_logprobs_close
+
+os.environ["TOKENIZERS_PARALLELISM"] = "true"
+
+MAX_MODEL_LEN = 1024
+
+# FIXME: Move this to confest
+MODELS = [
+    ("TinyLlama/TinyLlama-1.1B-Chat-v1.0",
+     hf_hub_download("TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
+                     filename="tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf")),
+    ("TinyLlama/TinyLlama-1.1B-Chat-v1.0",
+     hf_hub_download("duyntnet/TinyLlama-1.1B-Chat-v1.0-imatrix-GGUF",
+                     filename="TinyLlama-1.1B-Chat-v1.0-IQ4_XS.gguf")),
+    ("Qwen/Qwen2-1.5B-Instruct",
+     hf_hub_download("Qwen/Qwen2-1.5B-Instruct-GGUF",
+                     filename="qwen2-1_5b-instruct-q4_k_m.gguf")),
+    ("Qwen/Qwen2-1.5B-Instruct",
+     hf_hub_download("legraphista/Qwen2-1.5B-Instruct-IMat-GGUF",
+                     filename="Qwen2-1.5B-Instruct.IQ4_XS.gguf")),
+]
+
+
+@pytest.mark.skipif(not is_quant_method_supported("gguf"),
+                    reason="gguf is not supported on this GPU type.")
+@pytest.mark.parametrize("model", MODELS)
+@pytest.mark.parametrize("dtype", ["half"])
+@pytest.mark.parametrize("max_tokens", [32])
+@pytest.mark.parametrize("num_logprobs", [5])
+@pytest.mark.parametrize("tp_size", [1, 2])
+def test_models(
+    num_gpus_available,
+    vllm_runner,
+    example_prompts,
+    model,
+    dtype: str,
+    max_tokens: int,
+    num_logprobs: int,
+    tp_size: int,
+) -> None:
+    if num_gpus_available < tp_size:
+        pytest.skip(f"Not enough GPUs for tensor parallelism {tp_size}")
+
+    original_model, gguf_model = model
+
+    tokenizer = AutoTokenizer.from_pretrained(original_model)
+    messages = [[{
+        'role': 'user',
+        'content': prompt
+    }] for prompt in example_prompts]
+    example_prompts = tokenizer.apply_chat_template(messages,
+                                                    tokenize=False,
+                                                    add_generation_prompt=True)
+
+    # Run unquantized model.
+    with vllm_runner(model_name=original_model,
+                     dtype=dtype,
+                     max_model_len=MAX_MODEL_LEN,
+                     tensor_parallel_size=tp_size) as original_model:
+
+        original_outputs = original_model.generate_greedy_logprobs(
+            example_prompts[:-1], max_tokens, num_logprobs)
+
+    # Run gguf model.
+    with vllm_runner(model_name=gguf_model,
+                     dtype=dtype,
+                     max_model_len=MAX_MODEL_LEN,
+                     tensor_parallel_size=tp_size) as gguf_model:
+        gguf_outputs = gguf_model.generate_greedy_logprobs(
+            example_prompts[:-1], max_tokens, num_logprobs)
+
+    check_logprobs_close(
+        outputs_0_lst=original_outputs,
+        outputs_1_lst=gguf_outputs,
+        name_0="original",
+        name_1="gguf",
+    )
diff --git a/tests/models/test_granite.py b/tests/models/test_granite.py
new file mode 100644
index 000000000..2435b5dc3
--- /dev/null
+++ b/tests/models/test_granite.py
@@ -0,0 +1,49 @@
+"""Compare the outputs of HF and vLLM for Granite models using greedy sampling.
+
+Run `pytest tests/models/test_granite.py`.
+"""
+import importlib.metadata
+
+import pytest
+
+from .utils import check_logprobs_close
+
+TRANSFORMERS_VERSION = tuple(
+    map(int,
+        importlib.metadata.version("transformers").split(".")))
+
+MODELS = [
+    "ibm/PowerLM-3b",
+]
+
+
+# GraniteForCausalLM will be in transformers >= 4.45
+@pytest.mark.skipif(TRANSFORMERS_VERSION < (4, 45),
+                    reason="granite model test requires transformers >= 4.45")
+@pytest.mark.parametrize("model", MODELS)
+@pytest.mark.parametrize("dtype", ["bfloat16"])
+@pytest.mark.parametrize("max_tokens", [64])
+@pytest.mark.parametrize("num_logprobs", [5])
+def test_models(
+    hf_runner,
+    vllm_runner,
+    example_prompts,
+    model: str,
+    dtype: str,
+    max_tokens: int,
+    num_logprobs: int,
+) -> None:
+    # TODO(sang): Sliding window should be tested separately.
+    with hf_runner(model, dtype=dtype) as hf_model:
+        hf_outputs = hf_model.generate_greedy_logprobs_limit(
+            example_prompts, max_tokens, num_logprobs)
+
+    with vllm_runner(model, dtype=dtype) as vllm_model:
+        vllm_outputs = vllm_model.generate_greedy_logprobs(
+            example_prompts, max_tokens, num_logprobs)
+    check_logprobs_close(
+        outputs_0_lst=hf_outputs,
+        outputs_1_lst=vllm_outputs,
+        name_0="hf",
+        name_1="vllm",
+    )
diff --git a/tests/models/test_intern_vit.py b/tests/models/test_intern_vit.py
new file mode 100644
index 000000000..816f846f6
--- /dev/null
+++ b/tests/models/test_intern_vit.py
@@ -0,0 +1,79 @@
+from typing import Optional
+
+import pytest
+import torch
+import torch.nn as nn
+from huggingface_hub import snapshot_download
+from transformers import AutoConfig, AutoModel, CLIPImageProcessor
+
+from ..conftest import _ImageAssets, cleanup
+
+pytestmark = pytest.mark.vlm
+
+# we use snapshot_download to prevent conflicts between
+# dynamic_module and trust_remote_code for hf_runner
+DOWNLOAD_PATTERN = ["*.json", "*.py", "*.safetensors", "*.txt", "*.model"]
+models = [
+    snapshot_download("OpenGVLab/InternViT-300M-448px",
+                      allow_patterns=DOWNLOAD_PATTERN),
+    snapshot_download("OpenGVLab/InternViT-6B-448px-V1-5",
+                      allow_patterns=DOWNLOAD_PATTERN),
+]
+
+
+def run_intern_vit_test(
+    image_assets: _ImageAssets,
+    model: str,
+    *,
+    dtype: str,
+    distributed_executor_backend: Optional[str] = None,
+):
+    img_processor = CLIPImageProcessor.from_pretrained(model)
+    images = [asset.pil_image for asset in image_assets]
+    pixel_values = [
+        img_processor(images, return_tensors='pt').pixel_values.to(dtype)
+        for images in images
+    ]
+
+    config = AutoConfig.from_pretrained(model, trust_remote_code=True)
+    if not getattr(config, "norm_type", None):
+        config.norm_type = "rms_norm"
+
+    hf_model = AutoModel.from_pretrained(model,
+                                         torch_dtype=dtype,
+                                         trust_remote_code=True).to("cuda")
+    hf_outputs_per_image = [
+        hf_model(pixel_value.to("cuda")).last_hidden_state
+        for pixel_value in pixel_values
+    ]
+
+    from vllm.model_executor.models.intern_vit import InternVisionModel
+    vllm_model = InternVisionModel(config)
+    vllm_model.load_weights(hf_model.state_dict().items())
+
+    del hf_model
+    cleanup()
+
+    vllm_model = vllm_model.to("cuda", dtype)
+    vllm_outputs_per_image = [
+        vllm_model(pixel_values=pixel_value.to("cuda"))
+        for pixel_value in pixel_values
+    ]
+    del vllm_model
+    cleanup()
+
+    cos_similar = nn.CosineSimilarity(dim=-1)
+    for vllm_output, hf_output in zip(vllm_outputs_per_image,
+                                      hf_outputs_per_image):
+        assert cos_similar(vllm_output, hf_output).mean() > 0.99
+
+
+@pytest.mark.parametrize("model", models)
+@pytest.mark.parametrize("dtype", [torch.half])
+@torch.inference_mode()
+def test_models(dist_init, image_assets, model, dtype: str) -> None:
+    run_intern_vit_test(
+        image_assets,
+        model,
+        dtype=dtype,
+    )
diff --git a/tests/models/test_llava_image_embeds.py b/tests/models/test_llava_image_embeds.py
new file mode 100644
index 000000000..cc444fe32
--- /dev/null
+++ b/tests/models/test_llava_image_embeds.py
@@ -0,0 +1,160 @@
+from typing import List, Optional, Tuple, Type
+
+import pytest
+from transformers import AutoConfig, AutoModelForVision2Seq, AutoTokenizer
+
+from vllm.sequence import SampleLogprobs
+
+from ..conftest import IMAGE_ASSETS, HfRunner, VllmRunner, _ImageAssets
+from .utils import check_logprobs_close
+
+pytestmark = pytest.mark.vlm
+
+HF_IMAGE_PROMPTS = IMAGE_ASSETS.prompts({
+    "stop_sign":
+    "USER: <image>\nWhat's the content of the image?\nASSISTANT:",
+    "cherry_blossom":
+    "USER: <image>\nWhat is the season?\nASSISTANT:",
+})
+
+models = [
+    "llava-hf/llava-1.5-7b-hf",
+]
+
+
+def vllm_to_hf_output(vllm_output: Tuple[List[int], str,
+                                         Optional[SampleLogprobs]],
+                      model: str):
+    """Sanitize vllm output to be comparable with hf output."""
+    output_ids, output_str, out_logprobs = vllm_output
+
+    config = AutoConfig.from_pretrained(model)
+    image_token_id = config.image_token_index
+
+    tokenizer = AutoTokenizer.from_pretrained(model)
+    eos_token_id = tokenizer.eos_token_id
+
+    hf_output_ids = [
+        token_id for idx, token_id in enumerate(output_ids)
+        if token_id != image_token_id or output_ids[idx - 1] != image_token_id
+    ]
+
+    assert output_str[0] == " "
+    hf_output_str = output_str[1:]
+    if hf_output_ids[-1] == eos_token_id:
+        hf_output_str = hf_output_str + tokenizer.decode(eos_token_id)
+
+    return hf_output_ids, hf_output_str, out_logprobs
+
+
+def run_test(
+    hf_runner: Type[HfRunner],
+    vllm_runner: Type[VllmRunner],
+    image_assets: _ImageAssets,
+    model: str,
+    *,
+    size_factors: List[float],
+    dtype: str,
+    max_tokens: int,
+    num_logprobs: int,
+    tensor_parallel_size: int,
+    distributed_executor_backend: Optional[str] = None,
+):
+    """Inference result should be the same between hf and vllm.
+
+    All the image fixtures for the test is under tests/images.
+    For huggingface runner, we provide the PIL images as input.
+    For vllm runner, we provide MultiModalDataDict objects 
+    and corresponding vision language config as input.
+    Note, the text input is also adjusted to abide by vllm contract.
+    The text output is sanitized to be able to compare with hf.
+    """
+
+    # vLLM to load from image embeddings
+    vllm_images = [asset.image_embeds for asset in image_assets]
+
+    # transformers to load from PIL images
+    hf_images = [asset.pil_image for asset in image_assets]
+
+    vllm_inputs_per_image = [(
+        [prompt for _ in size_factors],
+        [image for _ in size_factors],
+    ) for image, prompt in zip(vllm_images, HF_IMAGE_PROMPTS)]
+
+    hf_inputs_per_image = [(
+        [prompt for _ in size_factors],
+        [image for _ in size_factors],
+    ) for image, prompt in zip(hf_images, HF_IMAGE_PROMPTS)]
+
+    # NOTE: take care of the order. run vLLM first, and then run HF.
+    # vLLM needs a fresh new process without cuda initialization.
+    # if we run HF first, the cuda initialization will be done and it
+    # will hurt multiprocessing backend with fork method (the default method).
+
+    # max_model_len should be greater than image_feature_size
+    with vllm_runner(model,
+                     dtype=dtype,
+                     tensor_parallel_size=tensor_parallel_size,
+                     distributed_executor_backend=distributed_executor_backend,
+                     enforce_eager=True) as vllm_model:
+        vllm_outputs_per_image = [
+            vllm_model.generate_greedy_logprobs(prompts,
+                                                max_tokens,
+                                                num_logprobs=num_logprobs,
+                                                images=images)
+            for prompts, images in vllm_inputs_per_image
+        ]
+
+    with hf_runner(model, dtype=dtype,
+                   auto_cls=AutoModelForVision2Seq) as hf_model:
+        hf_outputs_per_image = [
+            hf_model.generate_greedy_logprobs_limit(prompts,
+                                                    max_tokens,
+                                                    num_logprobs=num_logprobs,
+                                                    images=images)
+            for prompts, images in hf_inputs_per_image
+        ]
+
+    for hf_outputs, vllm_outputs in zip(hf_outputs_per_image,
+                                        vllm_outputs_per_image):
+        # TODO: Check whether using original CLIPVisionModel can improve
+        # consistency against HF
+        check_logprobs_close(
+            outputs_0_lst=hf_outputs,
+            outputs_1_lst=[
+                vllm_to_hf_output(vllm_output, model)
+                for vllm_output in vllm_outputs
+            ],
+            name_0="hf",
+            name_1="vllm",
+        )
+
+
+@pytest.mark.parametrize("model", models)
+@pytest.mark.parametrize(
+    "size_factors",
+    [
+        # No image
+        [],
+        # Single-scale
+        [1.0],
+        # Single-scale, batched
+        [1.0, 1.0, 1.0],
+    ],
+)
+@pytest.mark.parametrize("dtype", ["half"])
+@pytest.mark.parametrize("max_tokens", [128])
+@pytest.mark.parametrize("num_logprobs", [5])
+def test_models(hf_runner, vllm_runner, image_assets, model, size_factors,
+                dtype: str, max_tokens: int, num_logprobs: int) -> None:
+    run_test(
+        hf_runner,
+        vllm_runner,
+        image_assets,
+        model,
+        size_factors=size_factors,
+        dtype=dtype,
+        max_tokens=max_tokens,
+        num_logprobs=num_logprobs,
+        tensor_parallel_size=1,
+    )
diff --git a/tests/models/test_phimoe.py b/tests/models/test_phimoe.py
new file mode 100644
index 000000000..2fb2eecc9
--- /dev/null
+++ b/tests/models/test_phimoe.py
@@ -0,0 +1,111 @@
+"""Compare the outputs of HF and vLLM for moe models using greedy sampling.
+
+Run `pytest tests/models/test_phimoe.py`.
+"""
+import pytest
+import torch
+
+from vllm.utils import is_cpu
+
+from .utils import check_logprobs_close
+
+MODELS = [
+    "microsoft/Phi-3.5-MoE-instruct",
+]
+
+
+def test_phimoe_routing_function():
+    from vllm.model_executor.models.phimoe import phimoe_routing_function
+    test_case = {
+        0: {
+            "hidden_states":
+            torch.tensor([1, 2, 3, 4, 5, 6, 7, 8],
+                         dtype=torch.float32,
+                         requires_grad=False).view(4, 2),
+            "gating_output":
+            torch.tensor([0.1, 0.2, 0.3, 0.4],
+                         dtype=torch.float32,
+                         requires_grad=False),
+            "topk":
+            2,
+            "renormalize":
+            False,
+        },
+        1: {
+            "hidden_states":
+            torch.tensor([1, 2, 3, 4, 5, 6, 7, 8],
+                         dtype=torch.float32,
+                         requires_grad=False).view(4, 2),
+            "gating_output":
+            torch.tensor([0.4, 0.2, 0.3, 0.4],
+                         dtype=torch.float32,
+                         requires_grad=False),
+            "topk":
+            2,
+            "renormalize":
+            False,
+        }
+    }
+
+    ground_truth = {
+        0: {
+            "topk_weights":
+            torch.tensor([1., 1.], dtype=torch.float32, requires_grad=False),
+            "topk_ids":
+            torch.tensor([3, 2], dtype=torch.long, requires_grad=False),
+        },
+        1: {
+            "topk_weights":
+            torch.tensor([0.5, 1.], dtype=torch.float32, requires_grad=False),
+            "topk_ids":
+            torch.tensor([0, 3], dtype=torch.long, requires_grad=False),
+        }
+    }
+
+    for test_id in test_case:
+        topk_weights, topk_ids = phimoe_routing_function(**test_case[test_id])
+        assert torch.allclose(topk_weights,
+                              ground_truth[test_id]["topk_weights"])
+        assert torch.equal(topk_ids, ground_truth[test_id]["topk_ids"])
+
+
+def get_gpu_memory():
+    try:
+        props = torch.cuda.get_device_properties(torch.cuda.current_device())
+        gpu_memory = props.total_memory / (1024**3)
+        return gpu_memory
+    except Exception:
+        return 0
+
+
+@pytest.mark.skipif(condition=is_cpu(),
+                    reason="This test takes a lot time to run on CPU, "
+                    "and vllm CI's disk space is not enough for this model.")
+@pytest.mark.skipif(condition=get_gpu_memory() < 100,
+                    reason="Skip this test if GPU memory is insufficient.")
+@pytest.mark.parametrize("model", MODELS)
+@pytest.mark.parametrize("dtype", ["bfloat16"])
+@pytest.mark.parametrize("max_tokens", [64])
+@pytest.mark.parametrize("num_logprobs", [5])
+def test_models(
+    hf_runner,
+    vllm_runner,
+    example_prompts,
+    model: str,
+    dtype: str,
+    max_tokens: int,
+    num_logprobs: int,
+) -> None:
+    with hf_runner(model, dtype=dtype) as hf_model:
+        hf_outputs = hf_model.generate_greedy_logprobs_limit(
+            example_prompts, max_tokens, num_logprobs)
+
+    with vllm_runner(model, dtype=dtype) as vllm_model:
+        vllm_outputs = vllm_model.generate_greedy_logprobs(
+            example_prompts, max_tokens, num_logprobs)
+    check_logprobs_close(
+        outputs_0_lst=hf_outputs,
+        outputs_1_lst=vllm_outputs,
+        name_0="hf",
+        name_1="vllm",
+    )
diff --git a/tests/models/test_qwen.py b/tests/models/test_qwen.py
new file mode 100644
index 000000000..0f974fcc1
--- /dev/null
+++ b/tests/models/test_qwen.py
@@ -0,0 +1,48 @@
+from typing import Type
+
+import pytest
+
+from ..conftest import HfRunner, VllmRunner
+from .utils import check_logprobs_close
+
+models = ["qwen/qwen-vl"]
+
+
+@pytest.mark.parametrize("dtype", ["half"])
+@pytest.mark.parametrize("max_tokens", [32])
+@pytest.mark.parametrize("num_logprobs", [5])
+@pytest.mark.parametrize("model", models)
+def test_text_only_qwen_model(
+    hf_runner: Type[HfRunner],
+    vllm_runner: Type[VllmRunner],
+    example_prompts,
+    model: str,
+    *,
+    dtype: str,
+    max_tokens: int,
+    num_logprobs: int,
+):
+    # This test checks language inputs only, since the visual component
+    # for qwen-vl is still unsupported in VLLM. In the near-future, the
+    # implementation and this test will be extended to consider
+    # visual inputs as well.
+    with hf_runner(model, dtype=dtype) as hf_model:
+        hf_outputs = hf_model.generate_greedy_logprobs_limit(
+            example_prompts,
+            max_tokens,
+            num_logprobs=num_logprobs,
+        )
+
+    with vllm_runner(model, dtype=dtype) as vllm_model:
+        vllm_outputs = vllm_model.generate_greedy_logprobs(
+            example_prompts,
+            max_tokens,
+            num_logprobs=num_logprobs,
+        )
+
+    check_logprobs_close(
+        outputs_0_lst=hf_outputs,
+        outputs_1_lst=vllm_outputs,
+        name_0="hf",
+        name_1="vllm",
+    )
diff --git a/tests/models/test_ultravox.py b/tests/models/test_ultravox.py
new file mode 100644
index 000000000..e98db9b65
--- /dev/null
+++ b/tests/models/test_ultravox.py
@@ -0,0 +1,202 @@
+from typing import List, Optional, Tuple, Type
+
+import numpy as np
+import pytest
+from transformers import AutoModel, AutoTokenizer, BatchEncoding
+
+from vllm.sequence import SampleLogprobs
+from vllm.utils import STR_DTYPE_TO_TORCH_DTYPE
+
+from ..conftest import HfRunner, VllmRunner
+from .utils import check_logprobs_close
+
+pytestmark = pytest.mark.vlm
+
+MODEL_NAME = "fixie-ai/ultravox-v0_3"
+
+AudioTuple = Tuple[np.ndarray, int]
+
+VLLM_PLACEHOLDER = "<|reserved_special_token_0|>"
+HF_PLACEHOLDER = "<|audio|>"
+
+
+@pytest.fixture(scope="session")
+def audio_assets():
+    from vllm.assets.audio import AudioAsset
+    return [AudioAsset("mary_had_lamb"), AudioAsset("winning_call")]
+
+
+@pytest.fixture(scope="module", params=("mary_had_lamb", "winning_call"))
+def audio(request):
+    from vllm.assets.audio import AudioAsset
+    return AudioAsset(request.param)
+
+
+def _get_prompt(audio_count, question, placeholder):
+    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
+    placeholder = f"{placeholder}\n" * audio_count
+
+    return tokenizer.apply_chat_template([{
+        'role': 'user',
+        'content': f"{placeholder}{question}"
+    }],
+                                         tokenize=False,
+                                         add_generation_prompt=True)
+
+
+def vllm_to_hf_output(vllm_output: Tuple[List[int], str,
+                                         Optional[SampleLogprobs]],
+                      model: str):
+    """Sanitize vllm output to be comparable with hf output."""
+    output_ids, output_str, out_logprobs = vllm_output
+
+    tokenizer = AutoTokenizer.from_pretrained(model)
+    eos_token_id = tokenizer.eos_token_id
+
+    hf_output_ids = output_ids[:]
+    hf_output_str = output_str
+    if hf_output_ids[-1] == eos_token_id:
+        hf_output_str = hf_output_str + tokenizer.decode(eos_token_id)
+
+    return hf_output_ids, hf_output_str, out_logprobs
+
+
+def run_test(
+    hf_runner: Type[HfRunner],
+    vllm_runner: Type[VllmRunner],
+    prompts_and_audios: List[Tuple[str, str, AudioTuple]],
+    model: str,
+    *,
+    dtype: str,
+    max_tokens: int,
+    num_logprobs: int,
+    tensor_parallel_size: int,
+    distributed_executor_backend: Optional[str] = None,
+):
+    """Inference result should be the same between hf and vllm."""
+    torch_dtype = STR_DTYPE_TO_TORCH_DTYPE[dtype]
+
+    # NOTE: take care of the order. run vLLM first, and then run HF.
+    # vLLM needs a fresh new process without cuda initialization.
+    # if we run HF first, the cuda initialization will be done and it
+    # will hurt multiprocessing backend with fork method (the default method).
+
+    with vllm_runner(model,
+                     dtype=dtype,
+                     tensor_parallel_size=tensor_parallel_size,
+                     distributed_executor_backend=distributed_executor_backend,
+                     enforce_eager=True) as vllm_model:
+        vllm_outputs_per_audio = [
+            vllm_model.generate_greedy_logprobs([vllm_prompt],
+                                                max_tokens,
+                                                num_logprobs=num_logprobs,
+                                                audios=[audio])
+            for vllm_prompt, _, audio in prompts_and_audios
+        ]
+
+    def process(hf_inputs: BatchEncoding):
+        hf_inputs["audio_values"] = hf_inputs["audio_values"] \
+            .to(torch_dtype)  # type: ignore
+        return hf_inputs
+
+    with hf_runner(model,
+                   dtype=dtype,
+                   postprocess_inputs=process,
+                   auto_cls=AutoModel) as hf_model:
+        import librosa
+
+        hf_outputs_per_audio = [
+            hf_model.generate_greedy_logprobs_limit(
+                [hf_prompt],
+                max_tokens,
+                num_logprobs=num_logprobs,
+                audios=[(librosa.resample(audio[0],
+                                          orig_sr=audio[1],
+                                          target_sr=16000), 16000)])
+            for _, hf_prompt, audio in prompts_and_audios
+        ]
+
+    for hf_outputs, vllm_outputs in zip(hf_outputs_per_audio,
+                                        vllm_outputs_per_audio):
+        check_logprobs_close(
+            outputs_0_lst=hf_outputs,
+            outputs_1_lst=[
+                vllm_to_hf_output(vllm_output, model)
+                for vllm_output in vllm_outputs
+            ],
+            name_0="hf",
+            name_1="vllm",
+        )
+
+
+def run_multi_audio_test(
+    vllm_runner: Type[VllmRunner],
+    prompts_and_audios: List[Tuple[str, List[AudioTuple]]],
+    model: str,
+    *,
+    dtype: str,
+    max_tokens: int,
+    num_logprobs: int,
+    tensor_parallel_size: int,
+    distributed_executor_backend: Optional[str] = None,
+):
+    with vllm_runner(model,
+                     dtype=dtype,
+                     tensor_parallel_size=tensor_parallel_size,
+                     distributed_executor_backend=distributed_executor_backend,
+                     enforce_eager=True,
+                     limit_mm_per_prompt={
+                         "audio":
+                         max((len(audio) for _, audio in prompts_and_audios))
+                     }) as vllm_model:
+        vllm_outputs = vllm_model.generate_greedy_logprobs(
+            [prompt for prompt, _ in prompts_and_audios],
+            max_tokens,
+            num_logprobs=num_logprobs,
+            audios=[audios for _, audios in prompts_and_audios])
+
+    # The HuggingFace model doesn't support multiple audios yet, so
+    # just assert that some tokens were generated.
+    assert all(tokens for tokens, *_ in vllm_outputs)
+
+
+@pytest.mark.parametrize("dtype", ["half"])
+@pytest.mark.parametrize("max_tokens", [128])
+@pytest.mark.parametrize("num_logprobs", [5])
+def test_models(hf_runner, vllm_runner, audio, dtype: str, max_tokens: int,
+                num_logprobs: int) -> None:
+
+    vllm_prompt = _get_prompt(1, "Describe the audio above.", VLLM_PLACEHOLDER)
+    hf_prompt = _get_prompt(1, "Describe the audio above.", HF_PLACEHOLDER)
+    run_test(
+        hf_runner,
+        vllm_runner,
+        [(vllm_prompt, hf_prompt, audio.audio_and_sample_rate)],
+        MODEL_NAME,
+        dtype=dtype,
+        max_tokens=max_tokens,
+        num_logprobs=num_logprobs,
+        tensor_parallel_size=1,
+    )
+
+
+@pytest.mark.parametrize("dtype", ["half"])
+@pytest.mark.parametrize("max_tokens", [128])
+@pytest.mark.parametrize("num_logprobs", [5])
+def test_models_with_multiple_audios(vllm_runner, audio_assets, dtype: str,
+                                     max_tokens: int,
+                                     num_logprobs: int) -> None:
+
+    vllm_prompt = _get_prompt(len(audio_assets),
+                              "Describe each of the audios above.",
+                              VLLM_PLACEHOLDER)
+    run_multi_audio_test(
+        vllm_runner,
+        [(vllm_prompt, [audio.audio_and_sample_rate
+                        for audio in audio_assets])],
+        MODEL_NAME,
+        dtype=dtype,
+        max_tokens=max_tokens,
+        num_logprobs=num_logprobs,
+        tensor_parallel_size=1,
+    )
diff --git a/vllm/_ipex_ops.py b/vllm/_ipex_ops.py
index c3d210c27..8dd101608 100644
--- a/vllm/_ipex_ops.py
+++ b/vllm/_ipex_ops.py
@@ -1,6 +1,4 @@
-# SPDX-License-Identifier: Apache-2.0
-
-from typing import Optional
+from typing import List, Optional, Tuple, Dict
 
 import torch
 
@@ -13,6 +11,7 @@ try:
 except ImportError as e:
     logger.warning("Import error msg: %s", e.msg)
 
+import vllm._C.ops
 
 class ipex_ops:
 
@@ -29,23 +28,31 @@ class ipex_ops:
 
     @staticmethod
     def silu_and_mul(out: torch.Tensor, x: torch.Tensor) -> None:
-        ipex.llm.functional.silu_and_mul(x, out)
+        x1, x2 = ipex_ops._reshape_activation_tensor(x)
+        ipex.llm.functional.silu_mul(x1, x2, out)
+        # vllm._C.ops.silu_and_mul(out, x)
 
     @staticmethod
     def gelu_and_mul(out: torch.Tensor, x: torch.Tensor) -> None:
-        ipex.llm.functional.gelu_and_mul(x, out)
+        # x1, x2 = ipex_ops._reshape_activation_tensor(x)
+        # ipex.llm.functional.gelu_mul(x1, x2, out, "none")
+        vllm._C.ops.gelu_and_mul(out, x)
 
     @staticmethod
     def gelu_tanh_and_mul(out: torch.Tensor, x: torch.Tensor) -> None:
-        ipex.llm.functional.gelu_and_mul(x, out)
+        # x1, x2 = ipex_ops._reshape_activation_tensor(x)
+        # ipex.llm.functional.gelu_mul(x1, x2, out, "tanh")
+        vllm._C.ops.gelu_tanh_and_mul(out, x)
 
     @staticmethod
-    def gelu_fast(x: torch.Tensor) -> torch.Tensor:
-        return torch.nn.functional.gelu(x)
+    def gelu_fast(out: torch.Tensor, x: torch.Tensor) -> None:
+        # out.copy_(torch.nn.functional.gelu(x))
+        vllm._C.ops.gelu_fast(out, x)
 
     @staticmethod
-    def gelu_new(x: torch.Tensor) -> torch.Tensor:
-        return torch.nn.functional.gelu(x)
+    def gelu_new(out: torch.Tensor, x: torch.Tensor) -> None:
+        # out.copy_(torch.nn.functional.gelu(x))
+        vllm._C.ops.gelu_new(out, x)
 
     @staticmethod
     def gelu_quick(out: torch.Tensor, x: torch.Tensor) -> None:
@@ -67,28 +74,20 @@ class ipex_ops:
         kv_cache_dtype: str,
         k_scale: float,
         v_scale: float,
+        logits_soft_cap: float,
         tp_rank: int = 0,
         blocksparse_local_blocks: int = 0,
         blocksparse_vert_stride: int = 0,
         blocksparse_block_size: int = 64,
         blocksparse_head_sliding_step: int = 0,
     ) -> None:
-        assert kv_cache_dtype == "auto"
-        num_heads = out.size(1)
-        num_queries_per_tokens = num_heads // num_kv_heads
-        ipex.llm.modules.PagedAttention.single_query_kv_attention(
-            out,
-            query.contiguous(),
-            key_cache.view_as(value_cache),
-            value_cache,
-            num_queries_per_tokens,
-            scale,
-            block_tables,
-            context_lens,
-            block_size,
-            max_context_len,
-            alibi_slopes,
-        )
+        # todo: ipex will refactor namespace
+        import vllm._C.ops
+        vllm._C.ops.paged_attention_v1(out, query,
+                                       key_cache.view_as(value_cache),
+                                       value_cache, num_kv_heads, scale,
+                                       block_tables, context_lens, block_size,
+                                       max_context_len, alibi_slopes, kv_cache_dtype, k_scale, logits_soft_cap)
 
     @staticmethod
     def paged_attention_v2(
@@ -109,28 +108,21 @@ class ipex_ops:
         kv_cache_dtype: str,
         k_scale: float,
         v_scale: float,
+        logits_soft_cap: float,
         tp_rank: int = 0,
         blocksparse_local_blocks: int = 0,
         blocksparse_vert_stride: int = 0,
         blocksparse_block_size: int = 64,
         blocksparse_head_sliding_step: int = 0,
     ) -> None:
-        assert kv_cache_dtype == "auto"
-        num_heads = out.size(1)
-        num_queries_per_tokens = num_heads // num_kv_heads
-        ipex.llm.modules.PagedAttention.single_query_kv_attention(
-            out,
-            query.contiguous(),
-            key_cache.view_as(value_cache),
-            value_cache,
-            num_queries_per_tokens,
-            scale,
-            block_tables,
-            context_lens,
-            block_size,
-            max_context_len,
-            alibi_slopes,
-        )
+        # todo: ipex will refactor namespace
+        import vllm._C.ops
+        vllm._C.ops.paged_attention_v2(out, exp_sum, max_logits, tmp_out,
+                                     query,
+                                     key_cache.view_as(value_cache),
+                                     value_cache, num_kv_heads, scale, block_tables,
+                                     context_lens, block_size,
+                                     max_context_len, alibi_slopes,kv_cache_dtype, k_scale, logits_soft_cap)
 
     @staticmethod
     def rotary_embedding(
@@ -141,33 +133,83 @@ class ipex_ops:
         cos_sin_cache: torch.Tensor,  # [cos_sin_dim, rot_dim]
         is_neox: bool,
     ) -> None:
-        rot_dim = cos_sin_cache.size(1)
-        ipex.llm.functional.rotary_embedding_batched(positions, query, key,
-                                                     head_size, cos_sin_cache,
-                                                     is_neox, rot_dim)
+        import vllm._C.ops
+        vllm._C.ops.rotary_embedding(positions, query, key, head_size, cos_sin_cache, is_neox)
+        # if positions.dim() == 1:
+        #     positions = positions.unsqueeze(0)
+        #     query = query.unsqueeze(0)
+        #     key = key.unsqueeze(0)
 
-    @staticmethod
-    def batched_rotary_embedding(positions: torch.Tensor, query: torch.Tensor,
-                                 key: torch.Tensor, head_size: int,
-                                 cos_sin_cache: torch.Tensor, is_neox: bool,
-                                 rot_dim: int,
-                                 cos_sin_cache_offsets: torch.Tensor) -> None:
-        ipex.llm.functional.rotary_embedding_batched(positions, query, key,
-                                                     head_size, cos_sin_cache,
-                                                     is_neox, rot_dim,
-                                                     cos_sin_cache_offsets)
+        # rotary_dim = cos_sin_cache.size(1)
+        # query = query.view(*query.shape[:-1], -1, head_size)
+        # key = key.view(*key.shape[:-1], -1, head_size)
+
+        # query_rot = query[..., :rotary_dim]
+        # key_rot = key[..., :rotary_dim]
+
+        # cos_sin = cos_sin_cache[positions.long()]
+        # cos, sin = cos_sin.chunk(2, dim=-1)
+
+        # if is_neox:
+        #     cos = cos.repeat(1, 1, 2).unsqueeze(-2)
+        #     sin = sin.repeat(1, 1, 2).unsqueeze(-2)
+        # else:
+        #     cos = cos.repeat_interleave(2, dim=-1).unsqueeze(-2)
+        #     sin = sin.repeat_interleave(2, dim=-1).unsqueeze(-2)
+        
+        # import vllm._C.ops
+        # vllm._C.ops.rotary_embedding(query_rot, key_rot, sin, cos,
+        #                                      rotary_dim, is_neox, positions)
+
+    # def batched_rotary_embedding(positions: torch.Tensor, query: torch.Tensor,
+    #                              key: torch.Tensor, head_size: int,
+    #                              cos_sin_cache: torch.Tensor, is_neox: bool,
+    #                              rot_dim: int,
+    #                              cos_sin_cache_offsets: torch.Tensor) -> None:
+        
+        # if positions.dim() == 1:
+        #     positions = positions.unsqueeze(0)
+        #     query = query.unsqueeze(0)
+        #     key = key.unsqueeze(0)
+        # cos_sin_cache_offsets = cos_sin_cache_offsets.view_as(positions)
+        # rotary_dim = cos_sin_cache.size(1)
+        # query = query.view(*query.shape[:-1], -1, head_size)
+        # key = key.view(*key.shape[:-1], -1, head_size)
+
+        # query_rot = query[..., :rotary_dim]
+        # key_rot = key[..., :rotary_dim]
+
+        # cos_sin = cos_sin_cache[torch.add(positions,
+        #                                   cos_sin_cache_offsets).long()]
+        # cos, sin = cos_sin.chunk(2, dim=-1)
+
+        # if is_neox:
+        #     cos = cos.repeat(1, 1, 2).unsqueeze(-2)
+        #     sin = sin.repeat(1, 1, 2).unsqueeze(-2)
+        # else:
+        #     cos = cos.repeat_interleave(2, dim=-1).unsqueeze(-2)
+        #     sin = sin.repeat_interleave(2, dim=-1).unsqueeze(-2)
+
+        # import vllm._C.ops
+        # vllm._C.ops.rotary_embedding(query_rot, key_rot, sin, cos,
+        #                                      rotary_dim, is_neox, positions)
 
     @staticmethod
-    def rms_norm(input: torch.Tensor, weight: torch.Tensor,
-                 epsilon: float) -> torch.Tensor:
-        return ipex.llm.functional.rms_norm(input, weight, epsilon)
+    def rms_norm(out: torch.Tensor, input: torch.Tensor, weight: torch.Tensor,
+                 epsilon: float) -> None:
+        # from intel_extension_for_pytorch.llm.modules.mha_fusion import RMSNorm
+        # ipex_rms_norm = RMSNorm(weight, epsilon)
+        # tmp = ipex_rms_norm.apply(input, weight, epsilon)
+        # out.copy_(tmp)
+        vllm._C.ops.rms_norm(out, input, weight, epsilon)
 
     @staticmethod
     def fused_add_rms_norm(input: torch.Tensor, residual: torch.Tensor,
                            weight: torch.Tensor, epsilon: float) -> None:
-        tmp = ipex.llm.functional.add_rms_norm(residual, input, weight, None,
-                                               epsilon, True)
-        input.copy_(tmp)
+        # tmp = ipex.llm.functional.add_rms_norm(residual, input, weight, None,
+        #                                        epsilon, True)
+        # input.copy_(tmp)
+        vllm._C.ops.fused_add_rms_norm(input, residual, weight, epsilon)
 
     @staticmethod
     def varlen_attention(
@@ -220,22 +262,250 @@ class ipex_ops:
         kv_cache_dtype: str,
         k_scale: float,
         v_scale: float,
+    ) -> None:
+        # assert kv_cache_dtype == "auto"
+        # ipex.llm.modules.PagedAttention.reshape_and_cache(
+        #     key, value, key_cache, value_cache, slot_mapping)
+        vllm._C.cache_ops.reshape_and_cache(key, value, key_cache, value_cache, slot_mapping, kv_cache_dtype, k_scale)
+    
+    @staticmethod
+    def reshape_and_cache_ipexllm(
+        key: torch.Tensor,
+        value: torch.Tensor,
+        key_cache: torch.Tensor,
+        value_cache: torch.Tensor,
+        slot_mapping: torch.Tensor,
+        kv_cache_dtype: str,
+        k_scale: float,
+        v_scale: float,
+    ) -> None:
+        if kv_cache_dtype == "fp8":
+            vllm._C.cache_ops.reshape_and_cache_ipexllm_fp8(key, value, key_cache, value_cache, slot_mapping, kv_cache_dtype, k_scale)
+        else:
+            vllm._C.cache_ops.reshape_and_cache_ipexllm(key, value, key_cache, value_cache, slot_mapping, kv_cache_dtype, k_scale)
+
+    @staticmethod
+    def paged_attention_gqa(
+        out: torch.Tensor,
+        query: torch.Tensor,
+        key_cache: torch.Tensor,
+        value_cache: torch.Tensor,
+        batch_size: int,
+        num_heads: int,
+        num_kv_heads: int,
+        scale: float,
+        block_tables: torch.Tensor,
+        seq_lens_tensor: torch.Tensor,
+        block_size: int,
+        head_size: int,
+        max_seq_len: int,
+        kv_cache_format: str
+    ):
+        if kv_cache_format == "fp8":
+            vllm._C.ops.paged_attention_gqa_fp8(
+                out,
+                query,
+                key_cache,
+                value_cache,
+                batch_size,
+                num_heads,
+                num_kv_heads,
+                scale,
+                block_tables,
+                seq_lens_tensor,
+                block_size,
+                head_size,
+                max_seq_len
+            )
+        else:
+            vllm._C.ops.paged_attention_gqa(
+                out,
+                query,
+                key_cache,
+                value_cache,
+                batch_size,
+                num_heads,
+                num_kv_heads,
+                scale,
+                block_tables,
+                seq_lens_tensor,
+                block_size,
+                head_size,
+                max_seq_len
+            )
+
+    @staticmethod
+    def reshape_and_cache_flash(
+        key: torch.Tensor,
+        value: torch.Tensor,
+        key_cache: torch.Tensor,
+        value_cache: torch.Tensor,
+        slot_mapping: torch.Tensor,
+        kv_cache_dtype: str,
+        k_scale: float,
+        v_scale: float,
     ) -> None:
         assert kv_cache_dtype == "auto"
-        ipex.llm.modules.PagedAttention.reshape_and_cache(
+        ipex.llm.modules.PagedAttention.reshape_and_cache_flash(
             key, value, key_cache, value_cache, slot_mapping)
 
     @staticmethod
-    def copy_blocks(key_caches: list[torch.Tensor],
-                    value_caches: list[torch.Tensor],
-                    block_mapping: torch.Tensor) -> None:
-        torch.xpu.copy_blocks(  # type: ignore
-            key_caches,
-            value_caches,
-            block_mapping,
+    def chunked_prefill(
+        query: torch.Tensor,
+        key_cache: torch.Tensor,
+        value_cache: torch.Tensor,
+        output: torch.Tensor,
+        cu_seqlens_q: torch.Tensor,
+        cu_seqlens_k: torch.Tensor,
+        seq_used_k: Optional[torch.Tensor],
+        block_table: torch.Tensor,
+        alibi_slopes: Optional[torch.Tensor],
+        max_seqlen_q: int,
+        max_seqlen_k: int,
+        p_dropout: float,
+        softmax_scale: float,
+        zero_tensors: bool,
+        is_casual: bool,
+        return_softmax: bool,
+        gen_: Optional[torch.Generator],
+    ):
+        return ipex.llm.modules.PagedAttention.flash_attn_varlen_func(
+            output,
+            query.contiguous(),
+            key_cache,
+            value_cache,
+            cu_seqlens_q,
+            cu_seqlens_k,
+            max_seqlen_q,
+            max_seqlen_k,
+            softmax_scale,
+            is_casual,
+            block_table,
+            alibi_slopes,
+            k_scale=1.0,
+            v_scale=1.0,
         )
+        # return torch.ops.torch_ipex.chunked_prefill(
+        #     query.contiguous(),
+        #     key_cache,
+        #     value_cache,
+        #     output,
+        #     cu_seqlens_q,
+        #     cu_seqlens_k,
+        #     seq_used_k,
+        #     block_table,
+        #     alibi_slopes,
+        #     max_seqlen_q,
+        #     max_seqlen_k,
+        #     p_dropout,
+        #     softmax_scale,
+        #     zero_tensors,
+        #     is_caual,
+        #     return_softmax,
+        #     gen_,
+        # )
+
+
+    @staticmethod
+    def copy_blocks(key_caches: List[torch.Tensor],
+                    value_caches: List[torch.Tensor],
+                    block_mapping) -> None:
+        # torch.xpu.copy_blocks(  # type: ignore
+        #     key_caches,
+        #     value_caches,
+        #     block_mapping,
+        # )
+        vllm._C.cache_ops.copy_blocks(key_caches, value_caches, block_mapping)
 
     @staticmethod
     def swap_blocks(src: torch.Tensor, dst: torch.Tensor,
                     block_mapping: torch.Tensor) -> None:
-        torch.xpu.swap_blocks(src, dst, block_mapping)  # type: ignore
+        vllm._C.cache_ops.swap_blocks(key_caches, value_caches, block_mapping)
+        # torch.xpu.swap_blocks(src, dst, block_mapping)  # type: ignore
+
+    @staticmethod
+    def bgmv_shrink(inputs: torch.Tensor,
+                    lora_a_weights: torch.Tensor,
+                    output_tensor: torch.Tensor,
+                    lora_indices_tensor: torch.Tensor,
+                    scaling: float = 1.0) -> None:
+        ipex.llm.functional.bgmv_shrink(inputs, lora_a_weights, output_tensor,
+                                        lora_indices_tensor, scaling)
+
+    @staticmethod
+    def bgmv_expand(inputs: torch.Tensor,
+                    lora_b_weights: torch.Tensor,
+                    output_tensor: torch.Tensor,
+                    lora_indices_tensor: torch.Tensor,
+                    add_inputs: bool = True) -> None:
+        ipex.llm.functional.bgmv_expand(inputs, lora_b_weights, output_tensor,
+                                        lora_indices_tensor, add_inputs)
+
+    @staticmethod
+    def bgmv_expand_slice(inputs: torch.Tensor,
+                          lora_b_weights: torch.Tensor,
+                          output_tensor: torch.Tensor,
+                          lora_indices_tensor: torch.Tensor,
+                          slice_offset: int,
+                          slice_size: int,
+                          add_inputs: bool = True) -> None:
+        ipex.llm.functional.bgmv_expand_slice(inputs, lora_b_weights,
+                                              output_tensor,
+                                              lora_indices_tensor,
+                                              slice_offset, slice_size,
+                                              add_inputs)
+
+    @staticmethod
+    def sgmv_shrink(inputs: torch.Tensor,
+                    lora_a_weights: torch.Tensor,
+                    output_tensor: torch.Tensor,
+                    b_seq_start_loc: torch.Tensor,
+                    seq_len_tensor: torch.Tensor,
+                    lora_indices_tensor: torch.Tensor,
+                    batches: int,
+                    max_seq_length: int,
+                    token_nums: int,
+                    scaling: float = 1.0) -> None:
+        assert inputs.size(0) == token_nums
+        ipex.llm.functional.sgmv_shrink(inputs, lora_a_weights, output_tensor,
+                                        b_seq_start_loc, seq_len_tensor,
+                                        lora_indices_tensor, batches,
+                                        max_seq_length, scaling)
+
+    @staticmethod
+    def sgmv_expand(inputs: torch.Tensor,
+                    lora_b_weights: torch.Tensor,
+                    output_tensor: torch.Tensor,
+                    b_seq_start_loc: torch.Tensor,
+                    seq_len_tensor: torch.Tensor,
+                    lora_indices_tensor: torch.Tensor,
+                    batches: int,
+                    max_seq_length: int,
+                    token_nums: int,
+                    add_inputs: bool = False) -> None:
+        assert inputs.size(0) == token_nums
+        ipex.llm.functional.sgmv_expand(inputs, lora_b_weights, output_tensor,
+                                        b_seq_start_loc, seq_len_tensor,
+                                        lora_indices_tensor, batches,
+                                        max_seq_length, add_inputs)
+
+    @staticmethod
+    def sgmv_expand_slice(inputs: torch.Tensor,
+                          lora_b_weights: torch.Tensor,
+                          output_tensor: torch.Tensor,
+                          b_seq_start_loc: torch.Tensor,
+                          seq_len_tensor: torch.Tensor,
+                          lora_indices_tensor: torch.Tensor,
+                          batches: int,
+                          max_seq_length: int,
+                          token_nums: int,
+                          slice_offset: int,
+                          slice_size: int,
+                          add_inputs: bool = False) -> None:
+        assert inputs.size(0) == token_nums
+        ipex.llm.functional.sgmv_expand_slice(inputs, lora_b_weights,
+                                              output_tensor, b_seq_start_loc,
+                                              seq_len_tensor,
+                                              lora_indices_tensor, batches,
+                                              max_seq_length, slice_offset,
+                                              slice_size, add_inputs)
diff --git a/vllm/attention/backends/ipex_attn.py b/vllm/attention/backends/ipex_attn.py
index d3c61ea26..3ec6ee9ee 100644
--- a/vllm/attention/backends/ipex_attn.py
+++ b/vllm/attention/backends/ipex_attn.py
@@ -5,7 +5,7 @@ from dataclasses import dataclass
 from typing import Any, Dict, List, Optional, Tuple, Type
 
 import torch
-
+import os
 from vllm._ipex_ops import ipex_ops
 from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
                                               AttentionLayer,
@@ -15,7 +15,11 @@ from vllm.attention.backends.utils import CommonAttentionState
 from vllm.attention.ops.paged_attn import (PagedAttention,
                                            PagedAttentionMetadata)
 
+from vllm.logger import init_logger
+logger = init_logger('vllm.attention.backends.ipex_attn')
+
 _PARTITION_SIZE = 512
+_IPEX_BACKEND_SUPPORTED_KV_CACHE_FORMAT=["fp8", "auto"]
 
 
 class IpexAttnBackend(AttentionBackend):
@@ -52,18 +56,16 @@ class IpexAttnBackend(AttentionBackend):
         dst_kv_cache: torch.Tensor,
         src_to_dst: torch.Tensor,
     ) -> None:
-        from vllm._ipex_ops import ipex_ops as ops
-        ops.swap_blocks(src_kv_cache, dst_kv_cache, src_to_dst)
+        torch.xpu.swap_blocks(src_kv_cache, dst_kv_cache, src_to_dst)
 
     @staticmethod
     def copy_blocks(
         kv_caches: List[torch.Tensor],
         src_to_dists: torch.Tensor,
     ) -> None:
-        from vllm._ipex_ops import ipex_ops as ops
         key_caches = [kv_cache[0] for kv_cache in kv_caches]
         value_caches = [kv_cache[1] for kv_cache in kv_caches]
-        ops.copy_blocks(key_caches, value_caches, src_to_dists)
+        torch.xpu.copy_blocks(key_caches, value_caches, src_to_dists)
 
 
 @dataclass
@@ -77,6 +79,11 @@ class IpexAttnMetadata(AttentionMetadata, PagedAttentionMetadata):
     seq_lens: Optional[List[int]]
     seqlen_q: Optional[torch.Tensor]
     max_seqlen: Optional[int]
+    query_start_loc: Optional[torch.Tensor]
+    context_lens: Optional[torch.Tensor]
+
+    _cached_prefill_metadata: Optional["IpexAttnMetadata"] = None
+    _cached_decode_metadata: Optional["IpexAttnMetadata"] = None
 
     def __post_init__(self):
         # Set during the execution of the first attention op.
@@ -89,21 +96,143 @@ class IpexAttnMetadata(AttentionMetadata, PagedAttentionMetadata):
     @property
     def prefill_metadata(self) -> Optional["IpexAttnMetadata"]:
         # Currently chunked prefill is not supported
-        if self.num_decode_tokens == 0:
-            assert self.num_prefills > 0
-            return self
+        if self.num_prefills == 0:
+            return None
 
-        return None
+        if self._cached_prefill_metadata is not None:
+            return self._cached_prefill_metadata
+
+        assert self.seq_lens is not None
+        assert self.seq_lens_tensor is not None
+        assert self.query_start_loc is not None
+        assert self.context_lens is not None
+        assert self.block_tables is not None
+
+        self._cached_prefill_metadata = IpexAttnMetadata(
+            is_prompt=self.is_prompt,
+            seqlen_q=self.seqlen_q,
+            max_seqlen=self.max_seqlen,
+            num_prefills=self.num_prefills,
+            multi_modal_placeholder_index_maps=None,
+            num_prefill_tokens=self.num_prefill_tokens,
+            num_decode_tokens=0,
+            slot_mapping=self.slot_mapping[:self.num_prefill_tokens],
+            seq_lens=self.seq_lens[:self.num_prefills],
+            seq_lens_tensor=self.seq_lens_tensor[:self.num_prefills],
+            # max_query_len=self.max_query_len,
+            max_decode_seq_len=0,
+            query_start_loc=self.query_start_loc[:self.num_prefills + 1] if (torch.is_tensor(self.query_start_loc)) else None,
+            # seq_start_loc=None,
+            context_lens=self.context_lens[:self.num_prefills] if (torch.is_tensor(self.context_lens)) else None,
+            block_tables=self.block_tables[:self.num_prefills],
+            enable_kv_scales_calculation=False,
+        )
+        return self._cached_prefill_metadata
 
     @property
     def decode_metadata(self) -> Optional["IpexAttnMetadata"]:
-        # Currently chunked prefill is not supported
-        if self.num_prefills > 0:
-            assert self.num_decode_tokens == 0
+        if self.num_decode_tokens == 0:
             return None
 
-        return self
-
+        if self._cached_decode_metadata is not None:
+            return self._cached_decode_metadata
+        assert self.block_tables is not None
+        assert self.seq_lens_tensor is not None
+
+        self._cached_decode_metadata = IpexAttnMetadata(
+            is_prompt=self.is_prompt,
+            seqlen_q=self.seqlen_q,
+            max_seqlen=self.max_seqlen,
+            num_prefills=0,
+            multi_modal_placeholder_index_maps=None,
+            num_prefill_tokens=0,
+            num_decode_tokens=self.num_decode_tokens,
+            slot_mapping=self.slot_mapping[self.num_prefill_tokens:],
+            seq_lens=self.seq_lens[self.num_prefills:],
+            seq_lens_tensor=self.seq_lens_tensor[self.num_prefills:],
+            # max_query_len=None,
+            max_decode_seq_len=self.max_decode_seq_len,
+            query_start_loc=None,
+            # seq_start_loc=None,
+            context_lens=self.context_lens[self.num_prefills:] if (torch.is_tensor(self.context_lens)) else None,
+            block_tables=self.block_tables[self.num_prefills:],
+            enable_kv_scales_calculation=False,
+        )
+        return self._cached_decode_metadata
+    
+    def advance_step(self, num_seqs, num_queries):
+        assert num_seqs == num_queries
+        
+        assert self.num_prefills == 0
+        assert self.num_prefill_tokens == 0
+        assert self.num_decode_tokens == num_seqs
+        assert self.slot_mapping.shape == (num_seqs, )
+
+        assert self.seq_lens is not None
+        assert len(self.seq_lens) == num_seqs
+        assert self.seq_lens_tensor is not None
+        assert self.seq_lens_tensor.shape == (num_seqs, )
+        # assert self.max_query_len == 1
+        # assert self.max_prefill_seq_len == 0
+        assert self.max_decode_seq_len == max(self.seq_lens)
+
+        # assert self.query_start_loc is not None
+        # assert self.query_start_loc.shape == (num_queries + 1, )
+        # assert self.seq_start_loc is not None
+        # assert self.seq_start_loc.shape == (num_seqs + 1, )
+
+        # assert self.context_lens_tensor is not None
+        # assert self.context_lens_tensor.shape == (num_queries, )
+
+        assert self.block_tables is not None
+        assert self.block_tables.shape[0] == num_seqs
+
+        # Update query lengths. Note that we update only queries and not seqs,
+        # since tensors may be padded due to captured cuda graph batch size
+        for i in range(num_queries):
+            self.seq_lens[i] += 1
+        self.max_decode_seq_len = max(self.seq_lens)
+
+
+from torch.nn.functional import scaled_dot_product_attention
+
+def _make_attention_mask(
+    att_bias: List[torch.Tensor],
+    seq_lens: List[int],
+    prompt_token_num: int,
+    dtype: torch.dtype,
+) -> torch.Tensor:
+    assert att_bias[0].dim() == 3
+    assert len(att_bias) == len(seq_lens)
+    head_size, _, _ = att_bias[0].size()
+    mask = torch.empty(head_size,
+                       prompt_token_num,
+                       prompt_token_num,
+                       dtype=dtype)
+    mask.fill_(-torch.inf)
+    start = 0
+    for prompt_len, sub_mask in zip(seq_lens, att_bias):
+        end = start + prompt_len
+        mask[:, start:end, start:end] = sub_mask
+        start += prompt_len
+    return mask
+
+
+def use_sdp_causal(head_dim, query_states, logits_soft_cap, attn_type):
+    return (
+        (logits_soft_cap != 0                        # for gemma model 
+        or head_dim in [-1, 64, 80, 96, 128, 256])        # for now
+        and query_states.device.type == "xpu"        # GPU
+        and query_states.dtype in [torch.float, torch.half]     # fp32/fp16
+        and attn_type is AttentionType.DECODER
+    )
+
+def use_gqa_kernel(num_heads, num_kv_heads, head_size, logits_soft_cap):
+    kv_cache_format = os.environ.get('USE_VLLM_KVCACHE')
+    if kv_cache_format is None and num_heads != num_kv_heads and head_size in [128, 96, 80, 64] and logits_soft_cap == 0:
+        return True
+    else:
+        return False
 
 class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
 
@@ -119,6 +248,7 @@ class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
         blocksparse_params: Optional[Dict[str, Any]] = None,
         logits_soft_cap: Optional[float] = None,
         attn_type: str = AttentionType.DECODER,
+        use_irope: bool = False,
     ) -> None:
         if blocksparse_params is not None:
             raise ValueError(
@@ -132,29 +262,40 @@ class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
         self.alibi_slopes = alibi_slopes
         self.sliding_window = sliding_window
         self.kv_cache_dtype = kv_cache_dtype
+        self.use_irope = use_irope
 
         assert self.num_heads % self.num_kv_heads == 0
         self.num_queries_per_kv = self.num_heads // self.num_kv_heads
         self.need_mask = (self.alibi_slopes is not None
                           or self.sliding_window is not None)
         if logits_soft_cap is None:
-            logits_soft_cap = 0
+            logits_soft_cap = 0.0
         self.logits_soft_cap = logits_soft_cap
+        self.attn_type = attn_type
 
         supported_head_sizes = PagedAttention.get_supported_head_sizes()
         if head_size not in supported_head_sizes:
             raise ValueError(
                 f"Head size {head_size} is not supported by PagedAttention. "
                 f"Supported head sizes are: {supported_head_sizes}.")
-        if is_quantized_kv_cache(kv_cache_dtype):
-            raise NotImplementedError(
-                "IPEX backend does not support FP8 KV cache. "
-                "Please use xFormers backend instead.")
-        if attn_type != AttentionType.DECODER:
-            raise NotImplementedError("Encoder self-attention and "
-                                      "encoder/decoder cross-attention "
-                                      "are not implemented for "
+        if attn_type != AttentionType.DECODER and attn_type != AttentionType.ENCODER_ONLY:
+            raise NotImplementedError("Encoder/decoder cross-attention "
+                                      "is not implemented for "
                                       "IpexAttnBackendImpl")
+        if kv_cache_dtype not in _IPEX_BACKEND_SUPPORTED_KV_CACHE_FORMAT:
+            raise NotImplementedError(f"IPEX backend does not support "
+                                       "KV cache format {kv_cache_dtype}")
+        # Also check for gqa models...
+        self.using_gqa_kernel = use_gqa_kernel(self.num_heads, self.num_kv_heads, self.head_size, self.logits_soft_cap)
+        if not self.using_gqa_kernel and kv_cache_dtype == "fp8":
+            raise NotImplementedError(f"IPEX backend currently only supports "
+                                      "fp8 kv cache in group-query attention")
+        
+        self.ipex_varlen_attn = False
+        flag = os.getenv("IPEX_LLM_PREFILL_VARLEN_BACKEND", None)
+        if flag is not None:
+            self.ipex_varlen_attn = True
+            logger.info_once(f"Using varlen_attention for prefilling.")
 
     def split_kv_cache(
         self,
@@ -162,16 +303,34 @@ class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
         num_kv_heads: int,
         head_size: int,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
-        x = 1
+        x = 16 // kv_cache.element_size()
         num_blocks = kv_cache.shape[1]
 
         key_cache = kv_cache[0]
         key_cache = key_cache.view(num_blocks, num_kv_heads, head_size // x,
                                    -1, x)
+
         value_cache = kv_cache[1]
         value_cache = value_cache.view(num_blocks, num_kv_heads, head_size, -1)
         return key_cache, value_cache
 
+
+    def split_kv_cache_ipexllm(
+        self,
+        kv_cache: torch.Tensor,
+        num_kv_heads: int,
+        head_size: int,
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        # For GQA kernel, key_cache and value_cache shape should be [num_blocks, num_kv_heads, head_size, block_size]
+        num_blocks = kv_cache.shape[1]
+
+        key_cache = kv_cache[0]
+        key_cache = key_cache.view(num_blocks, num_kv_heads, -1, head_size)
+        value_cache = kv_cache[1]
+        value_cache = value_cache.view(num_blocks, num_kv_heads, -1, head_size)
+        return key_cache, value_cache
+
+
     def forward(
         self,
         layer: AttentionLayer,
@@ -202,75 +361,177 @@ class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
         key = key.view(-1, self.num_kv_heads, self.head_size)
         value = value.view(-1, self.num_kv_heads, self.head_size)
 
-        if kv_cache.numel() > 0:
-            key_cache, value_cache = self.split_kv_cache(
-                kv_cache, self.num_kv_heads, self.head_size)
-            ipex_ops.reshape_and_cache(
-                key,
-                value,
-                key_cache,
-                value_cache,
-                attn_metadata.slot_mapping.flatten(),
-                self.kv_cache_dtype,
-                layer._k_scale,
-                layer._v_scale,
-            )
-
-        if attn_metadata.is_prompt:
-            assert attn_metadata.seq_lens is not None
-            if (kv_cache.numel() == 0
-                    or attn_metadata.block_tables.numel() == 0):
+        if kv_cache.numel() > 0 and self.attn_type == AttentionType.DECODER:
+            if self.using_gqa_kernel:
+                key_cache, value_cache = self.split_kv_cache_ipexllm(
+                    kv_cache, self.num_kv_heads, self.head_size)      
+                ipex_ops.reshape_and_cache_ipexllm(
+                    key,
+                    value,
+                    key_cache,
+                    value_cache,
+                    attn_metadata.slot_mapping.flatten(),
+                    self.kv_cache_dtype,
+                    layer._k_scale,
+                    layer._v_scale,
+                )
+            else:
+                key_cache, value_cache = self.split_kv_cache(
+                    kv_cache, self.num_kv_heads, self.head_size)   
+                ipex_ops.reshape_and_cache(
+                    key,
+                    value,
+                    key_cache,
+                    value_cache,
+                    attn_metadata.slot_mapping.flatten(),
+                    self.kv_cache_dtype,
+                    layer._k_scale,
+                    layer._v_scale,
+                )
+
+        # New added code-segment
+        num_prefill_tokens = attn_metadata.num_prefill_tokens
+        num_decode_tokens = attn_metadata.num_decode_tokens
+        assert query.shape[0] == num_prefill_tokens + num_decode_tokens
+        assert key.shape[0] == num_prefill_tokens + num_decode_tokens
+        assert value.shape[0] == num_prefill_tokens + num_decode_tokens
+
+
+        output = torch.empty_like(query)
+        # Query for decode. KV is not needed because it is already cached.
+        decode_query = query[num_prefill_tokens:]
+        # QKV for prefill.
+        query = query[:num_prefill_tokens]
+        key = key[:num_prefill_tokens]
+        value = value[:num_prefill_tokens]
+
+        assert query.shape[0] == num_prefill_tokens
+        assert decode_query.shape[0] == num_decode_tokens
+        # If mask is not set, then is_causal=True
+        # If mask is set, then is_causal=False
+        is_causal = not self.need_mask
+        if self.attn_type == AttentionType.ENCODER_ONLY:
+            is_causal = False
+
+        if prefill_meta := attn_metadata.prefill_metadata:
+            assert prefill_meta.seq_lens is not None
+            if (kv_cache is None or prefill_meta.block_tables.numel() == 0):
                 if self.num_kv_heads != self.num_heads:
                     key = key.repeat_interleave(self.num_queries_per_kv, dim=1)
                     value = value.repeat_interleave(self.num_queries_per_kv,
                                                     dim=1)
 
-                if attn_metadata.attn_bias is None:
+                if prefill_meta.attn_bias is None:
                     if self.alibi_slopes is not None:
+                        self.alibi_slopes = self.alibi_slopes.to(query.device)
                         att_masks = _make_alibi_bias(
                             self.alibi_slopes, query.dtype,
-                            attn_metadata.seq_lens)  # type: ignore
+                            prefill_meta.seq_lens)  # type: ignore
                     elif self.sliding_window is not None:
                         att_masks = _make_sliding_window_bias(
-                            attn_metadata.seq_lens, self.sliding_window,
+                            prefill_meta.seq_lens, self.sliding_window,
                             query.dtype)  # type: ignore
                     else:
-                        att_masks = _make_sliding_window_bias(
-                            attn_metadata.seq_lens, None, dtype=query.dtype)
-                    attn_metadata.attn_bias = att_masks
-
-                output = torch.empty(
-                    (num_tokens, self.num_heads, self.head_size),
-                    dtype=query.dtype,
-                    device=query.device)
-                ipex_ops.varlen_attention(
-                    query,
-                    key,
-                    value,
-                    output,
-                    attn_metadata.seqlen_q,
-                    attn_metadata.seqlen_q,
-                    attn_metadata.max_seqlen,
-                    attn_metadata.max_seqlen,
-                    pdropout=0.0,
-                    softmax_scale=self.scale,
-                    zero_tensors=False,
-                    is_causal=True,
-                    return_softmax=False,
-                    gen_=None,
-                    logits_soft_cap=self.logits_soft_cap,
-                )
+                        att_masks = [None] * len(prefill_meta.seq_lens)
+                    prefill_meta.attn_bias = att_masks
+                
+                if self.ipex_varlen_attn:
+                    output = torch.empty(
+                        (num_tokens, self.num_heads, self.head_size),
+                        dtype=query.dtype,
+                        device=query.device)
+                        
+                    tmp = [0]
+                    tmp.extend(prefill_meta.seq_lens)
+                    seqlen = torch.tensor(tmp)
+                    seqlen_q = torch.cumsum(seqlen, dim=0).to(device=query.device)
+                    ipex_ops.varlen_attention(query,
+                                              key,
+                                              value,
+                                              output,
+                                              seqlen_q,
+                                              seqlen_q,
+                                              prefill_meta.max_seqlen,
+                                              prefill_meta.max_seqlen,
+                                              pdropout=0.0,
+                                              softmax_scale=self.scale,
+                                              zero_tensors=False,
+                                              is_causal=is_causal,
+                                              return_softmax=False,
+                                              gen_=None,
+                                              logits_soft_cap=self.logits_soft_cap)
+                else:                                          
+                    output = torch.empty(
+                                (num_tokens, self.num_heads, self.head_size),
+                                dtype=query.dtype, device=query.device)
+                    query = query.movedim(0, query.dim() - 2)
+                    key = key.movedim(0, key.dim() - 2)
+                    value = value.movedim(0, value.dim() - 2)
+                    import math
+                    scale = 1 / math.sqrt(self.head_size) if self.scale is None else self.scale
+                    start = 0
+                    for seq_len, mask in zip(prefill_meta.seq_lens,
+                                             prefill_meta.attn_bias):
+                        end = start + seq_len
+                        if self.alibi_slopes is None and use_sdp_causal(self.head_size, query, self.logits_soft_cap, self.attn_type):
+                            import xe_addons
+                            if mask is not None:
+                                mask = mask.unsqueeze(0)
+                            if self.logits_soft_cap == 0 or self.head_size != 256:
+                                sub_out = xe_addons.sdp_causal(
+                                    query[None, :, start:end, :].contiguous(),
+                                    key[None, :, start:end, :].contiguous(),
+                                    value[None, :, start:end, :].contiguous(),
+                                    mask,
+                                    scale).squeeze(0).movedim(
+                                        query.dim() - 2, 0)
+                            else:
+                                sub_out = xe_addons.gemma2_sdp_causal(
+                                    query[None, :, start:end, :].contiguous(),
+                                    key[None, :, start:end, :].contiguous(),
+                                    value[None, :, start:end, :].contiguous(),
+                                    mask,
+                                    self.logits_soft_cap,
+                                    self.scale).squeeze(0).movedim(
+                                        query.dim() - 2, 0)                            
+                        else:
+                            sub_out = torch.nn.functional.scaled_dot_product_attention(
+                                query[None, :, start:end, :],
+                                key[None, :, start:end, :],
+                                value[None, :, start:end, :],
+                                attn_mask=mask,
+                                dropout_p=0.0,
+                                is_causal=is_causal,
+                                scale=self.scale).squeeze(0).movedim(
+                                    query.dim() - 2, 0)
+                        output[start:end, :, :] = sub_out
+                        start = end
             else:
                 # prefix-enabled attention
-                raise RuntimeError(
-                    "IPEX backend doesn't support prefix decoding.")
-
-        else:
+                if self.num_kv_heads != self.num_heads:
+                    key = key.repeat_interleave(self.num_queries_per_kv, dim=1)
+                    value = value.repeat_interleave(self.num_queries_per_kv,
+                                                    dim=1)
+                import vllm._C.ops
+                assert self.head_size == 128 or self.head_size == 64
+                value = os.environ.get('USE_CONTEXT_V1')
+                if self.using_gqa_kernel:
+                    # if using_gqa_kernel, then only the v1 kernel can be used
+                    out = vllm._C.ops.context_attention_forward_v1(query, key_cache, value_cache, prefill_meta.block_tables, prefill_meta.query_start_loc, prefill_meta.seq_lens_tensor, prefill_meta.context_lens, prefill_meta.max_seqlen, torch.amax(prefill_meta.context_lens).item())
+                elif value is None:
+                    # Otherwise, by default use v2 attention forward kernel...
+                    query_len = prefill_meta.query_start_loc[1:] - prefill_meta.query_start_loc[:-1]
+                    out = vllm._C.ops.context_attention_forward_v2(query, key_cache, value_cache, prefill_meta.block_tables, prefill_meta.query_start_loc, prefill_meta.seq_lens_tensor, prefill_meta.context_lens, prefill_meta.max_seqlen, torch.amax(prefill_meta.context_lens).item(), torch.amax(query_len).item())
+                else:
+                    out = vllm._C.ops.context_attention_forward_v1(query, key_cache, value_cache, prefill_meta.block_tables, prefill_meta.query_start_loc, prefill_meta.seq_lens_tensor, prefill_meta.context_lens, prefill_meta.max_seqlen, torch.amax(prefill_meta.context_lens).item())
+                assert output[:num_prefill_tokens].shape == out.shape
+                output[:num_prefill_tokens] = out
+
+        if decode_meta := attn_metadata.decode_metadata:
             # Decoding run.
-            max_seq_len = attn_metadata.max_decode_seq_len
-            output = torch.empty_like(query)
-            block_size = value_cache.shape[3]
-            num_seqs, num_heads, head_size = query.shape
+            max_seq_len = decode_meta.max_decode_seq_len
+            out = torch.empty_like(decode_query)
+            num_seqs, num_heads, head_size = decode_query.shape
             max_num_partitions = ((max_seq_len + _PARTITION_SIZE - 1) //
                                   _PARTITION_SIZE)
             # NOTE(woosuk): We use a simple heuristic to decide whether to use
@@ -281,59 +542,86 @@ class IpexAttnBackendImpl(AttentionImpl[IpexAttnMetadata]):
             # TODO(woosuk): Tune this heuristic.
             # For context len > 8192, use V2 kernel to avoid shared memory
             # shortage.
-            use_v1 = (max_seq_len <= 8192 and
-                      (max_num_partitions == 1 or num_seqs * num_heads > 512))
-            if use_v1:
-                # Run PagedAttention V1.
-                ipex_ops.paged_attention_v1(
-                    output,
-                    query,
+
+            bsz = len(decode_meta.seq_lens)
+            import vllm._C.ops
+
+            if self.using_gqa_kernel:
+                block_size = value_cache.shape[2]
+                ipex_ops.paged_attention_gqa(
+                    out,
+                    decode_query,
                     key_cache,
                     value_cache,
+                    bsz,
+                    self.num_heads,
                     self.num_kv_heads,
                     self.scale,
-                    attn_metadata.block_tables,
-                    attn_metadata.seq_lens_tensor,
+                    decode_meta.block_tables,
+                    decode_meta.seq_lens_tensor,
                     block_size,
+                    head_size,
                     max_seq_len,
-                    self.alibi_slopes,
-                    self.kv_cache_dtype,
-                    layer._k_scale,
-                    layer._v_scale,
+                    self.kv_cache_dtype
                 )
             else:
-                # Run PagedAttention V2.
-                assert _PARTITION_SIZE % block_size == 0
-                tmp_output = torch.empty(
-                    size=(num_seqs, num_heads, max_num_partitions, head_size),
-                    dtype=output.dtype,
-                    device=output.device,
-                )
-                exp_sums = torch.empty(
-                    size=(num_seqs, num_heads, max_num_partitions),
-                    dtype=torch.float32,
-                    device=output.device,
-                )
-                max_logits = torch.empty_like(exp_sums)
-                ipex_ops.paged_attention_v2(
-                    output,
-                    exp_sums,
-                    max_logits,
-                    tmp_output,
-                    query,
-                    key_cache,
-                    value_cache,
-                    self.num_kv_heads,
-                    self.scale,
-                    attn_metadata.block_tables,
-                    attn_metadata.seq_lens_tensor,
-                    block_size,
-                    max_seq_len,
-                    self.alibi_slopes,
-                    self.kv_cache_dtype,
-                    layer._k_scale,
-                    layer._v_scale,
-                )
+                block_size = value_cache.shape[3]
+                use_v1 = (max_seq_len <= 8192 and
+                        (max_num_partitions == 1 or num_seqs * num_heads > 512))
+                if use_v1:
+                    # Run PagedAttention V1.
+                    ipex_ops.paged_attention_v1(
+                        out,
+                        decode_query,
+                        key_cache,
+                        value_cache,
+                        self.num_kv_heads,
+                        self.scale,
+                        decode_meta.block_tables,
+                        decode_meta.seq_lens_tensor,
+                        block_size,
+                        max_seq_len,
+                        self.alibi_slopes,
+                        self.kv_cache_dtype,
+                        layer._k_scale,
+                        layer._v_scale,
+                        self.logits_soft_cap,
+                    )
+                else:
+                    # Run PagedAttention V2.
+                    assert _PARTITION_SIZE % block_size == 0
+                    tmp_output = torch.empty(
+                        size=(num_seqs, num_heads, max_num_partitions, head_size),
+                        dtype=output.dtype,
+                        device=output.device,
+                    )
+                    exp_sums = torch.empty(
+                        size=(num_seqs, num_heads, max_num_partitions),
+                        dtype=torch.float32,
+                        device=output.device,
+                    )
+                    max_logits = torch.empty_like(exp_sums)
+                    ipex_ops.paged_attention_v2(
+                        out,
+                        exp_sums,
+                        max_logits,
+                        tmp_output,
+                        decode_query,
+                        key_cache,
+                        value_cache,
+                        self.num_kv_heads,
+                        self.scale,
+                        decode_meta.block_tables,
+                        decode_meta.seq_lens_tensor,
+                        block_size,
+                        max_seq_len,
+                        self.alibi_slopes,
+                        self.kv_cache_dtype,
+                        layer._k_scale,
+                        layer._v_scale,
+                        self.logits_soft_cap,
+                    )
+            output[num_prefill_tokens:] = out
 
             # Reshape the output tensor.
         return output.view(-1, self.num_heads * self.head_size)
diff --git a/vllm/attention/ops/blocksparse_attention/interface.py b/vllm/attention/ops/blocksparse_attention/interface.py
index 6ab69ea5b..9604f35f6 100644
--- a/vllm/attention/ops/blocksparse_attention/interface.py
+++ b/vllm/attention/ops/blocksparse_attention/interface.py
@@ -9,7 +9,8 @@ from vllm.platforms import current_platform
 from .utils import (dense_to_crow_col, get_head_sliding_step,
                     get_sparse_attn_mask)
 
-IS_COMPUTE_8_OR_ABOVE = current_platform.has_device_capability(80)
+IS_COMPUTE_8_OR_ABOVE = not current_platform.is_xpu(
+) and current_platform.has_device_capability(80)
 
 if IS_COMPUTE_8_OR_ABOVE:
     from .blocksparse_attention_kernel import blocksparse_flash_attn_varlen_fwd
diff --git a/vllm/attention/ops/ipex_attn.py b/vllm/attention/ops/ipex_attn.py
index 6d96f5832..c9ed08d44 100644
--- a/vllm/attention/ops/ipex_attn.py
+++ b/vllm/attention/ops/ipex_attn.py
@@ -2,15 +2,17 @@
 
 from typing import Dict, List, Optional, Tuple
 
-try:
-    import intel_extension_for_pytorch.llm.modules as ipex_modules
-    _use_ipex = True
-except ImportError:
-    _use_ipex = False
+# try:
+#     import intel_extension_for_pytorch.llm.modules as ipex_modules
+#     _use_ipex = True
+# except ImportError:
+#     _use_ipex = False
+_use_ipex = False
 
 import torch
 
 from vllm import _custom_ops as ops
+from vllm._ipex_ops import ipex_ops
 
 
 class _PagedAttention:
@@ -189,5 +191,44 @@ class _IPEXPagedAttention(_PagedAttention):
             scale, block_tables, context_lens, block_size, max_context_len,
             alibi_slopes)
 
+        return output
+
+    @staticmethod
+    def forward_prefix(
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+        kv_cache_dtype: str,
+        key_cache: torch.Tensor,
+        value_cache: torch.Tensor,
+        block_tables: torch.Tensor,
+        subquery_start_loc: torch.Tensor,
+        prompt_lens_tensor: torch.Tensor,
+        context_lens: torch.Tensor,
+        max_subquery_len: int,
+        alibi_slopes: Optional[torch.Tensor],
+        *args,
+    ) -> torch.Tensor:
+        raise NotImplementedError
+
+    @staticmethod
+    def swap_blocks(
+        src_kv_cache: torch.Tensor,
+        dst_kv_cache: torch.Tensor,
+        src_to_dst: Dict[int, int],
+        *args,
+    ) -> None:
+        ipex_ops.swap_blocks(src_kv_cache, dst_kv_cache, src_to_dst)
+
+    @staticmethod
+    def copy_blocks(
+        kv_caches: List[torch.Tensor],
+        src_to_dists: Dict[int, List[int]],
+        *args,
+    ) -> None:
+        key_caches = [kv_cache[0] for kv_cache in kv_caches]
+        value_caches = [kv_cache[1] for kv_cache in kv_caches]
+        ipex_ops.copy_blocks(key_caches, value_caches, src_to_dists)
+
 
 PagedAttention = _IPEXPagedAttention if _use_ipex else _PagedAttention
diff --git a/vllm/attention/ops/paged_attn.py b/vllm/attention/ops/paged_attn.py
index 827c3041a..9e7cb41e7 100644
--- a/vllm/attention/ops/paged_attn.py
+++ b/vllm/attention/ops/paged_attn.py
@@ -53,6 +53,7 @@ class PagedAttention:
         num_kv_heads: int,
         head_size: int,
     ) -> Tuple[torch.Tensor, torch.Tensor]:
+        # kv_cache in the shape of [2, num_blocks, block_size x num_kv_heads x head_dim]
         x = 16 // kv_cache.element_size()
         num_blocks = kv_cache.shape[1]
 
diff --git a/vllm/config.py b/vllm/config.py
index bd52fc90b..7d4e3555a 100644
--- a/vllm/config.py
+++ b/vllm/config.py
@@ -266,6 +266,8 @@ class ModelConfig:
         enable_sleep_mode: bool = False,
         override_generation_config: Optional[dict[str, Any]] = None,
         model_impl: Union[str, ModelImpl] = ModelImpl.AUTO,
+        low_bit_model_path: Optional[str] = None,
+        low_bit_save_path: Optional[str] = None,
     ) -> None:
         self.model = maybe_model_redirect(model)
         self.tokenizer = maybe_model_redirect(tokenizer)
@@ -283,6 +285,8 @@ class ModelConfig:
         self.rope_scaling = rope_scaling
         self.rope_theta = rope_theta
         self.model_impl = model_impl
+        self.low_bit_model_path = low_bit_model_path
+        self.low_bit_save_path = low_bit_save_path
 
         if hf_overrides is None:
             hf_overrides = {}
@@ -1383,6 +1387,7 @@ class LoadConfig:
     """
 
     load_format: Union[str, LoadFormat, "BaseModelLoader"] = LoadFormat.AUTO
+    use_low_bit_loader: bool = False
     download_dir: Optional[str] = None
     model_loader_extra_config: Optional[Union[str, dict]] = field(
         default_factory=dict)
@@ -3519,7 +3524,7 @@ class VllmConfig:
             quant_config = get_quant_config(model_config, load_config)
             capability_tuple = current_platform.get_device_capability()
 
-            if capability_tuple is not None:
+            if capability_tuple is not None and not current_platform.is_xpu():
                 capability = capability_tuple.to_int()
                 if capability < quant_config.get_min_capability():
                     raise ValueError(
diff --git a/vllm/engine/arg_utils.py b/vllm/engine/arg_utils.py
index 89c9b6747..a5be57ce0 100644
--- a/vllm/engine/arg_utils.py
+++ b/vllm/engine/arg_utils.py
@@ -210,6 +210,8 @@ class EngineArgs:
     enable_reasoning: Optional[bool] = None
     reasoning_parser: Optional[str] = None
     use_tqdm_on_load: bool = True
+    low_bit_model_path: Optional[str] = None
+    low_bit_save_path: Optional[str] = None
 
     def __post_init__(self):
         if not self.tokenizer:
@@ -991,6 +993,18 @@ class EngineArgs:
             "using. This is used to parse the reasoning content into OpenAI "
             "API format. Required for ``--enable-reasoning``.")
 
+        parser.add_argument(
+            "--low-bit-model-path",
+            type=nullable_str,
+            default=None,
+            help="Path for Low-bit loader")
+
+        parser.add_argument(
+            "--low-bit-save-path",
+            type=nullable_str,
+            default=None,
+            help="Path for Low-bit saver")
+        
         parser.add_argument(
             "--disable-cascade-attn",
             action="store_true",
@@ -1061,6 +1075,8 @@ class EngineArgs:
             override_generation_config=self.override_generation_config,
             enable_sleep_mode=self.enable_sleep_mode,
             model_impl=self.model_impl,
+            low_bit_model_path=self.low_bit_model_path,
+            low_bit_save_path=self.low_bit_save_path,
         )
 
     def create_load_config(self) -> LoadConfig:
@@ -1504,12 +1520,13 @@ class EngineArgs:
             _raise_or_fallback(feature_name=name, recommend_to_remove=True)
             return False
 
-        # Platforms must decide if they can support v1 for this model
-        if not current_platform.supports_v1(model_config=model_config):
-            _raise_or_fallback(
-                feature_name=f"device type={current_platform.device_type}",
-                recommend_to_remove=False)
-            return False
+        # # No support for device type other than CUDA, AMD (experiemntal) or
+        # # TPU (experimental) so far.
+        # if not (current_platform.is_cuda_alike() or current_platform.is_tpu()):
+        #     _raise_or_fallback(
+        #         feature_name=f"device type={current_platform.device_type}",
+        #         recommend_to_remove=False)
+        #     return False
         #############################################################
         # Experimental Features - allow users to opt in.
 
diff --git a/vllm/entrypoints/chat_utils.py b/vllm/entrypoints/chat_utils.py
index e48ffae9b..866a5052e 100644
--- a/vllm/entrypoints/chat_utils.py
+++ b/vllm/entrypoints/chat_utils.py
@@ -854,7 +854,8 @@ def _get_full_multimodal_text_prompt(placeholder_counts: dict[str, int],
 
     # NOTE: For now we always add missing placeholders at the front of
     # the prompt. This may change to be customizable in the future.
-    return "\n".join(missing_placeholders + [text_prompt])
+    #return "\n".join(missing_placeholders + [text_prompt])
+    return "".join(missing_placeholders + [text_prompt])
 
 
 # No need to validate using Pydantic again
diff --git a/vllm/entrypoints/openai/api_server.py b/vllm/entrypoints/openai/api_server.py
index 6a8bdd060..13c076df1 100644
--- a/vllm/entrypoints/openai/api_server.py
+++ b/vllm/entrypoints/openai/api_server.py
@@ -1111,6 +1111,8 @@ if __name__ == "__main__":
     # NOTE(simon):
     # This section should be in sync with vllm/entrypoints/cli/main.py for CLI
     # entrypoints.
+    logger.warning("Warning: Please use `ipex_llm.vllm.xpu.entrypoints.openai.api_server` "
+                   "instead of `vllm.entrypoints.openai.api_server` to start the API server")
     cli_env_setup()
     parser = FlexibleArgumentParser(
         description="vLLM OpenAI-Compatible RESTful API server.")
diff --git a/vllm/entrypoints/openai/serving_engine.py b/vllm/entrypoints/openai/serving_engine.py
index bbc8eddd8..852aa3ff7 100644
--- a/vllm/entrypoints/openai/serving_engine.py
+++ b/vllm/entrypoints/openai/serving_engine.py
@@ -35,7 +35,7 @@ from vllm.entrypoints.openai.protocol import (ChatCompletionRequest,
 from vllm.entrypoints.openai.serving_models import OpenAIServingModels
 from vllm.entrypoints.openai.tool_parsers import ToolParser
 # yapf: enable
-from vllm.inputs import TokensPrompt
+from vllm.inputs import TokensPrompt, TokenInputs
 from vllm.inputs.parse import parse_and_batch_prompt
 from vllm.logger import init_logger
 from vllm.lora.request import LoRARequest
@@ -451,8 +451,9 @@ class OpenAIServing:
                 prompt=tokenizer.decode(request_prompt),
                 prompt_token_ids=request_prompt)
 
-        engine_prompt = TokensPrompt(
-            prompt_token_ids=prompt_inputs["prompt_token_ids"])
+        engine_prompt = TokenInputs(
+            prompt_token_ids=prompt_inputs["prompt_token_ids"],
+            prompt=prompt_inputs["prompt"])
         if mm_data is not None:
             engine_prompt["multi_modal_data"] = mm_data
         if request.mm_processor_kwargs is not None:
diff --git a/vllm/executor/ray_distributed_executor.py b/vllm/executor/ray_distributed_executor.py
index 9b0b98731..dd6e6be9d 100644
--- a/vllm/executor/ray_distributed_executor.py
+++ b/vllm/executor/ray_distributed_executor.py
@@ -72,7 +72,7 @@ class RayDistributedExecutor(DistributedExecutorBase):
 
     def _init_executor(self) -> None:
         self.forward_dag: Optional[ray.dag.CompiledDAG] = None
-        if envs.VLLM_USE_V1:
+        if envs.VLLM_USE_V1 and not current_platform.is_xpu():
             # V1 uses SPMD worker and compiled DAG
             os.environ["VLLM_USE_RAY_SPMD_WORKER"] = "1"
             os.environ["VLLM_USE_RAY_COMPILED_DAG"] = "1"
@@ -124,10 +124,12 @@ class RayDistributedExecutor(DistributedExecutorBase):
                 self.driver_worker.execute_method)
 
     def shutdown(self) -> None:
+        '''
         logger.info(
             "Shutting down Ray distributed executor. If you see error log "
             "from logging.cc regarding SIGTERM received, please ignore because "
             "this is the expected termination process in Ray.")
+        '''
         if hasattr(self, "forward_dag") and self.forward_dag is not None:
             self.forward_dag.teardown()
             import ray
@@ -428,6 +430,7 @@ class RayDistributedExecutor(DistributedExecutorBase):
             else:
                 self.non_driver_workers.append(worker)
 
+
     def _driver_execute_model(
         self, execute_model_req: Optional[ExecuteModelRequest]
     ) -> Optional[List[SamplerOutput]]:
diff --git a/vllm/executor/ray_utils.py b/vllm/executor/ray_utils.py
index 37cc07bfb..e12cf7fba 100644
--- a/vllm/executor/ray_utils.py
+++ b/vllm/executor/ray_utils.py
@@ -281,6 +281,10 @@ def initialize_ray_cluster(
         ray_address: The address of the Ray cluster. If None, uses
             the default Ray cluster address.
     """
+    lowbit = os.getenv("IPEX_LLM_LOWBIT", None)
+    if lowbit is not None:
+        from ipex_llm.vllm.xpu.model_convert import _ipex_llm_convert
+        _ipex_llm_convert(lowbit)
     assert_ray_available()
     from vllm.platforms import current_platform
 
diff --git a/vllm/inputs/preprocess.py b/vllm/inputs/preprocess.py
index 669fb96e6..d28984b4b 100644
--- a/vllm/inputs/preprocess.py
+++ b/vllm/inputs/preprocess.py
@@ -339,6 +339,7 @@ class InputPreprocessor:
             tokens_content = parsed["content"]
 
             prompt_token_ids = tokens_content["prompt_token_ids"]
+            prompt = tokens_content["prompt"] if "prompt" in tokens_content else ""
             token_type_ids = tokens_content.get("token_type_ids")
             multi_modal_data = tokens_content.get("multi_modal_data")
             mm_processor_kwargs = tokens_content.get("mm_processor_kwargs")
@@ -354,6 +355,7 @@ class InputPreprocessor:
 
             return token_inputs(
                 prompt_token_ids=prompt_token_ids,
+                prompt=prompt,
                 token_type_ids=token_type_ids,
                 multi_modal_data=multi_modal_data,
                 mm_processor_kwargs=mm_processor_kwargs,
diff --git a/vllm/inputs/registry.py b/vllm/inputs/registry.py
index 0579893e5..dfb422ff5 100644
--- a/vllm/inputs/registry.py
+++ b/vllm/inputs/registry.py
@@ -330,7 +330,7 @@ class InputRegistry:
         from vllm.multimodal.profiling import MultiModalProfiler
         from vllm.sequence import SequenceData
 
-        if mm_registry.has_processor(model_config):
+        if False and mm_registry.has_processor(model_config):
             processor = mm_registry.create_processor(model_config,
                                                      disable_cache=True)
             profiler = MultiModalProfiler(processor)
diff --git a/vllm/lora/punica.py b/vllm/lora/punica.py
new file mode 100644
index 000000000..5f711bfe5
--- /dev/null
+++ b/vllm/lora/punica.py
@@ -0,0 +1,616 @@
+"""
+Based on:
+Chen, L., Ye, Z., Wu, Y., Zhuo, D., Ceze, L., & Krishnamurthy, A. (2023). 
+Punica: Multi-Tenant LoRA Serving. 
+https://arxiv.org/abs/2310.18547
+"""
+
+from typing import TYPE_CHECKING, Callable, List, Optional, Tuple, Union
+
+import torch
+
+from vllm.triton_utils import HAS_TRITON
+from vllm.utils import is_xpu
+
+if HAS_TRITON and not is_xpu():
+    from vllm.lora.ops.bgmv_expand import bgmv_expand
+    from vllm.lora.ops.bgmv_expand_slice import bgmv_expand_slice
+    from vllm.lora.ops.bgmv_shrink import bgmv_shrink
+    from vllm.lora.ops.sgmv_expand import sgmv_expand
+    from vllm.lora.ops.sgmv_expand_slice import sgmv_expand_slice
+    from vllm.lora.ops.sgmv_shrink import sgmv_shrink
+elif is_xpu():
+    from vllm._ipex_ops import ipex_ops
+    bgmv_expand = ipex_ops.bgmv_expand
+    bgmv_expand_slice = ipex_ops.bgmv_expand_slice
+    bgmv_shrink = ipex_ops.bgmv_shrink
+    sgmv_expand = ipex_ops.sgmv_expand
+    sgmv_expand_slice = ipex_ops.sgmv_expand_slice
+    sgmv_shrink = ipex_ops.sgmv_shrink
+
+if TYPE_CHECKING:
+    # avoid circuit import
+    from vllm.lora.layers import LoRAMapping
+    from vllm.lora.models import LongContextLoRAContext
+
+
+def compute_meta(
+    token_lora_tensor: torch.Tensor
+) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int, int, bool]:
+    """
+    Get the information required for the sgmv kernel. With the  features:
+    1. If consecutive requests in the batch use the same LoRA, this function
+    will combine them into a single request, improving sgmv kernel inference
+    performance.
+    2. At the beginning of each prefill stage inference, recalculations are
+    needed based on the input, but only once.
+    """
+
+    lora_indices_tensor, seq_length_tensor = torch.unique_consecutive(
+        token_lora_tensor, return_counts=True)
+    cum_result = torch.cumsum(seq_length_tensor, dim=0)
+    b_seq_start_tensor = torch.zeros_like(seq_length_tensor)
+    b_seq_start_tensor[1:].copy_(cum_result[:-1])
+    max_length = seq_length_tensor.max().item()
+
+    batch_size = lora_indices_tensor.size(0)
+    no_lora = False
+    # -1 means no lora should be applied. Use `no_lora` to determine whether
+    # the current step requires LoRA. If LoRA is not needed, the prefill stage
+    # does not need to launch the triton kernel, which can improve performance
+    if batch_size == 1 and lora_indices_tensor == -1:
+        no_lora = True
+    return (b_seq_start_tensor, seq_length_tensor, lora_indices_tensor,
+            batch_size, max_length, no_lora)
+
+
+# TODO see if this can be vectorized
+def convert_mapping(
+    mapping: "LoRAMapping",
+    lora_index_to_id: List[Optional[int]],
+    max_loras: int,
+    vocab_size: int,
+    extra_vocab_size: int,
+    long_lora_context: Optional["LongContextLoRAContext"] = None,
+    device: torch.device = "cuda",
+) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor,
+           Optional[torch.Tensor], List[int]]:
+    """Converts LoRAMapping to index tensors.
+
+    Args:
+        mapping: LoRAMapping mapping rows in a batch to LoRA ids.
+        lora_index_to_id: List mapping LoRA ids to LoRA indices.
+        max_loras: Maximum number of LoRAs.
+        vocab_size: Model vocab size.
+        extra_vocab_size: Extra vocab size each LoRA can have.
+        long_lora_context: Passed if there are long context lora in a batch.
+
+    Returns:
+        A tuple of tensors:
+            base_indices: Tensor of shape [batch_size] mapping batch rows to
+                LoRA indices.
+            sampler_indices: Tensor of shape [batch_size] mapping requests to
+                LoRA indices for sampler. For generation, this will be the
+                same as base_indicies. For prefill, this will map requests
+                to LoRA indices.
+            sampler_indices_padded: Tensor of shape [batch_size] mapping
+                requests to LoRA indices for sampler with padding.
+                Same as sampler_indicies, but -1 is replaced with
+                max_loras.
+            embeddings_indices: Tensor of shape [2, batch_size] mapping
+                requests to embedding indices. First row is for embeddings
+                added by the LoRAs, second row is for the LoRA.lora_a
+                embeddings.
+            long_lora_indices: Tensor of shape [batch_size] mapping
+                requests to RoPE offsets and rot dims for long LoRAs.
+                None if long context lora doesn't exist.
+            indices_len: List of lengths of the above tensors. It contains
+                (base_indices, sampler_indices, sampler_indices_padded,
+                embeddings_indices, long_lora_indices).
+    """
+    index_mapping_indices: List[int] = list(mapping.index_mapping).copy()
+    embedding_indices = index_mapping_indices.copy()
+    lora_indices = index_mapping_indices.copy()
+    long_lora_offsets: Optional[torch.Tensor] = None
+    if long_lora_context:
+        long_lora_offsets = torch.zeros(len(index_mapping_indices),
+                                        device=device,
+                                        dtype=torch.long)
+    prompt_mapping: List[int] = [
+        lora_index_to_id.index(x) if x > 0 else -1
+        for x in mapping.prompt_mapping
+    ]
+    lora_idx = None
+    for i in range(len(index_mapping_indices)):
+        # TODO index can be slow. optimize
+        lora_idx = (lora_index_to_id.index(index_mapping_indices[i])
+                    if index_mapping_indices[i] > 0 else -1)
+        embedding_indices[i] = lora_idx if index_mapping_indices[i] > 0 else 0
+        lora_indices[i] = lora_idx
+        if long_lora_context:
+            assert long_lora_offsets is not None
+            lora_offset: int = long_lora_context.offsets_by_lora_id.get(
+                index_mapping_indices[i], 0)
+            long_lora_offsets[i] = lora_offset
+
+    indices_list: List[Union[List[int], torch.Tensor]] = [
+        index_mapping_indices,
+        lora_indices,
+        embedding_indices,
+    ]
+    if long_lora_context:
+        assert long_lora_offsets is not None
+        indices_list.append(long_lora_offsets)
+    indices = torch.tensor(indices_list, dtype=torch.long, device=device)
+    prompt_mapping_tensor = torch.tensor(prompt_mapping,
+                                         device=device,
+                                         dtype=torch.long)
+    embeddings_indices = torch.stack([
+        indices[2] * extra_vocab_size,
+        indices[2] * (vocab_size + extra_vocab_size),
+    ])
+    embeddings_indices[embeddings_indices == -1] = max_loras - 1
+    base_indices = indices[1]
+    sampler_indices = prompt_mapping_tensor
+    sampler_indices_padded = sampler_indices.clone()
+    sampler_indices_padded[sampler_indices_padded == -1] = max_loras - 1
+    sampler_indices_padded = torch.arange(
+        0, len(sampler_indices_padded), device=device, dtype=torch.long) + (
+            sampler_indices_padded * len(sampler_indices_padded))
+    long_lora_indices = None
+    long_lora_indices_len: Optional[int] = None
+    if long_lora_context:
+        long_lora_indices = indices[3]
+        long_lora_indices_len = long_lora_indices.shape[-1]
+    # Contain length of indices tensors. Used to index into each tensor.
+    indices_len = [
+        base_indices.shape[-1],
+        sampler_indices.shape[-1],
+        sampler_indices_padded.shape[-1],
+        embeddings_indices.shape[-1],
+    ]
+    if long_lora_indices_len is not None:
+        indices_len.append(long_lora_indices_len)
+    else:
+        # If long_lora doesn't exist,append None
+        indices_len.append(None)
+
+    return (
+        base_indices,
+        sampler_indices,
+        sampler_indices_padded,
+        embeddings_indices,
+        long_lora_indices,
+        indices_len,
+    )
+
+
+class PunicaWrapper:
+    """
+    PunicaWrapper is designed to manage and provide metadata for the punica 
+    kernel. The main function  is to maintain the state information for 
+    Multi-LoRA, and to provide the interface for the punica kernel.
+    """
+
+    def __init__(self, max_num_batched_tokens: int, max_batches: int,
+                 device: torch.device):
+        self.device = device
+        self._token_lora_indices = torch.empty(max_num_batched_tokens,
+                                               dtype=torch.long,
+                                               device=device)
+        self._sampler_indices = torch.empty(max_num_batched_tokens,
+                                            dtype=torch.long,
+                                            device=device)
+        self._sampler_indices_padded = torch.empty(max_num_batched_tokens,
+                                                   dtype=torch.long,
+                                                   device=device)
+        self._embeddings_indices = torch.empty(2,
+                                               max_num_batched_tokens,
+                                               dtype=torch.long,
+                                               device=device)
+        self._long_lora_indices = torch.empty(max_num_batched_tokens,
+                                              dtype=torch.long,
+                                              device=device)
+
+        # 5 is the number of indicies tensors.
+        # base_indices, sampler_indices, sampler_indices_padded,
+        # embeddings_indices,long_lora_indices
+        self.indices_len: List[Optional[int]] = [None] * 5
+        # these attributes are the information required for sgmv kernel
+        self._seq_start_locs = torch.empty(max_batches,
+                                           dtype=torch.long,
+                                           device=device)
+        self._seq_lengths = torch.empty(max_batches,
+                                        dtype=torch.long,
+                                        device=device)
+        self._lora_indices_per_batch = torch.empty(max_batches,
+                                                   dtype=torch.long,
+                                                   device=device)
+        self.max_length: int = 0
+        self.batch_size: int = -1
+        self.is_prefill = False
+        self.no_lora = False
+
+    def update_metadata(
+        self,
+        mapping: "LoRAMapping",
+        lora_index_to_id: List[Optional[int]],
+        max_loras: int,
+        vocab_size: int,
+        extra_vocab_size: int,
+        long_lora_context: Optional["LongContextLoRAContext"] = None,
+    ):
+
+        self._update_base_metadata(mapping, lora_index_to_id, max_loras,
+                                   vocab_size, extra_vocab_size,
+                                   long_lora_context)
+        if mapping.is_prefill:
+            # Update metadata required for prefill-related operators.
+            self._update_prefill_metada(self.token_lora_indices)
+            self.is_prefill = True
+        else:
+            self.is_prefill = False
+
+    def _update_base_metadata(
+        self,
+        mapping: "LoRAMapping",
+        lora_index_to_id: List[Optional[int]],
+        max_loras: int,
+        vocab_size: int,
+        extra_vocab_size: int,
+        long_lora_context: Optional["LongContextLoRAContext"] = None,
+    ):
+        (
+            base_indices,
+            sampler_indices,
+            sampler_indices_padded,
+            embeddings_indices,
+            long_lora_offsets_tensor,
+            indices_len,
+        ) = convert_mapping(
+            mapping,
+            lora_index_to_id,
+            max_loras,
+            vocab_size,
+            extra_vocab_size,
+            long_lora_context,
+            self.device,
+        )
+        self._token_lora_indices[:base_indices.shape[0]].copy_(base_indices)
+        self._sampler_indices[:sampler_indices.shape[0]].copy_(sampler_indices)
+        self._sampler_indices_padded[:sampler_indices_padded.shape[0]].copy_(
+            sampler_indices_padded)
+        self._embeddings_indices[:embeddings_indices.
+                                 shape[0], :embeddings_indices.shape[1]].copy_(
+                                     embeddings_indices)
+        if long_lora_offsets_tensor is not None:
+            self._long_lora_indices[:long_lora_offsets_tensor.shape[0]].copy_(
+                long_lora_offsets_tensor)
+        else:
+            self._long_lora_indices.zero_()
+
+        self.indices_len[:] = indices_len
+
+    def _update_prefill_metada(self, token_lora_tensor: torch.Tensor) -> None:
+
+        (b_seq_start_tensor, seq_length_tensor, lora_indices_tensor,
+         batch_size, max_length, no_lora) = compute_meta(token_lora_tensor)
+
+        self._seq_start_locs[:b_seq_start_tensor.shape[0]].copy_(
+            b_seq_start_tensor)
+        self._seq_lengths[:seq_length_tensor.shape[0]].copy_(seq_length_tensor)
+        self._lora_indices_per_batch[:lora_indices_tensor.shape[0]].copy_(
+            lora_indices_tensor)
+        self.batch_size = batch_size
+        self.max_length = max_length
+        self.no_lora = no_lora
+
+    @property
+    def prefill_metadata(
+            self) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int, int]:
+        """
+        This property provides a convenient way to access the necessary 
+        metadata for prefill-related  kernel computations.
+            1. seq_start_locs: Tensor of sequence start positions
+            2. seq_lengths: Tensor of sequence lengths
+            3. lora_indices_per_batch: Tensor of lora indices, and an index of 
+                -1 means no lora should be applied.
+            4. batch_size: batch size after clustering identical lora indices
+            5. max_length: The maximum sequence length in the batch
+        """
+        return (self._seq_start_locs[:self.batch_size],
+                self._seq_lengths[:self.batch_size],
+                self._lora_indices_per_batch[:self.batch_size],
+                self.batch_size, self.max_length)
+
+    @property
+    def token_lora_indices(self) -> torch.Tensor:
+        """
+        This property provides the lora indices corresponding to each token 
+        in the batch. An index of -1 means no lora should be applied.
+        """
+        token_lora_len = self.indices_len[0]
+        return self._token_lora_indices[:token_lora_len]
+
+    @property
+    def sampler_indices(self) -> torch.Tensor:
+        """ 
+        This property is used to access the lora indices specifically for 
+        LogitsProcessorWithLoRA
+        """
+        sampler_indices_len = self.indices_len[1]
+        return self._sampler_indices[:sampler_indices_len]
+
+    @property
+    def sampler_indices_padded(self) -> torch.Tensor:
+        """
+        This property provides access to padded sampler indices
+        """
+        indices_padded_len = self.indices_len[2]
+        return self._sampler_indices_padded[:indices_padded_len]
+
+    @property
+    def embeddings_indices(self) -> torch.Tensor:
+        """
+        This property provides access to the indices used for lora embeddings, 
+        specifically for VocabParallelEmbeddingWithLoRA
+        """
+        embeddings_indices_len = self.indices_len[3]
+        return self._embeddings_indices[:, :embeddings_indices_len]
+
+    @property
+    def long_lora_indices(self) -> torch.Tensor:
+        """ 
+        This property provides access to the indices used for long context 
+        lora, specifically for LinearScalingRotaryEmbeddingWithLora
+        """
+        long_lora_len = self.indices_len[4]
+        return self._long_lora_indices[:long_lora_len]
+
+    def shrink_prefill(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        scale: float,
+    ):
+        #No LoRA request, so return directly
+        if self.no_lora:
+            return
+        sgmv_shrink(
+            x,
+            w_t_all,
+            y,
+            *self.prefill_metadata,
+            scale,
+        )
+
+    def shrink_decode(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        scale: float,
+    ):
+        bgmv_shrink(x, w_t_all, y, self.token_lora_indices, scale)
+
+    def expand_prefill(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        add_input: bool,
+    ):
+        #No LoRA request, so return directly
+        if self.no_lora:
+            return
+        sgmv_expand(
+            x,
+            w_t_all,
+            y,
+            *self.prefill_metadata,
+            add_input,
+        )
+
+    def expand_decode(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        add_input: bool,
+    ):
+        bgmv_expand(x, w_t_all, y, self.token_lora_indices, add_input)
+
+    def expand_slice_prefill(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        y_offset: Optional[int],
+        y_slice_size: Optional[int],
+        add_input: bool,
+    ):
+        #No LoRA request, so return directly
+        if self.no_lora:
+            return
+        sgmv_expand_slice(
+            x,
+            w_t_all,
+            y,
+            *self.prefill_metadata,
+            y_offset,
+            y_slice_size,
+            add_input,
+        )
+
+    def expand_slice_decode(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        y_offset: Optional[int],
+        y_slice_size: Optional[int],
+        add_input: bool,
+    ):
+        bgmv_expand_slice(x, w_t_all, y, self.token_lora_indices, y_offset,
+                          y_slice_size, add_input)
+
+    def add_shrink(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        scale: float,
+    ):
+        """
+        Perform the ` y+=x@w_t_all` computation, which is suitable for the
+        GEMM of lora'a.
+        When `is_prefill is` true, it indicates that it is currently the
+        prefill stage, and the `shrink_prefill` function should be called.
+        Otherwise, it is the decode stage, and the shrink_decode function
+        should be called.
+        """
+        shrink_fun: Callable = (self.shrink_prefill
+                                if self.is_prefill else self.shrink_decode)
+        shrink_fun(y, x, w_t_all, scale)
+
+    def add_expand(
+        self,
+        y: torch.Tensor,
+        x: torch.Tensor,
+        w_t_all: torch.Tensor,
+        add_input: bool = True,
+    ):
+        """
+        Perform the ` y+=x@w_t_all` computation, which is suitable for the
+        GEMM of lora'b.
+        When `is_prefill` is true, it indicates that it is currently the
+        prefill stage, and the `expand_prefill` function should be called.
+        Otherwise, it is the decode stage, and the expand_decode function
+        should be called.
+        """
+
+        expand_fun: Callable = (self.expand_prefill
+                                if self.is_prefill else self.expand_decode)
+        expand_fun(y, x, w_t_all, add_input)
+
+    def add_expand_slice(self,
+                         y: torch.Tensor,
+                         x: torch.Tensor,
+                         w_t_all: torch.Tensor,
+                         y_offset: Optional[int],
+                         y_slice_size: Optional[int],
+                         add_input: bool = True):
+        """
+        Similar to `add_expand`
+        """
+
+        expand_slice_fun: Callable = (self.expand_slice_prefill
+                                      if self.is_prefill else
+                                      self.expand_slice_decode)
+        expand_slice_fun(y, x, w_t_all, y_offset, y_slice_size, add_input)
+
+    def add_lora(self,
+                 y: torch.Tensor,
+                 x: torch.Tensor,
+                 wa_t_all: torch.Tensor,
+                 wb_t_all: torch.Tensor,
+                 scale: float,
+                 y_offset: Optional[int] = None,
+                 y_slice_size: Optional[int] = None,
+                 *,
+                 buffer: Optional[torch.Tensor] = None) -> None:
+        """
+        Semantics:
+        y[i] += (
+            x[i].unsqueeze(0)
+            @ wa_t_all[indices[i], layer_idx, :, :].transpose(-1, -2)
+            @ wb_t_all[indices[i], layer_idx, :, :].transpose(-1, -2)
+            * scale
+            ).squeeze(0)
+        Args:
+            y (torch.Tensor):  Output tensor. Will be changed in-place.
+            x (torch.Tensor): Input tensor
+            wa_t_all (torch.Tensor): lora_a's weight
+            wb_t_all (torch.Tensor): lora_b's weight
+            scale (float): Scaling factor.
+            y_offset (Optional[int], optional): Offset to apply to the starting
+                column of y.
+            y_slice_size (Optional[int], optional): Size of the y column slice..
+            buffer (Optional[torch.Tensor], optional): Defaults to None.
+        """
+        y_org = y
+        y = y.view(-1, y.shape[-1])
+        x = x.view(-1, x.shape[-1])
+        r = wb_t_all.size(-1)
+        if buffer is None:
+            # We set the buffer to be float32 by default ,refer to:
+            # https://github.com/triton-lang/triton/issues/1387
+            buffer = torch.zeros((x.size(0), r),
+                                 dtype=torch.float32,
+                                 device=x.device)
+
+        self.add_shrink(buffer, x, wa_t_all, scale)
+        if y_offset is None and y_slice_size is None:
+            self.add_expand(y, buffer, wb_t_all, add_input=True)
+        else:
+            self.add_expand_slice(y,
+                                  buffer,
+                                  wb_t_all,
+                                  y_offset,
+                                  y_slice_size,
+                                  add_input=True)
+        y = y.view_as(y_org)
+
+    def add_lora_packed_nslice(self, y: torch.Tensor, x: torch.Tensor,
+                               lora_a_stacked: Tuple[torch.Tensor,
+                                                     torch.Tensor,
+                                                     torch.Tensor],
+                               lora_b_stacked: Tuple[torch.Tensor,
+                                                     torch.Tensor,
+                                                     torch.Tensor],
+                               scale: float,
+                               output_slices: Tuple[int, ...]) -> None:
+        """
+        Applies lora to each input. Similar to add_lora, This method is 
+        used for layers that are composed of multiple sublayers
+        (slices) packed together.
+        """
+        y_org = y
+        x = x.view(-1, x.shape[-1])
+        y = y.view(-1, y.shape[-1])
+        offset_left = 0
+        # TODO fuse these kernels
+        for slice_idx in range(len(output_slices)):
+            self.add_lora(y, x, lora_a_stacked[slice_idx],
+                          lora_b_stacked[slice_idx], scale, offset_left,
+                          output_slices[slice_idx])
+            offset_left += output_slices[slice_idx]
+
+        y = y.view_as(y_org)
+
+    def add_lora_logits(self,
+                        y: torch.Tensor,
+                        x: torch.Tensor,
+                        wa_t_all: torch.Tensor,
+                        wb_t_all: torch.Tensor,
+                        scale,
+                        *,
+                        buffer: Optional[torch.Tensor] = None) -> None:
+        """
+        LogitsProcessorWithLoRA always using bgmv
+        """
+        y_org = y
+        y = y.view(-1, y.shape[-1])
+        x = x.view(-1, x.shape[-1])
+        r = wb_t_all.size(-1)
+        if buffer is None:
+            # We set the buffer to be float32 by default ,refer to:
+            # https://github.com/triton-lang/triton/issues/1387
+            buffer = torch.zeros((x.size(0), r),
+                                 dtype=torch.float32,
+                                 device=x.device)
+
+        bgmv_shrink(x, wa_t_all, buffer, self.sampler_indices, scale)
+        bgmv_expand(buffer, wb_t_all, y, self.sampler_indices, add_inputs=True)
+        y = y.view_as(y_org)
diff --git a/vllm/lora/punica_wrapper/punica_gpu.py b/vllm/lora/punica_wrapper/punica_gpu.py
index bb6d2808e..341339b13 100644
--- a/vllm/lora/punica_wrapper/punica_gpu.py
+++ b/vllm/lora/punica_wrapper/punica_gpu.py
@@ -13,10 +13,14 @@ import torch
 import vllm.envs as envs
 from vllm.lora.layers import LoRAMapping
 from vllm.triton_utils import HAS_TRITON
+from vllm.platforms import current_platform
 
-if HAS_TRITON:
+if HAS_TRITON and not current_platform.is_xpu():
     from vllm.lora.ops.triton_ops import (LoRAKernelMeta, lora_expand,
                                           lora_shrink)
+elif current_platform.is_xpu():
+    # TODO(xiangyu): check here
+    pass
 
 from .punica_base import PunicaWrapperBase
 
diff --git a/vllm/model_executor/layers/activation.py b/vllm/model_executor/layers/activation.py
index 1de0f499c..675d601fa 100644
--- a/vllm/model_executor/layers/activation.py
+++ b/vllm/model_executor/layers/activation.py
@@ -260,6 +260,12 @@ class QuickGELU(CustomOp):
         out = torch.empty_like(x)
         self.op(out, x)
         return out
+    # def forward_xpu(self, x: torch.Tensor) -> torch.Tensor:
+    #     from vllm._ipex_ops import ipex_ops as ops
+
+    #     out = torch.empty_like(x)
+    #     ops.gelu_quick(out, x)
+    #     return out
 
     # TODO implement forward_xpu for QuickGELU
     # def forward_xpu(self, x: torch.Tensor) -> torch.Tensor:
diff --git a/vllm/model_executor/layers/fused_moe/ipex_llm_moe.py b/vllm/model_executor/layers/fused_moe/ipex_llm_moe.py
new file mode 100644
index 000000000..d55d78b77
--- /dev/null
+++ b/vllm/model_executor/layers/fused_moe/ipex_llm_moe.py
@@ -0,0 +1,372 @@
+# SPDX-License-Identifier: Apache-2.0
+
+from abc import abstractmethod
+from enum import Enum
+from typing import Callable, List, Optional, Tuple
+import os
+
+import torch
+from torch import nn
+import torch.nn.functional as F
+from torch.nn.parameter import UninitializedParameter
+
+import vllm.envs as envs
+from vllm.config import get_current_vllm_config
+from vllm.distributed import (get_dp_group, get_tensor_model_parallel_rank,
+                              get_tensor_model_parallel_world_size,
+                              tensor_model_parallel_all_reduce)
+from vllm.forward_context import ForwardContext, get_forward_context
+from vllm.logger import init_logger
+from vllm.model_executor.custom_op import CustomOp
+from vllm.model_executor.layers.quantization.base_config import (
+    QuantizationConfig, QuantizeMethodBase)
+from vllm.model_executor.utils import set_weight_attrs
+from vllm.platforms import current_platform
+from vllm.platforms.interface import CpuArchEnum
+from vllm.utils import direct_register_custom_op
+
+fused_experts = None  # type: ignore
+fused_moe_pallas = None  # type: ignore
+if current_platform.is_xpu():
+    from .moe_pallas import fused_moe_xpu
+else:
+    fused_moe_xpu = None  # type: ignore
+logger = init_logger(__name__)
+
+from ipex_llm.ggml.quantize import ggml_tensor_qtype, gguf_mixed_qtype
+import ipex_llm.ggml.model.llama.llama_cpp as ggml
+from ipex_llm.transformers.low_bit_linear import LowBitLinear, FP4Params, \
+        FP16Linear, BF16Linear, ggml_convert_qtype, ggml_int4_convert_fp32
+
+from vllm.model_executor.layers.fused_moe import FusedMoEMethodBase
+from vllm.model_executor.layers.quantization.gguf import GGUFUninitializedParameter
+from torch.nn.parameter import Parameter, UninitializedParameter
+from vllm.model_executor.layers.activation import SiluAndMul
+
+
+import gguf
+from gguf import GGMLQuantizationType as WeightType
+import ipex_llm.ggml.model.llama.llama_cpp as ggml
+from ipex_llm.ggml.quantize import ggml_tensor_qtype
+
+from ipex_llm.transformers.low_bit_linear import MatMulLowBit
+import xe_linear
+import xe_batch
+import xe_addons
+
+from vllm._ipex_ops import ipex_ops
+import vllm._C.ops
+
+# @CustomOp.register("unquantized_fused_moe")
+class IPEXLLMFusedMoEMethod(FusedMoEMethodBase):
+    """MoE method without quantization."""
+
+    def create_weights(self, layer: torch.nn.Module, num_experts: int,
+                       hidden_size: int, intermediate_size_per_partition: int,
+                       params_dtype: torch.dtype, **extra_weight_attrs):
+
+        # Fused gate_up_proj (column parallel)
+        w13_weight = torch.nn.Parameter(torch.empty(
+            num_experts,
+            2 * intermediate_size_per_partition,
+            hidden_size,
+            dtype=params_dtype),
+                                        requires_grad=False)
+        layer.register_parameter("w13_weight", w13_weight)
+        set_weight_attrs(w13_weight, extra_weight_attrs)
+
+        # down_proj (row parallel)
+        w2_weight = torch.nn.Parameter(torch.empty(
+            num_experts,
+            hidden_size,
+            intermediate_size_per_partition,
+            dtype=params_dtype),
+                                       requires_grad=False)
+        layer.register_parameter("w2_weight", w2_weight)
+        set_weight_attrs(w2_weight, extra_weight_attrs)
+
+
+    def process_weights_after_loading(self, layer: torch.nn.Module) -> None:
+        # w1: [num_experts, intermediate_size * 2, hidden_size]
+        # w2: [num_experts, hidden_size, intermediate_size]
+        self.num_experts = layer.w13_weight.data.shape[0]
+        self.intermediate_size = layer.w2_weight.data.shape[2]
+        self.hidden_size = layer.w13_weight.data.shape[2]
+
+        local_rank = os.environ["LOCAL_RANK"]
+        self.device = torch.device(f"xpu:{local_rank}")
+        lowbit = os.getenv("IPEX_LLM_LOWBIT", "sym_int4")
+        qtype = ggml_tensor_qtype[lowbit]
+        self.qtype = qtype
+
+        w13_params = []
+        for i in range(self.num_experts):
+            cur_params = FP4Params(data=layer.w13_weight.data[i,:,:],
+                                    requires_grad=False,
+                                    quantized=False,
+                                    _shape=None,
+                                    convert_shape_only=False,
+                                    qtype=self.qtype).to(self.device)
+            w13_params.append(cur_params)
+        layer._parameters['w13_weight'] = None
+        self.qw1_weight = w13_params
+        
+        w2_params = []
+        for i in range(self.num_experts):
+            cur_params = FP4Params(data=layer.w2_weight.data[i,:,:],
+                                    requires_grad=False,
+                                    quantized=False,
+                                    _shape=None,
+                                    convert_shape_only=False,
+                                    qtype=self.qtype).to(self.device)
+            w2_params.append(cur_params)
+        layer._parameters['w2_weight'] = None
+        self.qw2_weight = w2_params
+
+        w1 = self.qw1_weight
+        self.w1_addrs = [expert.data_ptr() for expert in w1]
+        self.w1_addrs = torch.tensor(self.w1_addrs, device=self.device, dtype=torch.uint64)
+        w2 = self.qw2_weight
+        self.w2_addrs = [expert.data_ptr() for expert in w2]
+        self.w2_addrs = torch.tensor(self.w2_addrs, device=self.device, dtype=torch.uint64)
+
+        logger.warning_once("model processed.")
+
+
+    def apply(
+        self,
+        layer: torch.nn.Module,
+        x: torch.Tensor,
+        router_logits: torch.Tensor,
+        top_k: int,
+        renormalize: bool,
+        use_grouped_topk: bool = False,
+        topk_group: Optional[int] = None,
+        num_expert_group: Optional[int] = None,
+        global_num_experts: int = -1,
+        expert_map: Optional[torch.Tensor] = None,
+        custom_routing_function: Optional[Callable] = None,
+        scoring_func: str = "softmax",
+        e_score_correction_bias: Optional[torch.Tensor] = None,
+        apply_router_weight_on_input: bool = False,
+        activation: str = "silu",
+    ) -> torch.Tensor:
+        return self.forward_xpu(
+            x=x,
+            layer=layer,
+            router_logits=router_logits,
+            top_k=top_k,
+            renormalize=renormalize,
+            use_grouped_topk=use_grouped_topk,
+            topk_group=topk_group,
+            num_expert_group=num_expert_group,
+            global_num_experts=global_num_experts,
+            expert_map=expert_map,
+            custom_routing_function=custom_routing_function,
+            scoring_func=scoring_func,
+            e_score_correction_bias=e_score_correction_bias,
+            activation=activation,
+            apply_router_weight_on_input=apply_router_weight_on_input)
+
+
+    def forward_xpu(
+        self,
+        layer: torch.nn.Module,
+        x: torch.Tensor,
+        use_grouped_topk: bool,
+        top_k: int,
+        router_logits: torch.Tensor,
+        renormalize: bool,
+        topk_group: Optional[int] = None,
+        num_expert_group: Optional[int] = None,
+        global_num_experts: int = -1,
+        expert_map: Optional[torch.Tensor] = None,
+        custom_routing_function: Optional[Callable] = None,
+        scoring_func: str = "softmax",
+        e_score_correction_bias: Optional[torch.Tensor] = None,
+        activation: str = "silu",
+        apply_router_weight_on_input: bool = False,
+        **kwargs,
+    ):
+        num_tokens = x.shape[:-1].numel()
+        if not envs.VLLM_USE_V1 and num_tokens > 256:
+            return self.fused_moe_xpu(hidden_states=x,
+                                    w1=self.qw1_weight,
+                                    w2=self.qw2_weight,
+                                    topk=top_k,
+                                    gating_output=router_logits,
+                                    global_num_experts=global_num_experts,
+                                    expert_map=expert_map,
+                                    renormalize=renormalize)
+        else:
+            return self.fused_moe_xpu_decode(hidden_states=x,
+                                    w1=self.qw1_weight,
+                                    w2=self.qw2_weight,
+                                    topk=top_k,
+                                    gating_output=router_logits,
+                                    global_num_experts=global_num_experts,
+                                    expert_map=expert_map,
+                                    renormalize=renormalize)
+
+    def fused_moe_xpu_decode(
+        self,
+        hidden_states: torch.Tensor,
+        w1,
+        w2,
+        gating_output: torch.Tensor,
+        topk: int,
+        global_num_experts,
+        expert_map,
+        renormalize: bool,
+    ) -> torch.Tensor:
+        """
+        Args:
+            hidden_states: [*, hidden_size]
+            w1: [num_experts, intermediate_size * 2, hidden_size]
+            w2: [num_experts, hidden_size, intermediate_size]
+            gating_output: [*, num_experts]
+        """
+        orig_shape = hidden_states.shape
+        hidden_size = hidden_states.shape[-1]
+        num_tokens = hidden_states.shape[:-1].numel()
+        num_experts = self.num_experts
+        intermediate_size = self.intermediate_size
+        qtype = self.qtype
+
+        device = hidden_states.device
+        dtype = hidden_states.dtype
+        hidden_states = hidden_states.view(num_tokens, hidden_size)
+        gating_output = gating_output.view(num_tokens, global_num_experts)
+        # topk_weights, topk_indices = F.softmax(gating_output, dim=-1, dtype=torch.float).topk(topk, dim=-1)
+        # if renormalize:
+        #     topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)
+        topk_indices, topk_weights = xe_addons.moe_softmax_topk(gating_output, topk, renormalize)
+        topk_weights = topk_weights.to(dtype)
+        if expert_map is not None:
+            expert_map = expert_map.to(device=device)
+            topk_indices = expert_map[topk_indices]
+
+        topk_indices = topk_indices.flatten()
+        token_indices = torch.arange(num_tokens, device=device).repeat_interleave(topk)
+        
+        # padding_len = cur_topk_indices[cur_topk_indices == -1].shape[0]
+
+        x = hidden_states[token_indices]
+
+        # x: [bsz * seq_len * num_selected_experts, hidden_size]
+        # w1_out: [bsz * seq_len * num_selected_experts, intermediate_size * 2]
+        # topk_indices: [bsz * seq_len * num_selected_experts]
+        # res: [bsz * seq_len * num_selected_experts, hidden_size]
+        x = vllm._C.ops.fused_moe_forward(x, topk_indices, self.w1_addrs, self.w2_addrs, hidden_size, intermediate_size, qtype)
+
+        # if padding_len > 0:
+        #     x = vllm._C.ops.fused_moe_forward(x[padding_len:], cur_topk_indices[padding_len:], self.w1_addrs, self.w2_addrs, hidden_size, intermediate_size, qtype)
+        # else:
+        #     x = vllm._C.ops.fused_moe_forward(x, cur_topk_indices, self.w1_addrs, self.w2_addrs, hidden_size, intermediate_size, qtype)
+
+        # if padding_len > 0:
+        #     padding_shape = (padding_len, hidden_size)
+        #     padding_x = torch.zeros(padding_shape, dtype=x.dtype, device=x.device)
+        #     x = torch.cat((padding_x, x), dim=0)
+
+        x = x.reshape(-1, topk, hidden_size)
+        x = x * topk_weights.unsqueeze_(dim=-1)
+        x = x.sum(dim=-2)
+        x = x.reshape(orig_shape)
+        return x
+
+    def fused_moe_xpu(
+        self,
+        hidden_states: torch.Tensor,
+        w1,
+        w2,
+        gating_output: torch.Tensor,
+        topk: int,
+        global_num_experts,
+        expert_map,
+        renormalize: bool,
+    ) -> torch.Tensor:
+        """
+        Args:
+            hidden_states: [*, hidden_size]
+            w1: [num_experts, intermediate_size * 2, hidden_size]
+            w2: [num_experts, hidden_size, intermediate_size]
+            gating_output: [*, num_experts]
+        """
+        orig_shape = hidden_states.shape
+        hidden_size = hidden_states.shape[-1]
+        num_tokens = hidden_states.shape[:-1].numel()
+        num_experts = self.num_experts
+        intermediate_size = self.intermediate_size
+        qtype = self.qtype
+
+        device = hidden_states.device
+        dtype = hidden_states.dtype
+        hidden_states = hidden_states.view(num_tokens, hidden_size)
+        gating_output = gating_output.view(num_tokens, global_num_experts)
+        # topk_weights, topk_indices = F.softmax(gating_output, dim=-1, dtype=torch.float).topk(topk, dim=-1)
+        # if renormalize:
+        #     topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)
+        topk_indices, topk_weights = xe_addons.moe_softmax_topk(gating_output, topk, renormalize)
+        topk_weights = topk_weights.to(dtype)
+        if expert_map is not None:
+            expert_map = expert_map.to(device=device)
+            topk_indices = expert_map[topk_indices]
+
+        topk_indices = topk_indices.flatten()
+        topk_argsort_indices = topk_indices.argsort()
+        topk_argsort_revert_indices = topk_argsort_indices.argsort()
+        token_indices = torch.arange(num_tokens, device=device).repeat_interleave(topk)
+        token_indices = token_indices[topk_argsort_indices]
+        group_sizes = custom_histogram(topk_indices.to(torch.int32), 0, num_experts - 1)
+        
+        x = hidden_states[token_indices]
+
+        x = custom_gmm(x, w1, group_sizes, intermediate_size * 2)
+        # x = F.silu(x[..., :intermediate_size]) * x[..., intermediate_size:]
+        output = torch.zeros((x.shape[0], intermediate_size), device=x.device, dtype=x.dtype)
+        ipex_ops.silu_and_mul(output, x)
+        x = output
+        x = custom_gmm(x, w2, group_sizes, hidden_size)
+        x = x[topk_argsort_revert_indices].reshape(-1, topk, hidden_size)
+
+        x = x * topk_weights.unsqueeze_(dim=-1)
+        x = x.sum(dim=-2)
+        x = x.reshape(orig_shape)
+        return x
+
+def custom_histogram(indices, min, max):
+    bin_counts = torch.histc(indices, bins=max - min + 1, min=min, max=max).to(torch.int32)
+    return bin_counts
+
+def custom_gmm(x, w, group_sizes, out_len):
+    result = torch.zeros(
+            (x.shape[0], out_len),
+            dtype=x.dtype,
+            device=x.device
+        )
+    start = 0
+    i = 0
+    
+    lowbit = os.getenv("IPEX_LLM_LOWBIT", "sym_int4")
+    qtype = ggml_tensor_qtype[lowbit]
+    for end_index in group_sizes.tolist():
+        if end_index > 0:
+            end = start + end_index
+
+            # result[start:end] = torch.matmul(x[start:end], w[i])
+            
+            # cur_x = x[start:end].contiguous()
+            # cur_w = xe_linear.dequant(cur_x, w[i].contiguous(), qtype)
+            # result[start:end] = torch.matmul(cur_x, cur_w.T)
+
+            cur_x = x[start:end].contiguous()
+            cur_w = w[i]
+            # print(cur_x.shape, " ", cur_w.shape, " ", out_len)
+            cur_res = xe_linear.forward_new(cur_x, cur_w, qtype, out_len)
+            # print(cur_res.shape)
+            result[start:end] = cur_res
+
+            start = end
+        i += 1
+    return result
diff --git a/vllm/model_executor/layers/fused_moe/layer.py b/vllm/model_executor/layers/fused_moe/layer.py
index 0e35d8a80..235f92bf8 100644
--- a/vllm/model_executor/layers/fused_moe/layer.py
+++ b/vllm/model_executor/layers/fused_moe/layer.py
@@ -32,6 +32,10 @@ if current_platform.is_tpu():
     from .moe_torch_iterative import fused_moe as fused_moe_pallas
 else:
     fused_moe_pallas = None  # type: ignore
+if current_platform.is_xpu():
+    from .moe_pallas import fused_moe_xpu
+else:
+    fused_moe_xpu = None  # type: ignore
 logger = init_logger(__name__)
 
 
@@ -132,16 +136,15 @@ class UnquantizedFusedMoEMethod(FusedMoEMethodBase, CustomOp):
             layer.w2_weight = torch.nn.Parameter(shuffled_w2,
                                                  requires_grad=False)
 
-        if current_platform.is_cpu():
-            if current_platform.get_cpu_architecture() == CpuArchEnum.X86:
-                import intel_extension_for_pytorch as ipex
-                layer.ipex_fusion = ipex.llm.modules.GatedMLPMOE(
-                    layer.w13_weight,
-                    layer.w2_weight,
-                    use_prepack=envs.VLLM_CPU_MOE_PREPACK,
-                )
-            else:
-                raise NotImplementedError("CPU MOE only supports x86 arch.")
+        if current_platform.is_xpu() or (current_platform.is_cpu() and current_platform.get_cpu_architecture() == CpuArchEnum.X86):
+            import intel_extension_for_pytorch as ipex
+            layer.ipex_fusion = ipex.llm.modules.GatedMLPMOE(
+                layer.w13_weight,
+                layer.w2_weight,
+                use_prepack=envs.VLLM_CPU_MOE_PREPACK,
+            )
+        else:
+            raise NotImplementedError("CPU MOE only supports x86 arch.")
 
     def apply(
         self,
@@ -254,6 +257,42 @@ class UnquantizedFusedMoEMethod(FusedMoEMethodBase, CustomOp):
             e_score_correction_bias,
         )
 
+    def forward_xpu(
+        self,
+        layer: torch.nn.Module,
+        x: torch.Tensor,
+        use_grouped_topk: bool,
+        top_k: int,
+        router_logits: torch.Tensor,
+        renormalize: bool,
+        topk_group: Optional[int] = None,
+        num_expert_group: Optional[int] = None,
+        global_num_experts: int = -1,
+        expert_map: Optional[torch.Tensor] = None,
+        custom_routing_function: Optional[Callable] = None,
+        scoring_func: str = "softmax",
+        e_score_correction_bias: Optional[torch.Tensor] = None,
+        activation: str = "silu",
+        apply_router_weight_on_input: bool = False,
+        **kwargs,
+    ):
+        assert custom_routing_function is None
+        return layer.ipex_fusion(
+            x,
+            use_grouped_topk,
+            top_k,
+            router_logits,
+            renormalize,
+            topk_group,
+            num_expert_group,
+        )
+        # return fused_moe_xpu(hidden_states=x,
+        #                         w1=layer.w13_weight,
+        #                         w2=layer.w2_weight,
+        #                         topk=top_k,
+        #                         gating_output=router_logits,
+        #                         renormalize=renormalize)
+
     def forward_hpu(
         self,
         layer: torch.nn.Module,
@@ -495,8 +534,15 @@ class FusedMoE(torch.nn.Module):
         # Note: get_quant_method will look at the layer's local_num_experts
         # for heuristic purposes, so it must be initialized first.
         if quant_config is None:
-            self.quant_method: Optional[QuantizeMethodBase] = (
-                UnquantizedFusedMoEMethod())
+            import os
+            lowbit = os.getenv("IPEX_LLM_LOWBIT", None)
+            if lowbit is not None:
+                from vllm.model_executor.layers.fused_moe.ipex_llm_moe import IPEXLLMFusedMoEMethod
+                self.quant_method: Optional[QuantizeMethodBase] = (
+                    IPEXLLMFusedMoEMethod())
+            else:
+                self.quant_method: Optional[QuantizeMethodBase] = (
+                    UnquantizedFusedMoEMethod())
         else:
             self.quant_method = quant_config.get_quant_method(self, prefix)
         assert self.quant_method is not None
diff --git a/vllm/model_executor/layers/fused_moe/moe_pallas.py b/vllm/model_executor/layers/fused_moe/moe_pallas.py
index 0365afa10..1e30b61d6 100644
--- a/vllm/model_executor/layers/fused_moe/moe_pallas.py
+++ b/vllm/model_executor/layers/fused_moe/moe_pallas.py
@@ -2,10 +2,68 @@
 
 import torch
 import torch.nn.functional as F
-from torch_xla.experimental.custom_kernel import _histogram
+# from torch_xla.experimental.custom_kernel import _histogram
 
 
-def fused_moe(
+# def fused_moe(
+#     hidden_states: torch.Tensor,
+#     w1: torch.Tensor,
+#     w2: torch.Tensor,
+#     gating_output: torch.Tensor,
+#     topk: int,
+#     renormalize: bool,
+# ) -> torch.Tensor:
+#     """
+#     Args:
+#         hidden_states: [*, hidden_size]
+#         w1: [num_experts, intermediate_size * 2, hidden_size]
+#         w2: [num_experts, hidden_size, intermediate_size]
+#         gating_output: [*, num_experts]
+#     """
+#     orig_shape = hidden_states.shape
+#     hidden_size = hidden_states.shape[-1]
+#     num_tokens = hidden_states.shape[:-1].numel()
+#     num_experts = w1.shape[0]
+#     intermediate_size = w2.shape[-1]
+#     device = hidden_states.device
+#     dtype = hidden_states.dtype
+#     assert (num_tokens * topk) % 16 == 0, (
+#         "The Pallas GMM kernel requires num_tokens * topk to be a multiple of "
+#         f"16 but got {num_tokens * topk}")
+
+#     hidden_states = hidden_states.view(num_tokens, hidden_size)
+#     gating_output = gating_output.view(num_tokens, num_experts)
+#     topk_weights = gating_output.softmax(dim=-1, dtype=torch.float)
+#     topk_weights, topk_indices = topk_weights.topk(topk, dim=-1)
+#     if renormalize:
+#         topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)
+#     topk_weights = topk_weights.to(dtype)
+
+#     topk_indices = topk_indices.flatten()
+#     topk_argsort_indices = topk_indices.argsort()
+#     topk_argsort_revert_indices = topk_argsort_indices.argsort()
+#     token_indices = torch.arange(num_tokens,
+#                                  device=device).repeat_interleave(topk)
+#     token_indices = token_indices[topk_argsort_indices]
+#     group_sizes = _histogram(topk_indices.to(torch.int32), 0, num_experts - 1)
+
+#     # NOTE(woosuk): The GMM Pallas kernel requires a different weight layout
+#     # from HF Transformers.
+#     w1 = w1.transpose(1, 2)
+#     w2 = w2.transpose(1, 2)
+
+#     x = hidden_states[token_indices]
+#     x = torch.ops.xla.gmm(x, w1, group_sizes)
+#     x = F.silu(x[..., :intermediate_size]) * x[..., intermediate_size:]
+#     x = torch.ops.xla.gmm(x, w2, group_sizes)
+#     x = x[topk_argsort_revert_indices].reshape(-1, topk, hidden_size)
+
+#     x = x * topk_weights.unsqueeze_(dim=-1)
+#     x = x.sum(dim=-2)
+#     x = x.reshape(orig_shape)
+#     return x
+
+def fused_moe_xpu(
     hidden_states: torch.Tensor,
     w1: torch.Tensor,
     w2: torch.Tensor,
@@ -27,14 +85,9 @@ def fused_moe(
     intermediate_size = w2.shape[-1]
     device = hidden_states.device
     dtype = hidden_states.dtype
-    assert (num_tokens * topk) % 16 == 0, (
-        "The Pallas GMM kernel requires num_tokens * topk to be a multiple of "
-        f"16 but got {num_tokens * topk}")
-
     hidden_states = hidden_states.view(num_tokens, hidden_size)
     gating_output = gating_output.view(num_tokens, num_experts)
-    topk_weights = gating_output.softmax(dim=-1, dtype=torch.float)
-    topk_weights, topk_indices = topk_weights.topk(topk, dim=-1)
+    topk_weights, topk_indices = F.softmax(gating_output, dim=-1, dtype=torch.float).topk(topk, dim=-1)
     if renormalize:
         topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)
     topk_weights = topk_weights.to(dtype)
@@ -42,23 +95,40 @@ def fused_moe(
     topk_indices = topk_indices.flatten()
     topk_argsort_indices = topk_indices.argsort()
     topk_argsort_revert_indices = topk_argsort_indices.argsort()
-    token_indices = torch.arange(num_tokens,
-                                 device=device).repeat_interleave(topk)
+    token_indices = torch.arange(num_tokens, device=device).repeat_interleave(topk)
     token_indices = token_indices[topk_argsort_indices]
-    group_sizes = _histogram(topk_indices.to(torch.int32), 0, num_experts - 1)
+    group_sizes = custom_histogram(topk_indices.to(torch.int32), 0, num_experts - 1)
 
-    # NOTE(woosuk): The GMM Pallas kernel requires a different weight layout
-    # from HF Transformers.
     w1 = w1.transpose(1, 2)
     w2 = w2.transpose(1, 2)
-
     x = hidden_states[token_indices]
-    x = torch.ops.xla.gmm(x, w1, group_sizes)
+    x = custom_gmm(x, w1, group_sizes)
     x = F.silu(x[..., :intermediate_size]) * x[..., intermediate_size:]
-    x = torch.ops.xla.gmm(x, w2, group_sizes)
+    x = custom_gmm(x, w2, group_sizes)
     x = x[topk_argsort_revert_indices].reshape(-1, topk, hidden_size)
 
     x = x * topk_weights.unsqueeze_(dim=-1)
     x = x.sum(dim=-2)
     x = x.reshape(orig_shape)
     return x
+
+def custom_histogram(indices, min, max):
+    bin_counts = torch.histc(indices, bins=max - min + 1, min=min, max=max).to(torch.int32)
+    return bin_counts
+
+
+def custom_gmm(x, w, group_sizes):
+    result = torch.zeros(
+            (x.shape[0], w.shape[-1]),
+            dtype=x.dtype,
+            device=x.device
+        )
+    start = 0
+    i = 0
+    for end_index in group_sizes.tolist():
+        if end_index > 0:
+            end = start + end_index
+            result[start:end] = torch.matmul(x[start:end], w[i])
+            start = end
+        i += 1
+    return result
diff --git a/vllm/model_executor/layers/layernorm.py b/vllm/model_executor/layers/layernorm.py
index 5e8eb6c54..4b31fc8e3 100644
--- a/vllm/model_executor/layers/layernorm.py
+++ b/vllm/model_executor/layers/layernorm.py
@@ -200,11 +200,14 @@ class RMSNorm(CustomOp):
                 self.variance_epsilon,
             )
             return x, residual
-        return ops.rms_norm(
+        out = torch.empty_like(x)
+        ops.rms_norm(
+            out,
             x,
             self.weight.data,
             self.variance_epsilon,
         )
+        return out
 
     def extra_repr(self) -> str:
         s = f"hidden_size={self.weight.data.size(0)}"
diff --git a/vllm/model_executor/layers/linear.py b/vllm/model_executor/layers/linear.py
index 1ae574072..02ae72be5 100644
--- a/vllm/model_executor/layers/linear.py
+++ b/vllm/model_executor/layers/linear.py
@@ -179,8 +179,8 @@ class UnquantizedLinearMethod(LinearMethodBase):
                                        input_size_per_partition,
                                        dtype=params_dtype),
                            requires_grad=False)
-        set_weight_attrs(weight, {"input_dim": 1, "output_dim": 0})
         layer.register_parameter("weight", weight)
+        set_weight_attrs(weight, {"input_dim": 1, "output_dim": 0})
         set_weight_attrs(weight, extra_weight_attrs)
 
     def apply(self,
diff --git a/vllm/model_executor/layers/quantization/gptq.py b/vllm/model_executor/layers/quantization/gptq.py
index 1c8d6cb1e..5191ae702 100644
--- a/vllm/model_executor/layers/quantization/gptq.py
+++ b/vllm/model_executor/layers/quantization/gptq.py
@@ -249,6 +249,7 @@ class GPTQLinearMethod(LinearMethodBase):
 
         # exllama needs to shuffle the weight after the weight is loaded
         # here we do the shuffle on first forward pass
+        '''
         if layer.exllama_state == ExllamaState.UNINITIALIZED:
             if self.quant_config.desc_act:
                 layer.g_idx.data = torch.argsort(layer.g_idx).to(torch.int)
@@ -259,7 +260,7 @@ class GPTQLinearMethod(LinearMethodBase):
             layer.exllama_state = ExllamaState.READY
             ops.gptq_shuffle(layer.qweight, layer.g_idx,
                              self.quant_config.weight_bits)
-
+        '''
     def apply(self,
               layer: torch.nn.Module,
               x: torch.Tensor,
diff --git a/vllm/model_executor/layers/quantization/ipex_quant.py b/vllm/model_executor/layers/quantization/ipex_quant.py
index c09cc13cb..65b58af2f 100644
--- a/vllm/model_executor/layers/quantization/ipex_quant.py
+++ b/vllm/model_executor/layers/quantization/ipex_quant.py
@@ -99,13 +99,14 @@ class IPEXConfig(QuantizationConfig):
     @classmethod
     def override_quantization_method(cls, hf_quant_cfg,
                                      user_quant) -> Optional[str]:
-        if not current_platform.is_cpu() and not current_platform.is_xpu():
-            return None
+        # not use IPEXConfig
+        # if not current_platform.is_cpu() and not current_platform.is_xpu():
+        #     return None
 
-        quant_method = hf_quant_cfg.get("quant_method", "").lower()
+        # quant_method = hf_quant_cfg.get("quant_method", "").lower()
 
-        if quant_method in ["awq", "gptq"]:
-            return cls.get_name()
+        # if quant_method in ["awq", "gptq"]:
+        #     return cls.get_name()
 
         return None
 
diff --git a/vllm/model_executor/layers/rotary_embedding.py b/vllm/model_executor/layers/rotary_embedding.py
index 624ed63ab..f826db908 100644
--- a/vllm/model_executor/layers/rotary_embedding.py
+++ b/vllm/model_executor/layers/rotary_embedding.py
@@ -956,37 +956,39 @@ class MRotaryEmbedding(RotaryEmbedding):
         """
         assert positions.ndim == 1 or positions.ndim == 2
 
-        num_tokens = positions.shape[-1]
-        cos_sin = self.cos_sin_cache[positions]
-        cos, sin = cos_sin.chunk(2, dim=-1)
-        if positions.ndim == 2:
-            assert self.mrope_section
-
-            cos = torch.cat([
-                m[i]
-                for i, m in enumerate(cos.split(self.mrope_section, dim=-1))
-            ],
-                            dim=-1)
-            sin = torch.cat([
-                m[i]
-                for i, m in enumerate(sin.split(self.mrope_section, dim=-1))
-            ],
-                            dim=-1)
-
-        query_shape = query.shape
-        query = query.view(num_tokens, -1, self.head_size)
-        query_rot = query[..., :self.rotary_dim]
-        query_pass = query[..., self.rotary_dim:]
-        query_rot = _apply_rotary_emb(query_rot, cos, sin, self.is_neox_style)
-        query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
-
-        key_shape = key.shape
-        key = key.view(num_tokens, -1, self.head_size)
-        key_rot = key[..., :self.rotary_dim]
-        key_pass = key[..., self.rotary_dim:]
-        key_rot = _apply_rotary_emb(key_rot, cos, sin, self.is_neox_style)
-        key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
-        return query, key
+        return self.forward_xpu(positions, query, key)
+
+        # num_tokens = positions.shape[-1]
+        # cos_sin = self.cos_sin_cache[positions]
+        # cos, sin = cos_sin.chunk(2, dim=-1)
+        # if positions.ndim == 2:
+        #     assert self.mrope_section
+
+        #     cos = torch.cat([
+        #         m[i]
+        #         for i, m in enumerate(cos.split(self.mrope_section, dim=-1))
+        #     ],
+        #                     dim=-1)
+        #     sin = torch.cat([
+        #         m[i]
+        #         for i, m in enumerate(sin.split(self.mrope_section, dim=-1))
+        #     ],
+        #                     dim=-1)
+
+        # query_shape = query.shape
+        # query = query.view(num_tokens, -1, self.head_size)
+        # query_rot = query[..., :self.rotary_dim]
+        # query_pass = query[..., self.rotary_dim:]
+        # query_rot = _apply_rotary_emb(query_rot, cos, sin, self.is_neox_style)
+        # query = torch.cat((query_rot, query_pass), dim=-1).reshape(query_shape)
+
+        # key_shape = key.shape
+        # key = key.view(num_tokens, -1, self.head_size)
+        # key_rot = key[..., :self.rotary_dim]
+        # key_pass = key[..., self.rotary_dim:]
+        # key_rot = _apply_rotary_emb(key_rot, cos, sin, self.is_neox_style)
+        # key = torch.cat((key_rot, key_pass), dim=-1).reshape(key_shape)
+        # return query, key
 
     @staticmethod
     def get_input_positions(
diff --git a/vllm/model_executor/layers/vocab_parallel_embedding.py b/vllm/model_executor/layers/vocab_parallel_embedding.py
index 1eb0c8c2e..27d246069 100644
--- a/vllm/model_executor/layers/vocab_parallel_embedding.py
+++ b/vllm/model_executor/layers/vocab_parallel_embedding.py
@@ -32,8 +32,8 @@ class UnquantizedEmbeddingMethod(QuantizeMethodBase):
                                        input_size_per_partition,
                                        dtype=params_dtype),
                            requires_grad=False)
-        set_weight_attrs(weight, {"input_dim": 1, "output_dim": 0})
         layer.register_parameter("weight", weight)
+        set_weight_attrs(weight, {"input_dim": 1, "output_dim": 0})
         set_weight_attrs(weight, extra_weight_attrs)
 
     def apply(self,
diff --git a/vllm/model_executor/model_loader/loader.py b/vllm/model_executor/model_loader/loader.py
index 5649cf2dd..66e30984e 100644
--- a/vllm/model_executor/model_loader/loader.py
+++ b/vllm/model_executor/model_loader/loader.py
@@ -1481,8 +1481,41 @@ class RunaiModelStreamerLoader(BaseModelLoader):
         return model.eval()
 
 
+class IPEXLLMLowBitLoader(BaseModelLoader):
+    def __init__(self, load_config: LoadConfig):
+        super().__init__(load_config)
+        logger.info("IPEXLLMLowBitLoader get selected. Ensure your model is converted before.")
+        if load_config.model_loader_extra_config:
+            raise ValueError(f"Model loader extra config is not supported for "
+                             f"load format {load_config.load_format}")
+
+    def download_model(self, model_config: ModelConfig) -> None:
+        """Download a model so that it can be immediately loaded."""
+        raise ValueError(f"IPEXLLMLowBitLoader does not support "
+                         f"download_model api.")
+
+    def load_model(self, vllm_config: VllmConfig) -> nn.Module:
+        model_config = vllm_config.model_config
+
+        from ipex_llm.optimize import low_memory_init, load_low_bit
+        with set_default_torch_dtype(model_config.dtype):
+            # Initialize an empty skeleton of the model
+            with low_memory_init():
+                model = _initialize_model(vllm_config=vllm_config)
+        # Load the real weights from the config
+        local_rank = os.environ["LOCAL_RANK"]
+        load_path = os.path.join(model_config.low_bit_model_path,
+                                 str(local_rank))
+        model = load_low_bit(model, load_path)
+        return model
+
+
 def get_model_loader(load_config: LoadConfig) -> BaseModelLoader:
     """Get a model loader based on the load format."""
+
+    if load_config.use_low_bit_loader:
+        return IPEXLLMLowBitLoader(load_config)
+
     if isinstance(load_config.load_format, type):
         return load_config.load_format(load_config)
 
diff --git a/vllm/model_executor/models/baichuan.py b/vllm/model_executor/models/baichuan.py
index 6a3112b5f..7e2b7c862 100644
--- a/vllm/model_executor/models/baichuan.py
+++ b/vllm/model_executor/models/baichuan.py
@@ -47,7 +47,7 @@ from vllm.model_executor.sampling_metadata import SamplingMetadata
 from vllm.sequence import IntermediateTensors
 
 from .interfaces import SupportsLoRA, SupportsPP, SupportsQuant
-from .utils import (AutoWeightsLoader, is_pp_missing_parameter,
+from .utils import (is_pp_missing_parameter,
                     make_empty_intermediate_tensors_factory, make_layers)
 
 
@@ -321,45 +321,6 @@ class BaiChuanModel(nn.Module):
         hidden_states, _ = self.norm(hidden_states, residual)
         return hidden_states
 
-    def load_weights(self, weights: Iterable[Tuple[str,
-                                                   torch.Tensor]]) -> Set[str]:
-        stacked_params_mapping = [
-            # (param_name, shard_name, shard_id)
-            ("gate_up_proj", "gate_proj", 0),
-            ("gate_up_proj", "up_proj", 1),
-        ]
-        params_dict = dict(self.named_parameters())
-        loaded_params: Set[str] = set()
-        for name, loaded_weight in weights:
-            if "rotary_emb.inv_freq" in name:
-                continue
-
-            for (param_name, weight_name, shard_id) in stacked_params_mapping:
-                if weight_name not in name:
-                    continue
-                name = name.replace(weight_name, param_name)
-                # Skip loading extra bias for GPTQ models.
-                if name.endswith(".bias") and name not in params_dict:
-                    continue
-                if is_pp_missing_parameter(name, self):
-                    continue
-                param = params_dict[name]
-                weight_loader = param.weight_loader
-                weight_loader(param, loaded_weight, shard_id)
-                break
-            else:
-                # Skip loading extra bias for GPTQ models.
-                if name.endswith(".bias") and name not in params_dict:
-                    continue
-                if is_pp_missing_parameter(name, self):
-                    continue
-                param = params_dict[name]
-                weight_loader = getattr(param, "weight_loader",
-                                        default_weight_loader)
-                weight_loader(param, loaded_weight)
-            loaded_params.add(name)
-        return loaded_params
-
 
 class BaiChuanBaseForCausalLM(nn.Module, SupportsLoRA, SupportsPP,
                               SupportsQuant):
@@ -392,7 +353,6 @@ class BaiChuanBaseForCausalLM(nn.Module, SupportsLoRA, SupportsPP,
         self.lm_head = ParallelLMHead(config.vocab_size,
                                       config.hidden_size,
                                       quant_config=quant_config)
-        self.lm_head.weight.weight_loader = self.lm_head_weight_loader
         if self.config.tie_word_embeddings:
             self.lm_head.weight = self.model.embed_tokens.weight
         self.logits_processor = LogitsProcessor(config.vocab_size)
@@ -433,22 +393,53 @@ class BaiChuanBaseForCausalLM(nn.Module, SupportsLoRA, SupportsPP,
 
     def load_weights(self, weights: Iterable[Tuple[str,
                                                    torch.Tensor]]) -> Set[str]:
-        loader = AutoWeightsLoader(self)
-        return loader.load_weights(weights)
-
-    def lm_head_weight_loader(self, param: nn.Parameter,
-                              loaded_weight: torch.Tensor):
-        # Unlike Baichuan, Baichuan2 normalizes the head weights.
-        # Refer to:
-        # https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat/blob/84603cde5ebffb6084e476cfaeceaf0b8b91fe54/modeling_baichuan.py#L508
-        # Distinguish between Baichuan and Baichuan2 by checking the
-        # vocab size. This is suggested by
-        # https://github.com/vllm-project/vllm/pull/1022#discussion_r1325652704
-        is_baichuan2 = self.config.vocab_size == 125696
-        if is_baichuan2:
-            loaded_weight = torch.nn.functional.normalize(loaded_weight)
-
-        default_weight_loader(param, loaded_weight)
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+        params_dict = dict(self.named_parameters())
+        loaded_params: Set[str] = set()
+        for name, loaded_weight in weights:
+            if "rotary_emb.inv_freq" in name:
+                continue
+            if name == "lm_head.weight":
+                # Unlike Baichuan, Baichuan2 normalizes the head weights.
+                # Refer to:
+                # https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat/blob/84603cde5ebffb6084e476cfaeceaf0b8b91fe54/modeling_baichuan.py#L508
+                # Distinguish between Baichuan and Baichuan2 by checking the
+                # vocab size. This is suggested by
+                # https://github.com/vllm-project/vllm/pull/1022#discussion_r1325652704
+                is_baichuan2 = self.config.vocab_size == 125696
+                if is_baichuan2:
+                    loaded_weight = torch.nn.functional.normalize(
+                        loaded_weight)
+
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                if weight_name not in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                # Skip loading extra bias for GPTQ models.
+                if name.endswith(".bias") and name not in params_dict:
+                    continue
+                if is_pp_missing_parameter(name, self):
+                    continue
+                param = params_dict[name]
+                weight_loader = getattr(param, "weight_loader",
+                                        default_weight_loader)
+                weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
 
 
 class BaichuanForCausalLM(BaiChuanBaseForCausalLM):
diff --git a/vllm/model_executor/models/chatglm.py b/vllm/model_executor/models/chatglm.py
index 1b1738f88..2c2ed67b9 100644
--- a/vllm/model_executor/models/chatglm.py
+++ b/vllm/model_executor/models/chatglm.py
@@ -130,12 +130,14 @@ class GLMMLP(nn.Module):
     def __init__(
         self,
         config: ChatGLMConfig,
+        layer,
         quant_config: Optional[QuantizationConfig] = None,
         prefix: str = "",
     ):
         super().__init__()
 
         self.add_bias = config.add_bias_linear
+        self.layer = layer
 
         # Project to 4h.
         self.dense_h_to_4h = MergedColumnParallelLinear(
@@ -160,7 +162,14 @@ class GLMMLP(nn.Module):
     def forward(self, hidden_states):
         # [s, b, 4hp]
         intermediate_parallel, _ = self.dense_h_to_4h(hidden_states)
-        intermediate_parallel = self.activation_func(intermediate_parallel)
+        # IPEX-LLM changes start: workaround fp16 overflow
+        if self.layer >= 38 and intermediate_parallel.device.type == "xpu":
+            d = intermediate_parallel.shape[-1] // 2
+            intermediate_parallel[..., d:] /= 10
+            intermediate_parallel = self.activation_func(intermediate_parallel)
+        else:
+            intermediate_parallel = self.activation_func(intermediate_parallel)
+        # IPEX-LLM changes end.
         # [s, b, h]
         output, _ = self.dense_4h_to_h(intermediate_parallel)
         return output
@@ -176,6 +185,7 @@ class GLMBlock(nn.Module):
     def __init__(
         self,
         config: ChatGLMConfig,
+        layer,
         cache_config: Optional[CacheConfig] = None,
         quant_config: Optional[QuantizationConfig] = None,
         prefix: str = "",
@@ -203,7 +213,9 @@ class GLMBlock(nn.Module):
             config.hidden_size, eps=config.layernorm_epsilon)
 
         # MLP
-        self.mlp = GLMMLP(config, quant_config, prefix=f"{prefix}.mlp")
+        self.mlp = GLMMLP(config, layer, quant_config, prefix=f"{prefix}.mlp")
+
+        self.layer = layer
 
     def forward(
         self,
@@ -235,8 +247,13 @@ class GLMBlock(nn.Module):
             residual = layernorm_output
         else:
             residual = layernorm_input
-
-        output = self.mlp(layernorm_output) + residual
+        # IPEX-LLM changes start: workaround fp16 overflow
+        if self.layer >= 38 and layernorm_output.device.type == "xpu":
+            output = self.mlp(layernorm_output) * 10 + residual
+            output = torch.nan_to_num(output)
+        else:
+            output = self.mlp(layernorm_output) + residual
+        # ipex-llm changes end
 
         return output
 
@@ -258,12 +275,15 @@ class GLMTransformer(nn.Module):
         self.num_layers = config.num_layers
 
         # Transformer layers.
-        self.start_layer, self.end_layer, self.layers = make_layers(
-            self.num_layers,
-            lambda prefix: GLMBlock(
-                config, cache_config, quant_config, prefix=prefix),
-            prefix=f"{prefix}.layers",
-        )
+        # Not sure if pp is available now
+        from vllm.distributed.utils import get_pp_indices
+        self.start_layer, self.end_layer = get_pp_indices(self.num_layers,
+                                                get_pp_group().rank_in_group,
+                                                get_pp_group().world_size)
+        self.layers = nn.ModuleList([
+            GLMBlock(config, i, cache_config, quant_config, prefix=f"{prefix}.layers.{i}")
+            for i in range(self.start_layer, self.end_layer)
+        ])
 
         if self.post_layer_norm:
             layer_norm_func = RMSNorm if config.rmsnorm else LayerNorm
diff --git a/vllm/model_executor/models/glm4.py b/vllm/model_executor/models/glm4.py
new file mode 100644
index 000000000..07453f636
--- /dev/null
+++ b/vllm/model_executor/models/glm4.py
@@ -0,0 +1,312 @@
+# SPDX-License-Identifier: Apache-2.0
+
+# Copyright 2025 The Zhipu AI team.
+# Copyright 2023 The vLLM team.
+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only GLM-4-0414 model compatible with HuggingFace weights."""
+from typing import Iterable, Optional, Set, Tuple, Union
+
+import torch
+from torch import nn
+from transformers import Glm4Config
+
+from vllm.attention import Attention, AttentionType
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.distributed import get_pp_group, get_tensor_model_parallel_world_size
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (QKVParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import ParallelLMHead
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+
+from .interfaces import SupportsLoRA, SupportsPP
+from .llama import LlamaMLP as Glm4MLP
+from .llama import LlamaModel
+from .utils import AutoWeightsLoader, PPMissingLayer, maybe_prefix
+
+
+class Glm4Attention(nn.Module):
+
+    def __init__(self,
+                 config: Glm4Config,
+                 hidden_size: int,
+                 num_heads: int,
+                 num_kv_heads: int,
+                 max_position: int = 4096 * 32,
+                 head_dim: Optional[int] = None,
+                 qkv_bias: bool = False,
+                 rope_theta: float = 10000,
+                 cache_config: Optional[CacheConfig] = None,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 rope_scaling: Optional[Tuple] = None,
+                 prefix: str = "",
+                 attn_type: str = AttentionType.DECODER) -> None:
+        super().__init__()
+        self.hidden_size = hidden_size
+        tp_size = get_tensor_model_parallel_world_size()
+        self.total_num_heads = num_heads
+        assert self.total_num_heads % tp_size == 0
+        self.num_heads = self.total_num_heads // tp_size
+        self.total_num_kv_heads = num_kv_heads
+        if self.total_num_kv_heads >= tp_size:
+            # Number of KV heads is greater than TP size, so we partition
+            # the KV heads across multiple tensor parallel GPUs.
+            assert self.total_num_kv_heads % tp_size == 0
+        else:
+            # Number of KV heads is less than TP size, so we replicate
+            # the KV heads across multiple tensor parallel GPUs.
+            assert tp_size % self.total_num_kv_heads == 0
+        partial_rotary_factor = getattr(config, "partial_rotary_factor", 0.5)
+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
+        self.head_dim = head_dim or hidden_size // self.total_num_heads
+        self.rotary_dim = int(partial_rotary_factor * self.head_dim)
+        self.q_size = self.num_heads * self.head_dim
+        self.kv_size = self.num_kv_heads * self.head_dim
+        self.scaling = self.head_dim**-0.5
+        self.rope_theta = rope_theta
+        self.qkv_proj = QKVParallelLinear(
+            hidden_size,
+            self.head_dim,
+            self.total_num_heads,
+            self.total_num_kv_heads,
+            bias=qkv_bias,
+            quant_config=quant_config,
+            prefix=f"{prefix}.qkv_proj",
+        )
+        self.o_proj = RowParallelLinear(
+            self.total_num_heads * self.head_dim,
+            hidden_size,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.o_proj",
+        )
+        self.rotary_emb = get_rope(
+            self.head_dim,
+            rotary_dim=self.rotary_dim,
+            max_position=max_position,
+            base=self.rope_theta,
+            rope_scaling=rope_scaling,
+            partial_rotary_factor=partial_rotary_factor,
+            is_neox_style=False,
+        )
+        self.attn = Attention(self.num_heads,
+                              self.head_dim,
+                              self.scaling,
+                              num_kv_heads=self.num_kv_heads,
+                              cache_config=cache_config,
+                              quant_config=quant_config,
+                              prefix=f"{prefix}.attn",
+                              attn_type=attn_type)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+    ) -> torch.Tensor:
+        qkv, _ = self.qkv_proj(hidden_states)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        q, k = self.rotary_emb(positions, q, k)
+        attn_output = self.attn(q, k, v)
+        output, _ = self.o_proj(attn_output)
+        return output
+
+
+class Glm4DecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: Glm4Config,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        rope_theta = getattr(config, "rope_theta", 1000000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+
+        self.self_attn = Glm4Attention(
+            config=config,
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            max_position=config.max_position_embeddings,
+            num_kv_heads=config.num_key_value_heads,
+            rope_theta=rope_theta,
+            qkv_bias=getattr(config, 'attention_bias', False),
+            head_dim=getattr(config, 'head_dim', None),
+            cache_config=cache_config,
+            quant_config=quant_config,
+            rope_scaling=rope_scaling,
+            prefix=f"{prefix}.self_attn",
+            attn_type=AttentionType.DECODER,
+        )
+        self.mlp = Glm4MLP(
+            hidden_size=self.hidden_size,
+            intermediate_size=config.intermediate_size,
+            hidden_act=config.hidden_act,
+            quant_config=quant_config,
+            prefix=f"{prefix}.mlp",
+        )
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+        self.post_self_attn_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+        self.post_mlp_layernorm = RMSNorm(config.hidden_size,
+                                          eps=config.rms_norm_eps)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor],
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        # Self Attention
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+        hidden_states = self.self_attn(
+            positions=positions,
+            hidden_states=hidden_states,
+        )
+
+        hidden_states = self.post_self_attn_layernorm(hidden_states)
+
+        # Fully Connected
+        hidden_states, residual = self.post_attention_layernorm(hidden_states, residual)
+        hidden_states = self.mlp(hidden_states)
+        hidden_states = self.post_mlp_layernorm(hidden_states)
+
+        return hidden_states, residual
+
+
+ALL_DECODER_LAYER_TYPES = {
+    "attention": Glm4DecoderLayer,
+}
+
+
+@support_torch_compile(
+    dynamic_arg_dims={
+        "input_ids": 0,
+        "positions": -1,
+        "intermediate_tensors": 0,
+        "inputs_embeds": 0,
+    })
+class Glm4Model(LlamaModel):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__(vllm_config=vllm_config,
+                         prefix=prefix,
+                         layer_type=Glm4DecoderLayer)
+
+
+class Glm4ForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
+    packed_modules_mapping = {
+        "qkv_proj": [
+            "q_proj",
+            "k_proj",
+            "v_proj",
+        ],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+
+        self.config = config
+        self.lora_config = lora_config
+
+        self.quant_config = quant_config
+        self.model = Glm4Model(vllm_config=vllm_config,
+                               prefix=maybe_prefix(prefix, "model"))
+
+        if get_pp_group().is_last_rank:
+            if config.tie_word_embeddings:
+                self.lm_head = self.model.embed_tokens
+            else:
+                self.lm_head = ParallelLMHead(config.vocab_size,
+                                              config.hidden_size,
+                                              quant_config=quant_config,
+                                              prefix=maybe_prefix(
+                                                  prefix, "lm_head"))
+        else:
+            self.lm_head = PPMissingLayer()
+
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+        self.sampler = get_sampler()
+
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.model.get_input_embeddings(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        hidden_states = self.model(input_ids, positions, intermediate_tensors,
+                                   inputs_embeds)
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.logits_processor(self.lm_head, hidden_states,
+                                       sampling_metadata)
+        return logits
+
+    def sample(
+        self,
+        logits: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        next_tokens = self.sampler(logits, sampling_metadata)
+        return next_tokens
+
+    def load_weights(self, weights: Iterable[Tuple[str,
+                                                   torch.Tensor]]) -> Set[str]:
+        loader = AutoWeightsLoader(
+            self,
+            skip_prefixes=(["lm_head."]
+                           if self.config.tie_word_embeddings else None),
+        )
+        return loader.load_weights(weights)
\ No newline at end of file
diff --git a/vllm/model_executor/models/glm4v.py b/vllm/model_executor/models/glm4v.py
index c190a4585..195f48f64 100644
--- a/vllm/model_executor/models/glm4v.py
+++ b/vllm/model_executor/models/glm4v.py
@@ -111,8 +111,11 @@ class EVA2CLIPAttention(nn.Module):
             prefix=f"{prefix}.dense",
         )
 
-        self.attn = MultiHeadAttention(self.num_heads_per_rank, self.head_dim,
-                                       self.scale)
+        # self.attn = MultiHeadAttention(self.num_heads_per_rank, self.head_dim,
+        #                                self.scale)
+        from siglip import SelfAttention
+        self.attn = SelfAttention(self.num_heads_per_rank, self.head_dim,
+                                self.scale)
         self.output_dropout = torch.nn.Dropout(config.dropout_prob)
 
     def forward(self, x: torch.Tensor) -> torch.Tensor:
diff --git a/vllm/model_executor/models/minicpmv.py b/vllm/model_executor/models/minicpmv.py
index 5fab9df3f..f8e6fbe24 100644
--- a/vllm/model_executor/models/minicpmv.py
+++ b/vllm/model_executor/models/minicpmv.py
@@ -149,6 +149,8 @@ class Resampler2_5(BaseResampler):
         self.max_size = max_size
         self._set_2d_pos_cache(self.max_size)
 
+        #self.apply(self._init_weights)
+
     def _set_2d_pos_cache(self,
                           max_size: Tuple[int, int],
                           device: torch.types.Device = "cpu") -> None:
@@ -1236,7 +1238,8 @@ class MiniCPMV2_5(MiniCPMVBaseModel, SupportsLoRA):
         return self.resampler(vision_embedding, tgt_sizes)
 
 
-class MiniCPMV2_6(MiniCPMVBaseModel, SupportsLoRA):
+class MiniCPMV2_6(MiniCPMVBaseModel):
+
     packed_modules_mapping = {
         "qkv_proj": [
             "q_proj",
@@ -1319,9 +1322,9 @@ class MiniCPMV2_6(MiniCPMVBaseModel, SupportsLoRA):
             patch_attn_mask[i, :num_patches_item] = True
 
         vision_embedding = self.vpm(
-            all_pixel_values,
+            all_pixel_values.type(dtype).to(device),
             patch_attention_mask=patch_attn_mask.unsqueeze(1),
-            tgt_sizes=tgt_sizes,
+            tgt_sizes=tgt_sizes.to(device),
         )
 
         return self.resampler(vision_embedding, tgt_sizes)
@@ -1363,7 +1366,7 @@ class MiniCPMV(MiniCPMVBaseModel, SupportsMultiModal, SupportsLoRA):
 
         # quant_config references base class members,
         # so update values before init is called
-        cls.packed_modules_mapping.update(instance_cls.packed_modules_mapping)
-        cls.embedding_modules.update(instance_cls.embedding_modules)
-        cls.embedding_padding_modules += instance_cls.embedding_padding_modules
+        # cls.packed_modules_mapping.update(instance_cls.packed_modules_mapping)
+        # cls.embedding_modules.update(instance_cls.embedding_modules)
+        # cls.embedding_padding_modules += instance_cls.embedding_padding_modules
         return instance_cls(vllm_config=vllm_config, prefix=prefix)
diff --git a/vllm/model_executor/models/na_vit.py b/vllm/model_executor/models/na_vit.py
new file mode 100644
index 000000000..d96085f46
--- /dev/null
+++ b/vllm/model_executor/models/na_vit.py
@@ -0,0 +1,831 @@
+import logging
+import math
+import os
+import warnings
+from typing import Optional, Tuple, Union
+
+import numpy as np
+import torch
+import torch.nn.functional as F
+from torch import nn
+from torch.nn.init import _calculate_fan_in_and_fan_out
+from transformers.activations import ACT2FN
+from transformers.configuration_utils import PretrainedConfig
+from transformers.modeling_attn_mask_utils import _prepare_4d_attention_mask
+from transformers.modeling_outputs import (BaseModelOutput,
+                                           BaseModelOutputWithPooling)
+from transformers.modeling_utils import PreTrainedModel
+from transformers.utils import (ModelOutput, is_flash_attn_2_available,
+                                replace_return_docstrings)
+
+logger = logging.getLogger("vllm")
+
+
+# For Siglip: copied from
+#   HuggingFaceM4/siglip-so400m-14-980-flash-attn2-navit and add tgt_sizes
+# Remove hints as there's little possibility to change these code.
+class SiglipVisionConfig(PretrainedConfig):
+
+    model_type = "siglip_vision_model"
+
+    def __init__(
+        self,
+        hidden_size=768,
+        intermediate_size=3072,
+        num_hidden_layers=12,
+        num_attention_heads=12,
+        num_channels=3,
+        image_size=224,
+        patch_size=16,
+        hidden_act="gelu_pytorch_tanh",
+        layer_norm_eps=1e-6,
+        attention_dropout=0.0,
+        **kwargs,
+    ):
+        super().__init__(**kwargs)
+
+        self.hidden_size = hidden_size
+        self.intermediate_size = intermediate_size
+        self.num_hidden_layers = num_hidden_layers
+        self.num_attention_heads = num_attention_heads
+        self.num_channels = num_channels
+        self.patch_size = patch_size
+        self.image_size = image_size
+        self.attention_dropout = attention_dropout
+        self.layer_norm_eps = layer_norm_eps
+        self.hidden_act = hidden_act
+
+    @classmethod
+    def from_pretrained(cls, pretrained_model_name_or_path: Union[str,
+                                                                  os.PathLike],
+                        **kwargs) -> "PretrainedConfig":
+        cls._set_token_in_kwargs(kwargs)
+
+        config_dict, kwargs = cls.get_config_dict(
+            pretrained_model_name_or_path, **kwargs)
+
+        # get the vision config dict if we are loading from SiglipConfig
+        if config_dict.get("model_type") == "siglip":
+            config_dict = config_dict["vision_config"]
+
+        if "model_type" in config_dict and hasattr(
+                cls,
+                "model_type") and config_dict["model_type"] != cls.model_type:
+            logger.warning(
+                "You are using a model of type %s to "
+                "instantiate a model of type %s. "
+                "This is not supported for all configurations"
+                "of models and can yield errors.", config_dict['model_type'],
+                cls.model_type)
+
+        return cls.from_dict(config_dict, **kwargs)
+
+
+_CHECKPOINT_FOR_DOC = "google/siglip-base-patch16-224"
+
+SIGLIP_PRETRAINED_MODEL_ARCHIVE_LIST = [
+    "google/siglip-base-patch16-224",
+    # See all SigLIP models at https://huggingface.co/models?filter=siglip
+]
+
+if is_flash_attn_2_available():
+    from flash_attn import flash_attn_func, flash_attn_varlen_func
+    from flash_attn.bert_padding import pad_input  # noqa
+    from flash_attn.bert_padding import index_first_axis, unpad_input
+
+
+# Copied from transformers.models.llama.modeling_llama._get_unpad_data
+def _get_unpad_data(attention_mask):
+    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)
+    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()
+    max_seqlen_in_batch = seqlens_in_batch.max().item()
+    cu_seqlens = F.pad(
+        torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))
+    return (
+        indices,
+        cu_seqlens,
+        max_seqlen_in_batch,
+    )
+
+
+def _trunc_normal_(tensor, mean, std, a, b):
+
+    def norm_cdf(x):
+        # Computes standard normal cumulative distribution function
+        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0
+
+    if (mean < a - 2 * std) or (mean > b + 2 * std):
+        warnings.warn(
+            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
+            "The distribution of values may be incorrect.",
+            stacklevel=2,
+        )
+
+    # Values are generated by using a truncated uniform distribution and
+    # then using the inverse CDF for the normal distribution.
+    # Get upper and lower cdf values
+    l_ = norm_cdf((a - mean) / std)
+    u = norm_cdf((b - mean) / std)
+
+    # Uniformly fill tensor with values from [l, u], then translate to
+    # [2l-1, 2u-1].
+    tensor.uniform_(2 * l_ - 1, 2 * u - 1)
+
+    # Use inverse cdf transform for normal distribution to get truncated
+    # standard normal
+    if tensor.dtype in [torch.float16, torch.bfloat16]:
+        # The `erfinv_` op is not (yet?) defined in float16+cpu, bfloat16+gpu
+        og_dtype = tensor.dtype
+        tensor = tensor.to(torch.float32)
+        tensor.erfinv_()
+        tensor = tensor.to(og_dtype)
+    else:
+        tensor.erfinv_()
+
+    # Transform to proper mean, std
+    tensor.mul_(std * math.sqrt(2.0))
+    tensor.add_(mean)
+
+    # Clamp to ensure it's in the proper range
+    if tensor.dtype == torch.float16:
+        # The `clamp_` op is not (yet?) defined in float16+cpu
+        tensor = tensor.to(torch.float32)
+        tensor.clamp_(min=a, max=b)
+        tensor = tensor.to(torch.float16)
+    else:
+        tensor.clamp_(min=a, max=b)
+
+
+def trunc_normal_tf_(tensor: torch.Tensor,
+                     mean: float = 0.0,
+                     std: float = 1.0,
+                     a: float = -2.0,
+                     b: float = 2.0) -> torch.Tensor:
+    with torch.no_grad():
+        _trunc_normal_(tensor, 0, 1.0, a, b)
+        tensor.mul_(std).add_(mean)
+
+
+def variance_scaling_(tensor, scale=1.0, mode="fan_in", distribution="normal"):
+    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)
+    if mode == "fan_in":
+        denom = fan_in
+    elif mode == "fan_out":
+        denom = fan_out
+    elif mode == "fan_avg":
+        denom = (fan_in + fan_out) / 2
+
+    variance = scale / denom
+
+    if distribution == "truncated_normal":
+        # constant is stddev of standard normal truncated to (-2, 2)
+        trunc_normal_tf_(tensor, std=math.sqrt(variance) / 0.87962566103423978)
+    elif distribution == "normal":
+        with torch.no_grad():
+            tensor.normal_(std=math.sqrt(variance))
+    elif distribution == "uniform":
+        bound = math.sqrt(3 * variance)
+        with torch.no_grad():
+            tensor.uniform_(-bound, bound)
+    else:
+        raise ValueError(f"invalid distribution {distribution}")
+
+
+def lecun_normal_(tensor):
+    variance_scaling_(tensor, mode="fan_in", distribution="truncated_normal")
+
+
+def default_flax_embed_init(tensor):
+    variance_scaling_(tensor, mode="fan_in", distribution="normal")
+
+
+class SiglipVisionModelOutput(ModelOutput):
+    image_embeds: Optional[torch.FloatTensor] = None
+    last_hidden_state: torch.FloatTensor = None
+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
+    attentions: Optional[Tuple[torch.FloatTensor]] = None
+
+
+class SiglipVisionEmbeddings(nn.Module):
+
+    def __init__(self, config: SiglipVisionConfig):
+        super().__init__()
+        self.config = config
+        self.embed_dim = config.hidden_size
+        self.image_size = config.image_size
+        self.patch_size = config.patch_size
+
+        self.patch_embedding = nn.Conv2d(
+            in_channels=config.num_channels,
+            out_channels=self.embed_dim,
+            kernel_size=self.patch_size,
+            stride=self.patch_size,
+            padding="valid",
+        )
+
+        self.num_patches_per_side = self.image_size // self.patch_size
+        self.num_patches = self.num_patches_per_side**2
+        self.num_positions = self.num_patches
+        self.position_embedding = nn.Embedding(self.num_positions,
+                                               self.embed_dim)
+
+    def forward(self,
+                pixel_values: torch.FloatTensor,
+                patch_attention_mask: torch.BoolTensor,
+                tgt_sizes: Optional[torch.IntTensor] = None) -> torch.Tensor:
+        batch_size = pixel_values.size(0)
+
+        patch_embeds = self.patch_embedding(pixel_values)
+        embeddings = patch_embeds.flatten(2).transpose(1, 2)
+
+        max_im_h, max_im_w = pixel_values.size(2), pixel_values.size(3)
+        max_nb_patches_h, max_nb_patches_w = (max_im_h // self.patch_size,
+                                              max_im_w // self.patch_size)
+        boundaries = torch.arange(1 / self.num_patches_per_side, 1.0,
+                                  1 / self.num_patches_per_side)
+        position_ids = torch.full(
+            size=(
+                batch_size,
+                max_nb_patches_h * max_nb_patches_w,
+            ),
+            fill_value=0,
+        )
+
+        for batch_idx, p_attn_mask in enumerate(patch_attention_mask):
+            if tgt_sizes is not None:
+                nb_patches_h = tgt_sizes[batch_idx][0]
+                nb_patches_w = tgt_sizes[batch_idx][1]
+            else:
+                nb_patches_h = p_attn_mask[:, 0].sum()
+                nb_patches_w = p_attn_mask[0].sum()
+
+            fractional_coords_h = torch.arange(0, 1 - 1e-6, 1 / nb_patches_h)
+            fractional_coords_w = torch.arange(0, 1 - 1e-6, 1 / nb_patches_w)
+
+            bucket_coords_h = torch.bucketize(fractional_coords_h,
+                                              boundaries,
+                                              right=True)
+            bucket_coords_w = torch.bucketize(fractional_coords_w,
+                                              boundaries,
+                                              right=True)
+
+            pos_ids = (bucket_coords_h[:, None] * self.num_patches_per_side +
+                       bucket_coords_w).flatten()
+            position_ids[batch_idx][p_attn_mask.view(-1).cpu()] = pos_ids
+
+        position_ids = position_ids.to(self.position_embedding.weight.device)
+
+        embeddings = embeddings + self.position_embedding(position_ids)
+        return embeddings
+
+
+def attention_softmax(attn_weights: torch.Tensor, training: bool):
+    if attn_weights.is_contiguous() and attn_weights.device.type == "xpu" and not training:
+        import xe_addons
+        xe_addons.attn_softmax_inplaced(attn_weights)
+    else:
+        attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1,
+                                                   dtype=torch.float32).to(attn_weights.dtype)
+    return attn_weights
+
+
+class SiglipAttention(nn.Module):
+    """Multi-headed attention from 'Attention Is All You Need' paper"""
+
+    # Copied from transformers.models.clip.modeling_clip.CLIPAttention.__init__
+    def __init__(self, config):
+        super().__init__()
+        self.config = config
+        self.embed_dim = config.hidden_size
+        self.num_heads = config.num_attention_heads
+        self.head_dim = self.embed_dim // self.num_heads
+        if self.head_dim * self.num_heads != self.embed_dim:
+            raise ValueError(
+                "embed_dim must be divisible by num_heads (got `embed_dim`: "
+                f"{self.embed_dim} and `num_heads`:"
+                f" {self.num_heads}).")
+        self.scale = self.head_dim**-0.5
+        self.dropout = config.attention_dropout
+
+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)
+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)
+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)
+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        attention_mask: Optional[torch.Tensor] = None,
+        output_attentions: Optional[bool] = False,
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor],
+               Optional[Tuple[torch.Tensor]]]:
+        """Input shape: Batch x Time x Channel"""
+
+        batch_size, q_len, _ = hidden_states.size()
+
+        # query_states = self.q_proj(hidden_states)
+        # key_states = self.k_proj(hidden_states)
+        # value_states = self.v_proj(hidden_states)
+
+        # query_states = query_states.view(batch_size, q_len, self.num_heads,
+        #                                  self.head_dim).transpose(1, 2)
+        # key_states = key_states.view(batch_size, q_len, self.num_heads,
+        #                              self.head_dim).transpose(1, 2)
+        # value_states = value_states.view(batch_size, q_len, self.num_heads,
+        #                                  self.head_dim).transpose(1, 2)
+
+        qkv = self.qkv_proj(hidden_states)
+        qkv = qkv.view(batch_size, q_len, self.num_heads * 3, self.head_dim)
+        qkv = qkv.transpose(1, 2)
+        query_states, key_states, value_states = qkv.chunk(3, dim=1)
+
+        from ipex_llm.transformers.models.common import padding_qkv_hd
+        query_states, key_states, value_states = padding_qkv_hd(
+            query_states, key_states, value_states,
+            72, 80
+        )
+        from ipex_llm.transformers.models.utils import use_sdp_non_causal
+        if use_sdp_non_causal(self.head_dim, query_states.device, query_states.dtype):
+            import xe_addons
+            attn_weights = None
+            attn_output = xe_addons.sdp_non_causal(query_states, key_states.contiguous(), value_states.contiguous(), attention_mask)
+        else:
+            k_v_seq_len = key_states.shape[-2]
+            attn_weights = torch.matmul(query_states, key_states.transpose(
+                2, 3)) * self.scale
+
+            if attn_weights.size() != (batch_size, self.num_heads, q_len,
+                                    k_v_seq_len):
+                raise ValueError(
+                    "Attention weights should be of size "
+                    f"{(batch_size, self.num_heads, q_len, k_v_seq_len)}, but is"
+                    f" {attn_weights.size()}")
+
+            if attention_mask is not None:
+                if attention_mask.size() != (batch_size, 1, q_len, k_v_seq_len):
+                    raise ValueError(
+                        "Attention mask should be of size "
+                        f"{(batch_size, 1, q_len, k_v_seq_len)}",
+                        f"but is {attention_mask.size()}")
+                attn_weights = attn_weights + attention_mask
+
+            # upcast attention to fp32
+            # attn_weights = nn.functional.softmax(attn_weights,
+            #                                      dim=-1,
+            #                                      dtype=torch.float32).to(
+            #                                          query_states.dtype)
+            attn_weights = attention_softmax(attn_weights, self.training)
+            attn_weights = nn.functional.dropout(attn_weights,
+                                                p=self.dropout,
+                                                training=self.training)
+            attn_output = torch.matmul(attn_weights, value_states)
+
+            if attn_output.size() != (batch_size, self.num_heads, q_len,
+                                    self.head_dim):
+                raise ValueError(
+                    "`attn_output` should be of size "
+                    f"{(batch_size, self.num_heads, q_len, self.head_dim)}, "
+                    "but is"
+                    f" {attn_output.size()}")
+        attn_output = attn_output[:, :, :, :self.head_dim]
+        attn_output = attn_output.transpose(1, 2).contiguous()
+        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim)
+
+        attn_output = self.out_proj(attn_output)
+
+        return attn_output, attn_weights
+
+
+class SiglipFlashAttention2(SiglipAttention):
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.is_causal = False  # Hack to make sure we don't use a causal mask
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        attention_mask: Optional[torch.LongTensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        past_key_value: Optional[Tuple[torch.Tensor]] = None,
+        output_attentions: bool = False,
+        use_cache: bool = False,
+        **kwargs,
+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor],
+               Optional[Tuple[torch.Tensor]]]:
+        output_attentions = False
+
+        bsz, q_len, _ = hidden_states.size()
+
+        query_states = self.q_proj(hidden_states)
+        key_states = self.k_proj(hidden_states)
+        value_states = self.v_proj(hidden_states)
+
+        query_states = query_states.view(bsz, q_len, self.num_heads,
+                                         self.head_dim).transpose(1, 2)
+        key_states = key_states.view(bsz, q_len, self.num_heads,
+                                     self.head_dim).transpose(1, 2)
+        value_states = value_states.view(bsz, q_len, self.num_heads,
+                                         self.head_dim).transpose(1, 2)
+
+        kv_seq_len = key_states.shape[-2]
+        if past_key_value is not None:
+            kv_seq_len += past_key_value.get_usable_length(
+                kv_seq_len, self.layer_idx)
+
+        query_states = query_states.transpose(1, 2)
+        key_states = key_states.transpose(1, 2)
+        value_states = value_states.transpose(1, 2)
+
+        dropout_rate = self.dropout if self.training else 0.0
+
+        input_dtype = query_states.dtype
+        if input_dtype == torch.float32:
+            if torch.is_autocast_enabled():
+                target_dtype = torch.get_autocast_gpu_dtype()
+            # Handle the case where the model is quantized
+            elif hasattr(self.config, "_pre_quantization_dtype"):
+                target_dtype = self.config._pre_quantization_dtype
+            else:
+                target_dtype = self.q_proj.weight.dtype
+
+            logger.warning(
+                "The input hidden states seems to be "
+                "silently casted in float32, "
+                "this might be related to the fact "
+                "you have upcasted embedding or layer norm layers in float32. "
+                "We will cast back the input in"
+                " %s.", target_dtype)
+
+            query_states = query_states.to(target_dtype)
+            key_states = key_states.to(target_dtype)
+            value_states = value_states.to(target_dtype)
+
+        attn_output = self._flash_attention_forward(query_states,
+                                                    key_states,
+                                                    value_states,
+                                                    attention_mask,
+                                                    q_len,
+                                                    dropout=dropout_rate)
+
+        attn_output = attn_output.reshape(bsz, q_len,
+                                          self.embed_dim).contiguous()
+        attn_output = self.out_proj(attn_output)
+
+        if not output_attentions:
+            attn_weights = None
+
+        return attn_output, attn_weights
+
+    def _flash_attention_forward(self,
+                                 query_states,
+                                 key_states,
+                                 value_states,
+                                 attention_mask,
+                                 query_length,
+                                 dropout=0.0,
+                                 softmax_scale=None):
+        causal = self.is_causal and query_length != 1
+
+        # Contains at least one padding token in the sequence
+        if attention_mask is not None:
+            batch_size = query_states.shape[0]
+            (query_states, key_states, value_states, indices_q, cu_seq_lens,
+             max_seq_lens) = self._upad_input(query_states, key_states,
+                                              value_states, attention_mask,
+                                              query_length)
+
+            cu_seqlens_q, cu_seqlens_k = cu_seq_lens
+            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens
+
+            attn_output_unpad = flash_attn_varlen_func(
+                query_states,
+                key_states,
+                value_states,
+                cu_seqlens_q=cu_seqlens_q,
+                cu_seqlens_k=cu_seqlens_k,
+                max_seqlen_q=max_seqlen_in_batch_q,
+                max_seqlen_k=max_seqlen_in_batch_k,
+                dropout_p=dropout,
+                softmax_scale=softmax_scale,
+                causal=causal,
+            )
+
+            attn_output = pad_input(attn_output_unpad, indices_q, batch_size,
+                                    query_length)
+        else:
+            attn_output = flash_attn_func(query_states,
+                                          key_states,
+                                          value_states,
+                                          dropout,
+                                          softmax_scale=softmax_scale,
+                                          causal=causal)
+
+        return attn_output
+
+    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask,
+                    query_length):
+        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(
+            attention_mask)
+        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape
+
+        key_layer = index_first_axis(
+            key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads,
+                              head_dim), indices_k)
+        value_layer = index_first_axis(
+            value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads,
+                                head_dim), indices_k)
+        if query_length == kv_seq_len:
+            query_layer = index_first_axis(
+                query_layer.reshape(batch_size * kv_seq_len, self.num_heads,
+                                    head_dim), indices_k)
+            cu_seqlens_q = cu_seqlens_k
+            max_seqlen_in_batch_q = max_seqlen_in_batch_k
+            indices_q = indices_k
+        elif query_length == 1:
+            max_seqlen_in_batch_q = 1
+            cu_seqlens_q = torch.arange(
+                batch_size + 1, dtype=torch.int32, device=query_layer.device
+            )  # There is a memcpy here, that is very bad.
+            indices_q = cu_seqlens_q[:-1]
+            query_layer = query_layer.squeeze(1)
+        else:
+            # The -q_len: slice assumes left padding.
+            attention_mask = attention_mask[:, -query_length:]
+            (query_layer, indices_q, cu_seqlens_q,
+             max_seqlen_in_batch_q) = unpad_input(query_layer, attention_mask)
+
+        return (
+            query_layer,
+            key_layer,
+            value_layer,
+            indices_q,
+            (cu_seqlens_q, cu_seqlens_k),
+            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),
+        )
+
+
+# Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->Siglip
+class SiglipMLP(nn.Module):
+
+    def __init__(self, config):
+        super().__init__()
+        self.config = config
+        self.activation_fn = ACT2FN[config.hidden_act]
+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)
+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        hidden_states = self.fc1(hidden_states)
+        hidden_states = self.activation_fn(hidden_states)
+        hidden_states = self.fc2(hidden_states)
+        return hidden_states
+
+
+# Copied from transformers.models.clip.modeling_clip.CLIPEncoderLayer
+# with CLIP->Siglip
+class SiglipEncoderLayer(nn.Module):
+
+    def __init__(self, config: SiglipVisionConfig):
+        super().__init__()
+        self.embed_dim = config.hidden_size
+        self._use_flash_attention_2 = (
+            config._attn_implementation == "flash_attention_2")
+        self.self_attn = (SiglipAttention(config)
+                          if not self._use_flash_attention_2 else
+                          SiglipFlashAttention2(config))
+        self.layer_norm1 = nn.LayerNorm(self.embed_dim,
+                                        eps=config.layer_norm_eps)
+        self.mlp = SiglipMLP(config)
+        self.layer_norm2 = nn.LayerNorm(self.embed_dim,
+                                        eps=config.layer_norm_eps)
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        attention_mask: torch.Tensor,
+        output_attentions: Optional[bool] = False,
+    ) -> Tuple[torch.FloatTensor]:
+        residual = hidden_states
+
+        hidden_states = self.layer_norm1(hidden_states)
+        hidden_states, attn_weights = self.self_attn(
+            hidden_states=hidden_states,
+            attention_mask=attention_mask,
+            output_attentions=output_attentions,
+        )
+        hidden_states = residual + hidden_states
+
+        residual = hidden_states
+        hidden_states = self.layer_norm2(hidden_states)
+        hidden_states = self.mlp(hidden_states)
+        hidden_states = residual + hidden_states
+
+        outputs = (hidden_states, )
+
+        if output_attentions:
+            outputs += (attn_weights, )
+
+        return outputs
+
+
+class SiglipPreTrainedModel(PreTrainedModel):
+    config_class = SiglipVisionConfig
+    base_model_prefix = "siglip"
+    supports_gradient_checkpointing = True
+
+    def _init_weights(self, module):
+        """Initialize the weights"""
+
+        if isinstance(module, SiglipVisionEmbeddings):
+            width = self.config.hidden_size
+            nn.init.normal_(module.position_embedding.weight,
+                            std=1 / np.sqrt(width))
+        elif isinstance(module, nn.Embedding):
+            default_flax_embed_init(module.weight)
+        elif isinstance(module, SiglipAttention):
+            nn.init.normal_(module.q_proj.weight)
+            nn.init.normal_(module.k_proj.weight)
+            nn.init.normal_(module.v_proj.weight)
+            nn.init.normal_(module.out_proj.weight)
+            nn.init.zeros_(module.q_proj.bias)
+            nn.init.zeros_(module.k_proj.bias)
+            nn.init.zeros_(module.v_proj.bias)
+            nn.init.zeros_(module.out_proj.bias)
+        elif isinstance(module, SiglipMLP):
+            nn.init.normal_(module.fc1.weight)
+            nn.init.normal_(module.fc2.weight)
+            nn.init.normal_(module.fc1.bias, std=1e-6)
+            nn.init.normal_(module.fc2.bias, std=1e-6)
+        elif isinstance(module, (nn.Linear, nn.Conv2d)):
+            lecun_normal_(module.weight)
+            if module.bias is not None:
+                nn.init.zeros_(module.bias)
+        elif isinstance(module, nn.LayerNorm):
+            module.bias.data.zero_()
+            module.weight.data.fill_(1.0)
+
+
+# Copied from transformers.models.clip.modeling_clip.CLIPEncoder
+# with CLIP->Siglip
+class SiglipEncoder(nn.Module):
+
+    def __init__(self, config: SiglipVisionConfig):
+        super().__init__()
+        self.config = config
+        self.layers = nn.ModuleList([
+            SiglipEncoderLayer(config) for _ in range(config.num_hidden_layers)
+        ])
+        self.gradient_checkpointing = False
+
+    # Ignore copy
+    def forward(
+        self,
+        inputs_embeds,
+        attention_mask: Optional[torch.Tensor] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, BaseModelOutput]:
+        output_attentions = output_attentions if output_attentions is not None \
+                                else self.config.output_attentions
+        output_hidden_states = (output_hidden_states
+                                if output_hidden_states is not None else
+                                self.config.output_hidden_states)
+        return_dict = return_dict if return_dict is not None \
+                        else self.config.use_return_dict
+
+        encoder_states = () if output_hidden_states else None
+        all_attentions = () if output_attentions else None
+
+        hidden_states = inputs_embeds
+        for encoder_layer in self.layers:
+            if output_hidden_states:
+                encoder_states = encoder_states + (hidden_states, )
+            if self.gradient_checkpointing and self.training:
+                layer_outputs = self._gradient_checkpointing_func(
+                    encoder_layer.__call__,
+                    hidden_states,
+                    attention_mask,
+                    output_attentions,
+                )
+            else:
+                layer_outputs = encoder_layer(
+                    hidden_states,
+                    attention_mask,
+                    output_attentions=output_attentions,
+                )
+
+            hidden_states = layer_outputs[0]
+
+            if output_attentions:
+                all_attentions = all_attentions + (layer_outputs[1], )
+
+        if output_hidden_states:
+            encoder_states = encoder_states + (hidden_states, )
+
+        if not return_dict:
+            return tuple(
+                v for v in [hidden_states, encoder_states, all_attentions]
+                if v is not None)
+        return BaseModelOutput(last_hidden_state=hidden_states,
+                               hidden_states=encoder_states,
+                               attentions=all_attentions)
+
+
+class SiglipVisionTransformer(SiglipPreTrainedModel):
+    config_class = SiglipVisionConfig
+    main_input_name = "pixel_values"
+    _supports_flash_attn_2 = True
+
+    def __init__(self, config: SiglipVisionConfig):
+        super().__init__(config)
+        self.config = config
+        embed_dim = config.hidden_size
+
+        self.embeddings = SiglipVisionEmbeddings(config)
+        self.encoder = SiglipEncoder(config)
+        self.post_layernorm = nn.LayerNorm(embed_dim,
+                                           eps=config.layer_norm_eps)
+        self._use_flash_attention_2 = (
+            config._attn_implementation == "flash_attention_2")
+
+        # Initialize weights and apply final processing
+        self.post_init()
+
+    def get_input_embeddings(self) -> nn.Module:
+        return self.embeddings.patch_embedding
+
+    @replace_return_docstrings(output_type=BaseModelOutputWithPooling,
+                               config_class=SiglipVisionConfig)
+    def forward(
+        self,
+        pixel_values,
+        patch_attention_mask: Optional[torch.BoolTensor] = None,
+        tgt_sizes: Optional[torch.IntTensor] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, BaseModelOutputWithPooling]:
+        r"""
+        Returns:
+        """
+        output_attentions = output_attentions if output_attentions is not None \
+                                else self.config.output_attentions
+        output_hidden_states = (output_hidden_states
+                                if output_hidden_states is not None else
+                                self.config.output_hidden_states)
+        return_dict = return_dict if return_dict is not None \
+                        else self.config.use_return_dict
+
+        batch_size = pixel_values.size(0)
+        if patch_attention_mask is None:
+            patch_attention_mask = torch.ones(
+                size=(
+                    batch_size,
+                    pixel_values.size(2) // self.config.patch_size,
+                    pixel_values.size(3) // self.config.patch_size,
+                ),
+                dtype=torch.bool,
+                device=pixel_values.device,
+            )
+
+        hidden_states = self.embeddings(
+            pixel_values=pixel_values,
+            patch_attention_mask=patch_attention_mask,
+            tgt_sizes=tgt_sizes)
+
+        patch_attention_mask = patch_attention_mask.view(batch_size, -1)
+        # The call to `_upad_input` in `_flash_attention_forward` is expensive
+        # So when the `patch_attention_mask` is full of 1s
+        # (i.e. attending to the whole sequence),
+        # avoiding passing the attention_mask,
+        # which is equivalent to attending to the full sequence
+        if not torch.any(~patch_attention_mask):
+            attention_mask = None
+        else:
+            attention_mask = (_prepare_4d_attention_mask(
+                patch_attention_mask, hidden_states.dtype)
+                              if not self._use_flash_attention_2 else
+                              patch_attention_mask)
+
+        encoder_outputs = self.encoder(
+            inputs_embeds=hidden_states,
+            attention_mask=attention_mask,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
+
+        last_hidden_state = encoder_outputs[0]
+        last_hidden_state = self.post_layernorm(last_hidden_state)
+
+        if not return_dict:
+            return (last_hidden_state, None) + encoder_outputs[1:]
+
+        return BaseModelOutputWithPooling(
+            last_hidden_state=last_hidden_state,
+            pooler_output=None,
+            hidden_states=encoder_outputs.hidden_states,
+            attentions=encoder_outputs.attentions,
+        )
diff --git a/vllm/model_executor/models/qwen2.py b/vllm/model_executor/models/qwen2.py
index c4d02e5dd..2831a5a12 100644
--- a/vllm/model_executor/models/qwen2.py
+++ b/vllm/model_executor/models/qwen2.py
@@ -263,7 +263,11 @@ class Qwen2DecoderLayer(nn.Module):
     })
 class Qwen2Model(nn.Module):
 
-    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+    def __init__(self,
+                 *,
+                 vllm_config: VllmConfig,
+                 prefix: str = "",
+                 decoder_layer_type: type[nn.Module] = Qwen2DecoderLayer):
         super().__init__()
 
         config = vllm_config.model_config.hf_config
@@ -297,12 +301,14 @@ class Qwen2Model(nn.Module):
         else:
             self.embed_tokens = PPMissingLayer()
 
+        # Use the provided decoder layer type or default to Qwen2DecoderLayer
+        decoder_layer_type = decoder_layer_type or Qwen2DecoderLayer
         self.start_layer, self.end_layer, self.layers = make_layers(
             config.num_hidden_layers,
-            lambda prefix: Qwen2DecoderLayer(config=config,
-                                             cache_config=cache_config,
-                                             quant_config=quant_config,
-                                             prefix=prefix),
+            lambda prefix: decoder_layer_type(config=config,
+                                              cache_config=cache_config,
+                                              quant_config=quant_config,
+                                              prefix=prefix),
             prefix=f"{prefix}.layers",
         )
 
diff --git a/vllm/model_executor/models/qwen2_5_vl.py b/vllm/model_executor/models/qwen2_5_vl.py
index 1e6ff1fec..e2480326a 100644
--- a/vllm/model_executor/models/qwen2_5_vl.py
+++ b/vllm/model_executor/models/qwen2_5_vl.py
@@ -304,6 +304,10 @@ class Qwen2_5_VisionAttention(nn.Module):
         elif self.attn_backend == _Backend.TORCH_SDPA:
             # Execute attention entry by entry for speed & less VRAM.
             outputs = []
+            head_dim = q.shape[-1]
+            import math
+            import xe_addons
+            scale = 1 / math.sqrt(head_dim)
             for i in range(1, len(cu_seqlens)):
                 start_idx = cu_seqlens[i - 1]
                 end_idx = cu_seqlens[i]
@@ -312,10 +316,16 @@ class Qwen2_5_VisionAttention(nn.Module):
                 v_i = v[:, start_idx:end_idx]
                 q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
                                  for x in [q_i, k_i, v_i])
-                output_i = F.scaled_dot_product_attention(q_i,
-                                                          k_i,
-                                                          v_i,
-                                                          dropout_p=0.0)
+                # output_i = F.scaled_dot_product_attention(q_i,
+                #                                           k_i,
+                #                                           v_i,
+                #                                           dropout_p=0.0)
+                output_i = xe_addons.sdp_non_causal(
+                    q_i.contiguous(),
+                    k_i.contiguous(),
+                    v_i.contiguous(),
+                    None,
+                    scale)
                 output_i = rearrange(output_i, "b h s d -> b s h d ")
                 outputs.append(output_i)
             context_layer = torch.cat(outputs, dim=1)
diff --git a/vllm/model_executor/models/qwen2_vl.py b/vllm/model_executor/models/qwen2_vl.py
index a7800d415..26af87512 100644
--- a/vllm/model_executor/models/qwen2_vl.py
+++ b/vllm/model_executor/models/qwen2_vl.py
@@ -273,12 +273,37 @@ class Qwen2VisionAttention(nn.Module):
                                       prefix=f"{prefix}.proj")
 
         # Detect attention implementation.
-        self.attn_backend: _Backend = get_vit_attn_backend(support_fa=True)
-        if self.attn_backend not in {
-                _Backend.FLASH_ATTN, _Backend.TORCH_SDPA, _Backend.XFORMERS
-        }:
-            raise RuntimeError(
-                f"Qwen2-VL does not support {self.attn_backend} backend now.")
+        # selected_backend: Optional[_Backend] = get_global_forced_attn_backend()
+        # if selected_backend is None:
+        #     backend_by_env_var: Optional[str] = envs.VLLM_ATTENTION_BACKEND
+        #     if backend_by_env_var is not None:
+        #         selected_backend = backend_name_to_enum(backend_by_env_var)
+        # if selected_backend is None:
+        #     # For Volta and Turing GPUs, use xformers instead.
+        #     device_available = current_platform.get_device_capability()[0] >= 8
+        #     if device_available:
+        #         from transformers.utils import is_flash_attn_2_available
+
+        #         if is_flash_attn_2_available():
+        #             self._use_flash_attn = True
+        #         else:
+        #             logger.warning(
+        #                 "Current Qwen2-VL implementation has a bug with "
+        #                 "`vllm-flash-attn` inside vision module, so we use "
+        #                 "xformers backend instead. You can run `pip install "
+        #                 "flash-attn to use flash-attention backend.")
+        #             self._use_flash_attn = False
+        #     else:
+        #         self._use_flash_attn = False
+        # else:
+        #     if selected_backend == _Backend.FLASH_ATTN:
+        #         self._use_flash_attn = True
+        #     elif selected_backend == _Backend.XFORMERS:
+        #         self._use_flash_attn = False
+        #     else:
+        #         raise RuntimeError(
+        #             f"Qwen2-VL does not support {selected_backend} backend now."
+        #         )
 
     def split_qkv(self, qkv: torch.Tensor) -> tuple[torch.Tensor, ...]:
         # [s, b, 3 * head * head_dim]
@@ -311,8 +336,9 @@ class Qwen2VisionAttention(nn.Module):
             max_seqlen: Optional[int] = None,  # Only used for Flash Attention
             seqlens: Optional[list[int]] = None,  # Only used for xFormers
     ) -> torch.Tensor:
-
-        # [s, b, c] --> [s, b, 3 * head * head_dim]
+        # TODO(xiangyu): Check logic here. It differs from current v0.8.1 version.
+        seq_length = x.shape[0]
+        # [s, b, c] --> [s, b, head * 3 * head_dim]
         x, _ = self.qkv(x)
 
         # [s, b, 3 * head * head_dim] -> 3 * [s, b, head, head_dim]
@@ -324,59 +350,61 @@ class Qwen2VisionAttention(nn.Module):
         if rotary_pos_emb is not None:
             q = apply_rotary_pos_emb_vision(q, rotary_pos_emb)
             k = apply_rotary_pos_emb_vision(k, rotary_pos_emb)
-
-        if self.attn_backend == _Backend.FLASH_ATTN:
-            # from vllm_flash_attn.flash_attn_interface import (
-            #   flash_attn_varlen_func)
-            from flash_attn import flash_attn_varlen_func
-
-            q, k, v = (rearrange(x, "b s ... -> (b s) ...") for x in [q, k, v])
-
-            output = flash_attn_varlen_func(q,
-                                            k,
-                                            v,
-                                            cu_seqlens_q=cu_seqlens,
-                                            cu_seqlens_k=cu_seqlens,
-                                            max_seqlen_q=max_seqlen,
-                                            max_seqlen_k=max_seqlen,
-                                            dropout_p=0,
-                                            causal=False)
-
-            context_layer = rearrange(output,
-                                      "(b s) ... -> b s ...",
-                                      b=batch_size)
-        elif self.attn_backend == _Backend.TORCH_SDPA:
-            # Execute attention entry by entry for speed & less VRAM.
-            outputs = []
-            for i in range(1, len(cu_seqlens)):
-                start_idx = cu_seqlens[i - 1]
-                end_idx = cu_seqlens[i]
-                q_i = q[:, start_idx:end_idx]
-                k_i = k[:, start_idx:end_idx]
-                v_i = v[:, start_idx:end_idx]
-                q_i, k_i, v_i = (rearrange(x, "b s h d -> b h s d")
-                                 for x in [q_i, k_i, v_i])
-                output_i = F.scaled_dot_product_attention(q_i,
-                                                          k_i,
-                                                          v_i,
-                                                          dropout_p=0.0)
-                output_i = rearrange(output_i, "b h s d -> b s h d ")
-                outputs.append(output_i)
-            context_layer = torch.cat(outputs, dim=1)
-        elif self.attn_backend == _Backend.XFORMERS:
-            from xformers import ops as xops
-            from xformers.ops.fmha.attn_bias import BlockDiagonalMask
-
-            attn_bias = BlockDiagonalMask.from_seqlens(q_seqlen=seqlens,
-                                                       kv_seqlen=None,
-                                                       device=q.device)
-
-            context_layer = xops.memory_efficient_attention_forward(
-                q, k, v, attn_bias=attn_bias, p=0, scale=None)
-        context_layer = rearrange(context_layer,
-                                  "b s h d -> s b (h d)").contiguous()
-
-        output, _ = self.proj(context_layer)
+        query = q.movedim(1, 2)
+        key = k.movedim(1, 2)
+        value = v.movedim(1, 2)
+        head_dim = query.shape[-1]
+        # if len(cu_seqlens) == 2 and cu_seqlens.tolist() == [0, seq_length]:
+        #     attention_mask = None
+        # else:
+        #     attention_mask = torch.full(
+        #         [1, seq_length, seq_length], torch.finfo(q.dtype).min, device=q.device, dtype=q.dtype
+        #     )
+        #     for i in range(1, len(cu_seqlens)):
+        #         attention_mask[..., cu_seqlens[i - 1]:cu_seqlens[i],
+        #                     cu_seqlens[i - 1]:cu_seqlens[i]] = 0
+        #from ipex_llm.transformers.models.common import attention_softmax
+        from ipex_llm.transformers.models.utils import use_sdp_non_causal
+        import math
+        seq_lens = []
+        for i in range(1, len(cu_seqlens)):
+            seq_lens.append(cu_seqlens[i]-cu_seqlens[i-1]) 
+        att_masks = [None] * len(seq_lens)
+
+        num_tokens = q.shape[0] * q.shape[1]
+        attn_output = torch.empty(
+                (num_tokens, self.num_attention_heads_per_partition, self.hidden_size_per_attention_head),
+                dtype=query.dtype, device=query.device)
+        start = 0
+        for seq_len, mask in zip(seq_lens,
+                                att_masks):
+            end = start + seq_len
+            if use_sdp_non_causal(head_dim, q.device, q.dtype):
+                import xe_addons
+                scale = 1 / math.sqrt(head_dim)
+                if mask is not None:
+                    mask = mask.unsqueeze(0)
+                sub_out = xe_addons.sdp_non_causal(
+                    query[:, :, start:end, :].contiguous(),
+                    key[:, :, start:end, :].contiguous(),
+                    value[:, :, start:end, :].contiguous(),
+                    mask,
+                    scale).squeeze(0).movedim(0, 1)
+            else:
+                sub_out = torch.nn.functional.scaled_dot_product_attention(
+                    query[:, :, start:end, :],
+                    key[:, :, start:end, :],
+                    value[:, :, start:end, :],
+                    attn_mask=mask,
+                    dropout_p=0.0,
+                    is_causal=False,
+                    scale= self.hidden_size_per_attention_head**-0.5).squeeze(0).movedim(
+                        0, 1)
+            attn_output[start:end, :, :] = sub_out
+            start = end
+        output = attn_output.reshape(-1, batch_size, self.hidden_size_per_attention_head * self.num_attention_heads_per_partition)
+
+        output, _ = self.proj(output)
         return output
 
 
@@ -633,9 +661,7 @@ class Qwen2VisionTransformer(nn.Module):
         grid_thw: torch.Tensor,
     ) -> torch.Tensor:
         # patchify
-        x = x.to(device=self.device, dtype=self.dtype)
         x = self.patch_embed(x)
-
         # compute position embedding
         rotary_pos_emb = self.rot_pos_emb(grid_thw)
 
@@ -1231,7 +1257,8 @@ class Qwen2VLForConditionalGeneration(nn.Module, SupportsMultiModal,
         if image_input["type"] == "image_embeds":
             image_embeds = image_input["image_embeds"].type(self.visual.dtype)
         else:
-            pixel_values = image_input["pixel_values"].type(self.visual.dtype)
+            # pixel_values = image_input["pixel_values"].type(self.visual.dtype)
+            pixel_values = image_input["pixel_values"].to(torch.float16)
             image_embeds = self.visual(pixel_values, grid_thw=grid_thw)
 
         # Split concatenated embeddings for each image item.
diff --git a/vllm/model_executor/models/qwen3.py b/vllm/model_executor/models/qwen3.py
new file mode 100644
index 000000000..338fffed9
--- /dev/null
+++ b/vllm/model_executor/models/qwen3.py
@@ -0,0 +1,329 @@
+# SPDX-License-Identifier: Apache-2.0
+
+# Copyright 2024 The Qwen team.
+# Copyright 2023 The vLLM team.
+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only Qwen3 model compatible with HuggingFace weights."""
+from typing import Iterable, Optional, Set, Tuple, Union
+
+import torch
+from torch import nn
+from transformers import Qwen3Config
+
+from vllm.attention import Attention, AttentionType
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.distributed import get_pp_group, get_tensor_model_parallel_world_size
+from vllm.logger import init_logger
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (QKVParallelLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import ParallelLMHead
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+
+from .interfaces import SupportsLoRA, SupportsPP
+from .qwen2 import Qwen2MLP as Qwen3MLP
+from .qwen2 import Qwen2Model
+from .utils import AutoWeightsLoader, PPMissingLayer, maybe_prefix
+
+logger = init_logger(__name__)
+
+
+class Qwen3Attention(nn.Module):
+
+    def __init__(self,
+                 hidden_size: int,
+                 num_heads: int,
+                 num_kv_heads: int,
+                 max_position: int = 4096 * 32,
+                 head_dim: Optional[int] = None,
+                 rms_norm_eps: float = 1e-06,
+                 qkv_bias: bool = False,
+                 rope_theta: float = 10000,
+                 cache_config: Optional[CacheConfig] = None,
+                 quant_config: Optional[QuantizationConfig] = None,
+                 rope_scaling: Optional[Tuple] = None,
+                 prefix: str = "",
+                 attn_type: str = AttentionType.DECODER) -> None:
+        super().__init__()
+        self.hidden_size = hidden_size
+        tp_size = get_tensor_model_parallel_world_size()
+        self.total_num_heads = num_heads
+        assert self.total_num_heads % tp_size == 0
+        self.num_heads = self.total_num_heads // tp_size
+        self.total_num_kv_heads = num_kv_heads
+        if self.total_num_kv_heads >= tp_size:
+            # Number of KV heads is greater than TP size, so we partition
+            # the KV heads across multiple tensor parallel GPUs.
+            assert self.total_num_kv_heads % tp_size == 0
+        else:
+            # Number of KV heads is less than TP size, so we replicate
+            # the KV heads across multiple tensor parallel GPUs.
+            assert tp_size % self.total_num_kv_heads == 0
+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
+        self.head_dim = head_dim or hidden_size // self.total_num_heads
+        self.q_size = self.num_heads * self.head_dim
+        self.kv_size = self.num_kv_heads * self.head_dim
+        self.scaling = self.head_dim**-0.5
+        self.rope_theta = rope_theta
+
+        self.qkv_proj = QKVParallelLinear(
+            hidden_size,
+            self.head_dim,
+            self.total_num_heads,
+            self.total_num_kv_heads,
+            bias=qkv_bias,
+            quant_config=quant_config,
+            prefix=f"{prefix}.qkv_proj",
+        )
+        self.o_proj = RowParallelLinear(
+            self.total_num_heads * self.head_dim,
+            hidden_size,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.o_proj",
+        )
+
+        self.rotary_emb = get_rope(
+            self.head_dim,
+            rotary_dim=self.head_dim,
+            max_position=max_position,
+            base=self.rope_theta,
+            rope_scaling=rope_scaling,
+        )
+        self.attn = Attention(self.num_heads,
+                              self.head_dim,
+                              self.scaling,
+                              num_kv_heads=self.num_kv_heads,
+                              cache_config=cache_config,
+                              quant_config=quant_config,
+                              prefix=f"{prefix}.attn",
+                              attn_type=attn_type)
+        self.q_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
+        self.k_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+    ) -> torch.Tensor:
+        qkv, _ = self.qkv_proj(hidden_states)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        # Add qk-norm
+        q_by_head = q.view(*q.shape[:-1], q.shape[-1] // self.head_dim,
+                           self.head_dim)
+        q_by_head = self.q_norm.forward_xpu(q_by_head.contiguous())
+        q = q_by_head.view(q.shape)
+        k_by_head = k.view(*k.shape[:-1], k.shape[-1] // self.head_dim,
+                           self.head_dim)
+        k_by_head = self.k_norm.forward_xpu(k_by_head.contiguous())
+        k = k_by_head.view(k.shape)
+        q, k = self.rotary_emb(positions, q, k)
+        attn_output = self.attn(q, k, v)
+        output, _ = self.o_proj(attn_output)
+        return output
+
+
+class Qwen3DecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: Qwen3Config,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        # Requires transformers > 4.32.0
+        rope_theta = getattr(config, "rope_theta", 1000000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+
+        # By default, Qwen3 uses causal attention as it is a decoder-only model.
+        # You can override the HF config with `is_causal=False` to enable
+        # bidirectional attention, which is used in some embedding models
+        # (e.g. Alibaba-NLP/gte-Qwen3-7B-instruct)
+        if getattr(config, "is_causal", True):
+            attn_type = AttentionType.DECODER
+        else:
+            attn_type = AttentionType.ENCODER_ONLY
+
+        self.self_attn = Qwen3Attention(
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            max_position=config.max_position_embeddings,
+            num_kv_heads=config.num_key_value_heads,
+            rope_theta=rope_theta,
+            rms_norm_eps=config.rms_norm_eps,
+            qkv_bias=getattr(config, 'attention_bias', False),
+            head_dim=getattr(config, 'head_dim', None),
+            cache_config=cache_config,
+            quant_config=quant_config,
+            rope_scaling=rope_scaling,
+            prefix=f"{prefix}.self_attn",
+            attn_type=attn_type,
+        )
+        self.mlp = Qwen3MLP(
+            hidden_size=self.hidden_size,
+            intermediate_size=config.intermediate_size,
+            hidden_act=config.hidden_act,
+            quant_config=quant_config,
+            prefix=f"{prefix}.mlp",
+        )
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor],
+    ) -> Tuple[torch.Tensor, torch.Tensor]:
+        # Self Attention
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+        hidden_states = self.self_attn(
+            positions=positions,
+            hidden_states=hidden_states,
+        )
+
+        # Fully Connected
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual)
+        hidden_states = self.mlp(hidden_states)
+        return hidden_states, residual
+
+
+ALL_DECODER_LAYER_TYPES = {
+    "attention": Qwen3DecoderLayer,
+}
+
+
+@support_torch_compile(
+    dynamic_arg_dims={
+        "input_ids": 0,
+        # positions is of shape (3, seq_len) if mrope is enabled for qwen2-vl,
+        # otherwise (seq_len, ).
+        "positions": -1,
+        "intermediate_tensors": 0,
+        "inputs_embeds": 0,
+    })
+class Qwen3Model(Qwen2Model):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__(vllm_config=vllm_config,
+                         prefix=prefix,
+                         decoder_layer_type=Qwen3DecoderLayer)
+
+
+class Qwen3ForCausalLM(nn.Module, SupportsLoRA, SupportsPP):
+    packed_modules_mapping = {
+        "qkv_proj": [
+            "q_proj",
+            "k_proj",
+            "v_proj",
+        ],
+        "gate_up_proj": [
+            "gate_proj",
+            "up_proj",
+        ],
+    }
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        lora_config = vllm_config.lora_config
+
+        self.config = config
+        self.lora_config = lora_config
+
+        self.quant_config = quant_config
+        self.model = Qwen3Model(vllm_config=vllm_config,
+                                prefix=maybe_prefix(prefix, "model"))
+
+        if get_pp_group().is_last_rank:
+            if config.tie_word_embeddings:
+                self.lm_head = self.model.embed_tokens
+            else:
+                self.lm_head = ParallelLMHead(config.vocab_size,
+                                              config.hidden_size,
+                                              quant_config=quant_config,
+                                              prefix=maybe_prefix(
+                                                  prefix, "lm_head"))
+        else:
+            self.lm_head = PPMissingLayer()
+
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+        self.sampler = get_sampler()
+
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.model.get_input_embeddings(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        hidden_states = self.model(input_ids, positions, intermediate_tensors,
+                                   inputs_embeds)
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.logits_processor(self.lm_head, hidden_states,
+                                       sampling_metadata)
+        return logits
+
+    def sample(
+        self,
+        logits: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        next_tokens = self.sampler(logits, sampling_metadata)
+        return next_tokens
+
+    def load_weights(self, weights: Iterable[Tuple[str,
+                                                   torch.Tensor]]) -> Set[str]:
+        loader = AutoWeightsLoader(
+            self,
+            skip_prefixes=(["lm_head."]
+                           if self.config.tie_word_embeddings else None),
+        )
+        return loader.load_weights(weights)
diff --git a/vllm/model_executor/models/qwen3_moe.py b/vllm/model_executor/models/qwen3_moe.py
new file mode 100644
index 000000000..bda3de2d3
--- /dev/null
+++ b/vllm/model_executor/models/qwen3_moe.py
@@ -0,0 +1,531 @@
+# SPDX-License-Identifier: Apache-2.0
+
+# Copyright 2024 The Qwen team.
+# Copyright 2023 The vLLM team.
+# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.
+#
+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX
+# and OPT implementations in this library. It has been modified from its
+# original forms to accommodate minor architectural differences compared
+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""Inference-only Qwen3MoE model compatible with HuggingFace weights."""
+from typing import Any, Dict, Iterable, Optional, Set, Tuple, Union
+
+import torch
+from torch import nn
+from transformers import PretrainedConfig
+
+from vllm.attention import Attention
+from vllm.compilation.decorators import support_torch_compile
+from vllm.config import CacheConfig, VllmConfig
+from vllm.distributed import (get_pp_group,
+                              get_tensor_model_parallel_world_size,
+                              tensor_model_parallel_all_reduce)
+from vllm.logger import init_logger
+from vllm.model_executor.layers.activation import SiluAndMul
+from vllm.model_executor.layers.fused_moe import FusedMoE
+from vllm.model_executor.layers.layernorm import RMSNorm
+from vllm.model_executor.layers.linear import (MergedColumnParallelLinear,
+                                               QKVParallelLinear,
+                                               ReplicatedLinear,
+                                               RowParallelLinear)
+from vllm.model_executor.layers.logits_processor import LogitsProcessor
+from vllm.model_executor.layers.quantization import QuantizationConfig
+from vllm.model_executor.layers.rotary_embedding import get_rope
+from vllm.model_executor.layers.sampler import SamplerOutput, get_sampler
+from vllm.model_executor.layers.vocab_parallel_embedding import (
+    ParallelLMHead, VocabParallelEmbedding)
+from vllm.model_executor.model_loader.weight_utils import default_weight_loader
+from vllm.model_executor.sampling_metadata import SamplingMetadata
+from vllm.sequence import IntermediateTensors
+
+from .interfaces import SupportsPP
+from .utils import (extract_layer_index, is_pp_missing_parameter,
+                    make_empty_intermediate_tensors_factory, make_layers,
+                    maybe_prefix)
+
+logger = init_logger(__name__)
+
+
+class Qwen3MoeMLP(nn.Module):
+
+    def __init__(
+        self,
+        hidden_size: int,
+        intermediate_size: int,
+        hidden_act: str,
+        quant_config: Optional[QuantizationConfig] = None,
+        reduce_results: bool = True,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.gate_up_proj = MergedColumnParallelLinear(
+            hidden_size, [intermediate_size] * 2,
+            bias=False,
+            quant_config=quant_config,
+            prefix=f"{prefix}.gate_up_proj")
+        self.down_proj = RowParallelLinear(intermediate_size,
+                                           hidden_size,
+                                           bias=False,
+                                           quant_config=quant_config,
+                                           reduce_results=reduce_results,
+                                           prefix=f"{prefix}.down_proj")
+        if hidden_act != "silu":
+            raise ValueError(f"Unsupported activation: {hidden_act}. "
+                             "Only silu is supported for now.")
+        self.act_fn = SiluAndMul()
+
+    def forward(self, x):
+        gate_up, _ = self.gate_up_proj(x)
+        x = self.act_fn(gate_up)
+        x, _ = self.down_proj(x)
+        return x
+
+
+class Qwen3MoeSparseMoeBlock(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ):
+        super().__init__()
+        self.tp_size = get_tensor_model_parallel_world_size()
+
+        if self.tp_size > config.num_experts:
+            raise ValueError(
+                f"Tensor parallel size {self.tp_size} is greater than "
+                f"the number of experts {config.num_experts}.")
+
+        self.experts = FusedMoE(num_experts=config.num_experts,
+                                top_k=config.num_experts_per_tok,
+                                hidden_size=config.hidden_size,
+                                intermediate_size=config.moe_intermediate_size,
+                                reduce_results=False,
+                                renormalize=config.norm_topk_prob,
+                                quant_config=quant_config,
+                                prefix=f"{prefix}.experts")
+
+        self.gate = ReplicatedLinear(config.hidden_size,
+                                     config.num_experts,
+                                     bias=False,
+                                     quant_config=None,
+                                     prefix=f"{prefix}.gate")
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        # NOTE: hidden_states can have either 1D or 2D shape.
+        orig_shape = hidden_states.shape
+        hidden_dim = hidden_states.shape[-1]
+        hidden_states = hidden_states.view(-1, hidden_dim)
+
+        # router_logits: (num_tokens, n_experts)
+        router_logits, _ = self.gate(hidden_states)
+        final_hidden_states = self.experts(hidden_states=hidden_states,
+                                           router_logits=router_logits)
+        final_hidden_states = final_hidden_states
+        if self.tp_size > 1:
+            final_hidden_states = tensor_model_parallel_all_reduce(
+                final_hidden_states)
+
+        return final_hidden_states.view(orig_shape)
+
+
+class Qwen3MoeAttention(nn.Module):
+
+    def __init__(
+        self,
+        hidden_size: int,
+        num_heads: int,
+        num_kv_heads: int,
+        rope_theta: float = 10000,
+        rope_scaling: Optional[Dict[str, Any]] = None,
+        max_position_embeddings: int = 8192,
+        head_dim: Optional[int] = None,
+        rms_norm_eps: float = 1e-06,
+        qkv_bias: bool = False,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = hidden_size
+        tp_size = get_tensor_model_parallel_world_size()
+        self.total_num_heads = num_heads
+        assert self.total_num_heads % tp_size == 0
+        self.num_heads = self.total_num_heads // tp_size
+        self.total_num_kv_heads = num_kv_heads
+        if self.total_num_kv_heads >= tp_size:
+            # Number of KV heads is greater than TP size, so we partition
+            # the KV heads across multiple tensor parallel GPUs.
+            assert self.total_num_kv_heads % tp_size == 0
+        else:
+            # Number of KV heads is less than TP size, so we replicate
+            # the KV heads across multiple tensor parallel GPUs.
+            assert tp_size % self.total_num_kv_heads == 0
+        self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size)
+        self.head_dim = head_dim or (hidden_size // self.total_num_heads)
+        self.q_size = self.num_heads * self.head_dim
+        self.kv_size = self.num_kv_heads * self.head_dim
+        self.scaling = self.head_dim**-0.5
+        self.rope_theta = rope_theta
+        self.max_position_embeddings = max_position_embeddings
+
+        self.qkv_proj = QKVParallelLinear(hidden_size,
+                                          self.head_dim,
+                                          self.total_num_heads,
+                                          self.total_num_kv_heads,
+                                          bias=qkv_bias,
+                                          quant_config=quant_config,
+                                          prefix=f"{prefix}.qkv_proj")
+
+        self.o_proj = RowParallelLinear(self.total_num_heads * self.head_dim,
+                                        hidden_size,
+                                        bias=False,
+                                        quant_config=quant_config,
+                                        prefix=f"{prefix}.o_proj")
+
+        self.rotary_emb = get_rope(
+            self.head_dim,
+            rotary_dim=self.head_dim,
+            max_position=max_position_embeddings,
+            base=rope_theta,
+            rope_scaling=rope_scaling,
+        )
+        self.attn = Attention(self.num_heads,
+                              self.head_dim,
+                              self.scaling,
+                              num_kv_heads=self.num_kv_heads,
+                              cache_config=cache_config,
+                              quant_config=quant_config,
+                              prefix=f"{prefix}.attn")
+
+        self.q_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
+        self.k_norm = RMSNorm(self.head_dim, eps=rms_norm_eps)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+    ) -> torch.Tensor:
+        qkv, _ = self.qkv_proj(hidden_states)
+        q, k, v = qkv.split([self.q_size, self.kv_size, self.kv_size], dim=-1)
+        # Add qk-norm
+        q_by_head = q.view(*q.shape[:-1], q.shape[-1] // self.head_dim,
+                           self.head_dim)
+        q_by_head = self.q_norm.forward_xpu(q_by_head.contiguous())
+        q = q_by_head.view(q.shape)
+
+        k_by_head = k.view(*k.shape[:-1], k.shape[-1] // self.head_dim,
+                           self.head_dim)
+        k_by_head = self.k_norm.forward_xpu(k_by_head.contiguous())
+        k = k_by_head.view(k.shape)
+        q, k = self.rotary_emb(positions, q, k)
+        attn_output = self.attn(q, k, v)
+        output, _ = self.o_proj(attn_output)
+        return output
+
+
+class Qwen3MoeDecoderLayer(nn.Module):
+
+    def __init__(
+        self,
+        config: PretrainedConfig,
+        cache_config: Optional[CacheConfig] = None,
+        quant_config: Optional[QuantizationConfig] = None,
+        prefix: str = "",
+    ) -> None:
+        super().__init__()
+        self.hidden_size = config.hidden_size
+        rope_theta = getattr(config, "rope_theta", 10000)
+        rope_scaling = getattr(config, "rope_scaling", None)
+        max_position_embeddings = getattr(config, "max_position_embeddings",
+                                          8192)
+        self.self_attn = Qwen3MoeAttention(
+            hidden_size=self.hidden_size,
+            num_heads=config.num_attention_heads,
+            num_kv_heads=config.num_key_value_heads,
+            rope_theta=rope_theta,
+            rope_scaling=rope_scaling,
+            max_position_embeddings=max_position_embeddings,
+            rms_norm_eps=config.rms_norm_eps,
+            qkv_bias=getattr(config, 'attention_bias', False),
+            head_dim=getattr(config, 'head_dim', None),
+            cache_config=cache_config,
+            quant_config=quant_config,
+            prefix=f"{prefix}.self_attn",
+        )
+
+        # `mlp_only_layers` in the config.
+        layer_idx = extract_layer_index(prefix)
+        mlp_only_layers = ([] if not hasattr(config, "mlp_only_layers") else
+                           config.mlp_only_layers)
+        if (layer_idx not in mlp_only_layers) and (
+                config.num_experts > 0 and
+            (layer_idx + 1) % config.decoder_sparse_step == 0):
+            self.mlp = Qwen3MoeSparseMoeBlock(config=config,
+                                              quant_config=quant_config,
+                                              prefix=f"{prefix}.mlp")
+        else:
+            self.mlp = Qwen3MoeMLP(hidden_size=config.hidden_size,
+                                   intermediate_size=config.intermediate_size,
+                                   hidden_act=config.hidden_act,
+                                   quant_config=quant_config,
+                                   prefix=f"{prefix}.mlp")
+        self.input_layernorm = RMSNorm(config.hidden_size,
+                                       eps=config.rms_norm_eps)
+        self.post_attention_layernorm = RMSNorm(config.hidden_size,
+                                                eps=config.rms_norm_eps)
+
+    def forward(
+        self,
+        positions: torch.Tensor,
+        hidden_states: torch.Tensor,
+        residual: Optional[torch.Tensor],
+    ) -> torch.Tensor:
+        # Self Attention
+        if residual is None:
+            residual = hidden_states
+            hidden_states = self.input_layernorm(hidden_states)
+        else:
+            hidden_states, residual = self.input_layernorm(
+                hidden_states, residual)
+        hidden_states = self.self_attn(
+            positions=positions,
+            hidden_states=hidden_states,
+        )
+
+        # Fully Connected
+        hidden_states, residual = self.post_attention_layernorm(
+            hidden_states, residual)
+        hidden_states = self.mlp(hidden_states)
+        return hidden_states, residual
+
+
+@support_torch_compile
+class Qwen3MoeModel(nn.Module):
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+
+        config = vllm_config.model_config.hf_config
+        cache_config = vllm_config.cache_config
+        quant_config = vllm_config.quant_config
+
+        self.padding_idx = config.pad_token_id
+        self.vocab_size = config.vocab_size
+
+        self.embed_tokens = VocabParallelEmbedding(
+            config.vocab_size,
+            config.hidden_size,
+            prefix=f"{prefix}.embed_tokens")
+        self.start_layer, self.end_layer, self.layers = make_layers(
+            config.num_hidden_layers,
+            lambda prefix: Qwen3MoeDecoderLayer(config=config,
+                                                cache_config=cache_config,
+                                                quant_config=quant_config,
+                                                prefix=prefix),
+            prefix=f"{prefix}.layers",
+        )
+        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
+        self.make_empty_intermediate_tensors = (
+            make_empty_intermediate_tensors_factory(
+                ["hidden_states", "residual"], config.hidden_size))
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.embed_tokens(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        if get_pp_group().is_first_rank:
+            if inputs_embeds is not None:
+                hidden_states = inputs_embeds
+            else:
+                hidden_states = self.get_input_embeddings(input_ids)
+            residual = None
+        else:
+            assert intermediate_tensors is not None
+            hidden_states = intermediate_tensors["hidden_states"]
+            residual = intermediate_tensors["residual"]
+        for i in range(self.start_layer, self.end_layer):
+            layer = self.layers[i]
+            hidden_states, residual = layer(positions, hidden_states, residual)
+        if not get_pp_group().is_last_rank:
+            return IntermediateTensors({
+                "hidden_states": hidden_states,
+                "residual": residual
+            })
+        hidden_states, _ = self.norm(hidden_states, residual)
+        return hidden_states
+
+
+class Qwen3MoeForCausalLM(nn.Module, SupportsPP):
+
+    fall_back_to_pt_during_load = False
+
+    def __init__(self, *, vllm_config: VllmConfig, prefix: str = ""):
+        super().__init__()
+        config = vllm_config.model_config.hf_config
+        quant_config = vllm_config.quant_config
+        self.config = config
+        self.quant_config = quant_config
+        self.model = Qwen3MoeModel(vllm_config=vllm_config,
+                                   prefix=maybe_prefix(prefix, "model"))
+        self.lm_head = ParallelLMHead(config.vocab_size,
+                                      config.hidden_size,
+                                      quant_config=quant_config)
+        if self.config.tie_word_embeddings:
+            self.lm_head.weight = self.model.embed_tokens.weight
+        self.logits_processor = LogitsProcessor(config.vocab_size)
+        self.sampler = get_sampler()
+        self.make_empty_intermediate_tensors = (
+            self.model.make_empty_intermediate_tensors)
+
+    def get_input_embeddings(self, input_ids: torch.Tensor) -> torch.Tensor:
+        return self.model.get_input_embeddings(input_ids)
+
+    def forward(
+        self,
+        input_ids: torch.Tensor,
+        positions: torch.Tensor,
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+    ) -> Union[torch.Tensor, IntermediateTensors]:
+        hidden_states = self.model(input_ids, positions, intermediate_tensors,
+                                   inputs_embeds)
+        return hidden_states
+
+    def compute_logits(
+        self,
+        hidden_states: torch.Tensor,
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[torch.Tensor]:
+        logits = self.logits_processor(self.lm_head, hidden_states,
+                                       sampling_metadata)
+        return logits
+
+    def sample(
+        self,
+        logits: Optional[torch.Tensor],
+        sampling_metadata: SamplingMetadata,
+    ) -> Optional[SamplerOutput]:
+        next_tokens = self.sampler(logits, sampling_metadata)
+        return next_tokens
+
+    def load_weights(self, weights: Iterable[Tuple[str,
+                                                   torch.Tensor]]) -> Set[str]:
+        stacked_params_mapping = [
+            # (param_name, shard_name, shard_id)
+            ("qkv_proj", "q_proj", "q"),
+            ("qkv_proj", "k_proj", "k"),
+            ("qkv_proj", "v_proj", "v"),
+            ("gate_up_proj", "gate_proj", 0),
+            ("gate_up_proj", "up_proj", 1),
+        ]
+
+        # Params for weights, fp8 weight scales, fp8 activation scales
+        # (param_name, weight_name, expert_id, shard_id)
+        expert_params_mapping = FusedMoE.make_expert_params_mapping(
+            ckpt_gate_proj_name="gate_proj",
+            ckpt_down_proj_name="down_proj",
+            ckpt_up_proj_name="up_proj",
+            num_experts=self.config.num_experts)
+
+        params_dict = dict(self.named_parameters())
+        loaded_params: Set[str] = set()
+        for name, loaded_weight in weights:
+            if "rotary_emb.inv_freq" in name:
+                continue
+            for (param_name, weight_name, shard_id) in stacked_params_mapping:
+                # Skip non-stacked layers and experts (experts handled below).
+                if weight_name not in name:
+                    continue
+                # We have mlp.experts[0].gate_proj in the checkpoint.
+                # Since we handle the experts below in expert_params_mapping,
+                # we need to skip here BEFORE we update the name, otherwise
+                # name will be updated to mlp.experts[0].gate_up_proj, which
+                # will then be updated below in expert_params_mapping
+                # for mlp.experts[0].gate_gate_up_proj, which breaks load.
+                if "mlp.experts" in name:
+                    continue
+                name = name.replace(weight_name, param_name)
+                # Skip loading extra bias for GPTQ models.
+                if ((name.endswith(".bias") or name.endswith("_bias"))
+                        and name not in params_dict):
+                    continue
+                # Skip layers on other devices.
+                if is_pp_missing_parameter(name, self):
+                    continue
+                if name not in params_dict:
+                    continue
+
+                param = params_dict[name]
+                weight_loader = param.weight_loader
+                weight_loader(param, loaded_weight, shard_id)
+                break
+            else:
+                for mapping in expert_params_mapping:
+                    param_name, weight_name, expert_id, shard_id = mapping
+                    if weight_name not in name:
+                        continue
+                    name = name.replace(weight_name, param_name)
+                    # Skip layers on other devices.
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    # Skip loading extra bias for GPTQ models.
+                    if ((name.endswith(".bias") or name.endswith("_bias"))
+                            and name not in params_dict):
+                        continue
+                    param = params_dict[name]
+                    weight_loader = param.weight_loader
+                    weight_loader(param,
+                                  loaded_weight,
+                                  name,
+                                  shard_id=shard_id,
+                                  expert_id=expert_id)
+                    break
+                else:
+                    # Skip loading extra bias for GPTQ models.
+                    if ((name.endswith(".bias") or name.endswith("_bias"))
+                            and name not in params_dict):
+                        continue
+                    # Skip layers on other devices.
+                    if is_pp_missing_parameter(name, self):
+                        continue
+                    # Remapping the name of FP8 kv-scale.
+                    if name.endswith("kv_scale"):
+                        remapped_kv_scale_name = name.replace(
+                            ".kv_scale", ".attn.kv_scale")
+                        if remapped_kv_scale_name not in params_dict:
+                            logger.warning_once(
+                                "Found kv scale in the checkpoint "
+                                f"(e.g. {name}), but not found the expected "
+                                f"name in the model "
+                                f"(e.g. {remapped_kv_scale_name}). "
+                                "kv-scale is not loaded.")
+                            continue
+                        else:
+                            name = remapped_kv_scale_name
+                    param = params_dict[name]
+                    weight_loader = getattr(param, "weight_loader",
+                                            default_weight_loader)
+                    weight_loader(param, loaded_weight)
+            loaded_params.add(name)
+        return loaded_params
diff --git a/vllm/model_executor/models/registry.py b/vllm/model_executor/models/registry.py
index c0a3c59ba..8614c2273 100644
--- a/vllm/model_executor/models/registry.py
+++ b/vllm/model_executor/models/registry.py
@@ -57,6 +57,7 @@ _TEXT_GENERATION_MODELS = {
     "Gemma2ForCausalLM": ("gemma2", "Gemma2ForCausalLM"),
     "Gemma3ForCausalLM": ("gemma3", "Gemma3ForCausalLM"),
     "GlmForCausalLM": ("glm", "GlmForCausalLM"),
+    "Glm4ForCausalLM": ("glm4", "Glm4ForCausalLM"),
     "GPT2LMHeadModel": ("gpt2", "GPT2LMHeadModel"),
     "GPTBigCodeForCausalLM": ("gpt_bigcode", "GPTBigCodeForCausalLM"),
     "GPTJForCausalLM": ("gpt_j", "GPTJForCausalLM"),
@@ -101,6 +102,8 @@ _TEXT_GENERATION_MODELS = {
     "QWenLMHeadModel": ("qwen", "QWenLMHeadModel"),
     "Qwen2ForCausalLM": ("qwen2", "Qwen2ForCausalLM"),
     "Qwen2MoeForCausalLM": ("qwen2_moe", "Qwen2MoeForCausalLM"),
+    "Qwen3ForCausalLM": ("qwen3", "Qwen3ForCausalLM"),
+    "Qwen3MoeForCausalLM": ("qwen3_moe", "Qwen3MoeForCausalLM"),
     "RWForCausalLM": ("falcon", "FalconForCausalLM"),
     "StableLMEpochForCausalLM": ("stablelm", "StablelmForCausalLM"),
     "StableLmForCausalLM": ("stablelm", "StablelmForCausalLM"),
@@ -108,6 +111,7 @@ _TEXT_GENERATION_MODELS = {
     "SolarForCausalLM": ("solar", "SolarForCausalLM"),
     "TeleChat2ForCausalLM": ("telechat2", "TeleChat2ForCausalLM"),
     "TeleFLMForCausalLM": ("teleflm", "TeleFLMForCausalLM"),
+    "TeleChatForCausalLM": ("telechat2", "TeleChat2ForCausalLM"),
     "XverseForCausalLM": ("llama", "LlamaForCausalLM"),
     "Zamba2ForCausalLM": ("zamba2", "Zamba2ForCausalLM"),
     # [Encoder-decoder]
diff --git a/vllm/model_executor/models/siglip.py b/vllm/model_executor/models/siglip.py
index cecad9e89..7eaabd1db 100644
--- a/vllm/model_executor/models/siglip.py
+++ b/vllm/model_executor/models/siglip.py
@@ -140,6 +140,74 @@ class SiglipVisionEmbeddings(nn.Module):
         return embeddings
 
 
+class SelfAttention(nn.Module):
+    """Multi-headed attention without any cache, used for ViT."""
+
+    def __init__(
+        self,
+        num_heads: int,
+        head_size: int,
+        scale: float,
+        num_kv_heads: Optional[int] = None,
+    ):
+        super().__init__()
+        self.num_heads = num_heads
+        self.head_size = head_size
+        self.scale = scale
+        self.num_kv_heads = num_heads if num_kv_heads is None else num_kv_heads
+
+        assert self.num_heads % self.num_kv_heads == 0
+        self.num_queries_per_kv = self.num_heads // self.num_kv_heads
+
+    def forward(
+        self,
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+    ) -> torch.Tensor:
+        """Input shape: batch_size x seq_len x hidden_size"""
+        # TODO(Isotr0py): Use existing backend implementations and support FA2
+        bsz, q_len, _ = query.size()
+        kv_len = key.size(1)
+
+        query = query.view(bsz, q_len, self.num_heads, self.head_size)
+        key = key.view(bsz, kv_len, self.num_kv_heads, self.head_size)
+        value = value.view(bsz, kv_len, self.num_kv_heads, self.head_size)
+
+        if (num_repeat := self.num_queries_per_kv) > 1:
+            # Handle MQA and GQA
+            key = torch.repeat_interleave(key, num_repeat, dim=2)
+            value = torch.repeat_interleave(value, num_repeat, dim=2)
+
+        query, key, value = (x.transpose(1, 2)
+                                for x in (query, key, value))
+        from vllm.attention.backends.ipex_attn import use_sdp_causal
+        import xe_addons, math
+        from vllm.attention.backends.abstract import AttentionType
+        mask = None
+        scale = 1 / math.sqrt(self.head_size) if self.scale is None else self.scale
+        from ipex_llm.transformers.models.common import padding_qkv_hd
+
+        num = 80
+        if self.head_size > 80:
+            num = 128
+        query, key, value, = padding_qkv_hd(
+            query, key, value,
+            self.head_size, num
+        )
+        if use_sdp_causal(query.shape[-1], query, 0, AttentionType.DECODER):
+            out = xe_addons.sdp_non_causal(query.contiguous(), key.contiguous(), value.contiguous(), mask, scale)[:, :, :, :self.head_size].transpose(1, 2)
+        # import torch.nn.functional as F
+        # out = F.scaled_dot_product_attention(query,
+        #                                      key,
+        #                                      value,
+        #                                      scale=self.scale)
+        # out = out.transpose(1, 2)
+        #return out.view(bsz, q_len, -1)
+        return out.reshape(bsz, q_len, -1)
+
+
+
 class SiglipAttention(nn.Module):
 
     def __init__(
@@ -179,8 +247,10 @@ class SiglipAttention(nn.Module):
         self.tp_size = get_tensor_model_parallel_world_size()
         self.num_heads_per_partition = divide(self.num_heads, self.tp_size)
 
-        self.attn = MultiHeadAttention(self.num_heads_per_partition,
-                                       self.head_dim, self.scale)
+        # self.attn = MultiHeadAttention(self.num_heads_per_partition,
+        #                                self.head_dim, self.scale)
+        self.attn = SelfAttention(self.num_heads_per_partition,
+                                  self.head_dim, self.scale)
 
     def forward(
         self,
diff --git a/vllm/model_executor/models/telechat2.py b/vllm/model_executor/models/telechat2.py
index a38035e37..570f2bcdd 100644
--- a/vllm/model_executor/models/telechat2.py
+++ b/vllm/model_executor/models/telechat2.py
@@ -22,10 +22,12 @@
 from typing import Iterable, Set, Tuple
 
 import torch
+import torch.nn as nn
 
 from vllm.config import VllmConfig
 from vllm.model_executor.model_loader.weight_utils import default_weight_loader
 from vllm.model_executor.models.llama import LlamaForCausalLM, LlamaModel
+from .llama import LlamaDecoderLayer
 
 from .utils import (AutoWeightsLoader, PPMissingLayer, WeightsMapper,
                     is_pp_missing_parameter)
@@ -44,9 +46,9 @@ class TeleChat2Model(LlamaModel):
         for layer in self.layers:
             if not isinstance(layer, PPMissingLayer):
                 layer.self_attn.qkv_proj.bias = None
-                layer.self_attn.qkv_proj.skip_bias_add = True
+                #layer.self_attn.qkv_proj.skip_bias_add = True
                 layer.mlp.gate_up_proj.bias = None
-                layer.mlp.gate_up_proj.skip_bias_add = True
+                #layer.mlp.gate_up_proj.skip_bias_add = True
 
     def load_weights(self, weights: Iterable[Tuple[str,
                                                    torch.Tensor]]) -> Set[str]:
@@ -120,7 +122,10 @@ class TeleChat2ForCausalLM(LlamaForCausalLM):
         },
     )
 
-    def _init_model(self, vllm_config: VllmConfig, prefix: str = ""):
+    def _init_model(self,
+                    vllm_config: VllmConfig,
+                    prefix: str = "",
+                    layer_type: type[nn.Module] = LlamaDecoderLayer):
         return TeleChat2Model(vllm_config=vllm_config, prefix=prefix)
 
     def load_weights(self, weights: Iterable[Tuple[str,
diff --git a/vllm/multimodal/utils.py b/vllm/multimodal/utils.py
index fc0fb8929..6454e7006 100644
--- a/vllm/multimodal/utils.py
+++ b/vllm/multimodal/utils.py
@@ -118,20 +118,29 @@ class MediaConnector:
         fetch_timeout: Optional[int] = None,
     ) -> _M:
         url_spec = urlparse(url)
-
         if url_spec.scheme.startswith("http"):
-            connection = self.connection
-            data = await connection.async_get_bytes(url, timeout=fetch_timeout)
+            try:
+                import requests
+                image = Image.open(requests.get(url, stream=True).raw)
+                return image
+            except:
+                connection = self.connection
+                data = await connection.async_get_bytes(url, timeout=fetch_timeout)
 
-            return media_io.load_bytes(data)
+                return media_io.load_bytes(data)
 
         if url_spec.scheme == "data":
             return self._load_data_url(url_spec, media_io)
 
         if url_spec.scheme == "file":
             return self._load_file_url(url_spec, media_io)
+    
+        import os
+        if url_spec.scheme == "" and os.path.exists(url):
+            image = Image.open(url).convert('RGB')
+            return image
 
-        msg = "The URL must be either a HTTP, data or file URL."
+        msg = "The URL must be either a HTTP, data, file URL or a exist path."
         raise ValueError(msg)
 
     def fetch_audio(
diff --git a/vllm/platforms/interface.py b/vllm/platforms/interface.py
index b6f6029de..b90fea9fd 100644
--- a/vllm/platforms/interface.py
+++ b/vllm/platforms/interface.py
@@ -40,8 +40,9 @@ class _Backend(enum.Enum):
     HPU_ATTN = enum.auto()
     PALLAS = enum.auto()
     PALLAS_VLLM_V1 = enum.auto()
-    IPEX = enum.auto()
     BLOCK_SPARSE_FLASH_ATTN = enum.auto()
+    IPEX = enum.auto()
+    IPEX_V1 = enum.auto()
     NO_ATTENTION = enum.auto()
 
 
@@ -131,6 +132,9 @@ class Platform:
 
     def is_cpu(self) -> bool:
         return self._enum == PlatformEnum.CPU
+    
+    def is_xpu(self) -> bool:
+        return self._enum == PlatformEnum.XPU
 
     def is_neuron(self) -> bool:
         return self._enum == PlatformEnum.NEURON
diff --git a/vllm/platforms/xpu.py b/vllm/platforms/xpu.py
index 225e756cd..25b83549a 100644
--- a/vllm/platforms/xpu.py
+++ b/vllm/platforms/xpu.py
@@ -4,6 +4,7 @@ from typing import TYPE_CHECKING, Optional
 
 import torch
 
+import vllm.envs as envs
 from vllm.logger import init_logger
 
 from .interface import DeviceCapability, Platform, PlatformEnum, _Backend
@@ -25,6 +26,9 @@ class XPUPlatform(Platform):
     # see https://github.com/ray-project/ray/blob/6a5eb5865eeb9ccf058a79b44f107e327e360673/python/ray/_private/accelerators/intel_gpu.py#L20 # noqa: E501
     ray_device_key: str = "GPU"
     device_control_env_var: str = "ONEAPI_DEVICE_SELECTOR"
+    additional_env_vars: list[str] = [
+        "IPEX_LLM_LOWBIT",
+    ]
 
     @classmethod
     def get_attn_backend_cls(cls, selected_backend: _Backend, head_size: int,
@@ -33,8 +37,13 @@ class XPUPlatform(Platform):
                              use_mla: bool) -> str:
         if selected_backend != _Backend.IPEX:
             logger.info("Cannot use %s backend on XPU.", selected_backend)
-        logger.info("Using IPEX attention backend.")
-        return "vllm.attention.backends.ipex_attn.IpexAttnBackend"
+        use_v1 = envs.VLLM_USE_V1
+        if use_v1:
+            logger.info("Using IPEX_V1 attention backend.")
+            return "vllm.v1.attention.backends.ipex_attn.IPEXAttentionBackend"
+        else:
+            logger.info("Using IPEX attention backend.")
+            return "vllm.attention.backends.ipex_attn.IpexAttnBackend"
 
     @staticmethod
     def get_device_capability(
@@ -63,6 +72,8 @@ class XPUPlatform(Platform):
     @classmethod
     def check_and_update_config(cls, vllm_config: VllmConfig) -> None:
         cache_config = vllm_config.cache_config
+        if cache_config and envs.VLLM_USE_V1:
+            cache_config.block_size = 64
         if cache_config and cache_config.block_size is None:
             cache_config.block_size = 16
 
@@ -87,31 +98,46 @@ class XPUPlatform(Platform):
             raise NotImplementedError(
                 "XPU does not support speculative decoding")
 
-        if vllm_config.device_config is not None:
-            assert vllm_config.device_config.device_type == "xpu"
+        # if vllm_config.device_config is not None:
+        #     assert vllm_config.device_config.device_type == "xpu"
 
         # check and update parallel config
         parallel_config = vllm_config.parallel_config
-        if parallel_config.worker_cls == "auto":
+
+        # TODO(xiangyu): check logic here
+
+        if envs.VLLM_USE_V1:
+            parallel_config.worker_cls = \
+                        "vllm.v1.worker.xpu_worker.XPUWorker"
+        else:
             parallel_config.worker_cls = "vllm.worker.xpu_worker.XPUWorker"
 
         if parallel_config.distributed_executor_backend is None:
-            parallel_config.distributed_executor_backend = "ray"
+            if parallel_config.world_size > 1:
+                parallel_config.distributed_executor_backend = "ray"
+            else:
+                parallel_config.distributed_executor_backend = "uni"
         elif parallel_config.distributed_executor_backend == "mp":
             # FIXME(kunshang):
             # spawn needs calling `if __name__ == '__main__':``
             # fork is not supported for xpu start new process.
-            logger.error(
-                "Both start methods (spawn and fork) have issue "
-                "on XPU if you use mp backend, setting it to ray instead.")
-            parallel_config.distributed_executor_backend = "ray"
-
-        elif parallel_config.distributed_executor_backend != "ray":
+            logger.warning(
+                "Please use spawn as start method if you want to use mp.")
+        elif parallel_config.distributed_executor_backend != "ray" and \
+                parallel_config.distributed_executor_backend != "uni":
             logger.warning(
                 "%s is not supported on XPU, fallback to ray distributed"
                 " executor backend.",
                 parallel_config.distributed_executor_backend)
             parallel_config.distributed_executor_backend = "ray"
+        # if (parallel_config.distributed_executor_backend is not None
+        #         and parallel_config.distributed_executor_backend != "ray"):
+        #     logger.warning(
+        #         "%s is not supported on XPU, fallback to ray distributed"
+        #         " executor backend.",
+        #         parallel_config.distributed_executor_backend)
+        #     parallel_config.distributed_executor_backend = "ray"
+        
 
     @classmethod
     def is_pin_memory_available(cls):
@@ -140,3 +166,7 @@ class XPUPlatform(Platform):
     @classmethod
     def get_device_communicator_cls(cls) -> str:
         return "vllm.distributed.device_communicators.xpu_communicator.XpuCommunicator"  # noqa
+
+    @classmethod
+    def use_all_gather(cls) -> bool:
+        return False
\ No newline at end of file
diff --git a/vllm/transformers_utils/configs/__init__.py b/vllm/transformers_utils/configs/__init__.py
index 53699341b..6bc039068 100644
--- a/vllm/transformers_utils/configs/__init__.py
+++ b/vllm/transformers_utils/configs/__init__.py
@@ -1,6 +1,7 @@
 # SPDX-License-Identifier: Apache-2.0
 
 from vllm.transformers_utils.configs.chatglm import ChatGLMConfig
+from vllm.transformers_utils.configs.telechat2 import Telechat2Config
 from vllm.transformers_utils.configs.cohere2 import Cohere2Config
 from vllm.transformers_utils.configs.dbrx import DbrxConfig
 from vllm.transformers_utils.configs.deepseek_vl2 import DeepseekVLV2Config
@@ -26,6 +27,7 @@ from vllm.transformers_utils.configs.telechat2 import Telechat2Config
 from vllm.transformers_utils.configs.ultravox import UltravoxConfig
 
 __all__ = [
+    "Telechat2Config",
     "ChatGLMConfig",
     "Cohere2Config",
     "DbrxConfig",
diff --git a/vllm/utils.py b/vllm/utils.py
index 5f32f8cb6..2ee0c1906 100644
--- a/vllm/utils.py
+++ b/vllm/utils.py
@@ -128,6 +128,8 @@ STR_NOT_IMPL_ENC_DEC_ERR_STRS = {
     "STR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER": STR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER,
 }
 
+BMG_TARGET_IDS = ["0xe20b", "0xe210"]
+
 # Constants related to forcing the attention backend selection
 
 # String name of register which may be set in order to
@@ -2564,3 +2566,14 @@ def sha256(input) -> int:
     input_bytes = pickle.dumps(input, protocol=pickle.HIGHEST_PROTOCOL)
     return int.from_bytes(hashlib.sha256(input_bytes).digest(),
                           byteorder="big")
+
+@cache
+def is_bmg_platform():
+    if not torch.xpu.is_available():
+        raise ValueError("Cannot detect the usage of XPU!")
+    device_index = torch.xpu.current_device()
+    device_name = torch.xpu.get_device_name(device_index)
+    for target_id in BMG_TARGET_IDS:
+        if target_id in device_name:
+            return True
+    return False
\ No newline at end of file
diff --git a/vllm/v1/attention/backends/flash_attn.py b/vllm/v1/attention/backends/flash_attn.py
index c271f438e..cf7180606 100755
--- a/vllm/v1/attention/backends/flash_attn.py
+++ b/vllm/v1/attention/backends/flash_attn.py
@@ -14,8 +14,8 @@ from vllm.attention.ops.triton_merge_attn_states import merge_attn_states
 from vllm.logger import init_logger
 from vllm.platforms import current_platform
 from vllm.utils import cdiv
-from vllm.vllm_flash_attn.fa_utils import (flash_attn_supports_fp8,
-                                           get_flash_attn_version)
+# from vllm.vllm_flash_attn.fa_utils import (flash_attn_supports_fp8,
+#                                            get_flash_attn_version)
 
 if TYPE_CHECKING:
     from vllm.v1.core.sched.output import SchedulerOutput
@@ -640,6 +640,7 @@ def cascade_attention(
     k_descale: Optional[torch.Tensor] = None,
     v_descale: Optional[torch.Tensor] = None,
 ) -> torch.Tensor:
+
     assert alibi_slopes is None, ("Cascade attention does not support ALiBi.")
     # TODO: Support sliding window.
     assert sliding_window == (-1, -1), (
diff --git a/vllm/v1/attention/backends/ipex_attn.py b/vllm/v1/attention/backends/ipex_attn.py
new file mode 100644
index 000000000..f4a435eaa
--- /dev/null
+++ b/vllm/v1/attention/backends/ipex_attn.py
@@ -0,0 +1,392 @@
+from dataclasses import dataclass
+from typing import Any, Dict, List, Optional, Tuple, Type
+
+import torch
+
+from vllm._ipex_ops import ipex_ops
+from vllm.attention.backends.abstract import (AttentionBackend, AttentionImpl,
+                                              AttentionMetadata, AttentionType, AttentionLayer)
+from vllm.forward_context import get_forward_context
+from vllm.v1.attention.backends.flash_attn import FlashAttentionMetadata
+from vllm.attention.ops.paged_attn import (PagedAttention,
+                                           PagedAttentionMetadata)
+from vllm.attention.backends.ipex_attn import use_gqa_kernel
+from vllm.utils import is_bmg_platform
+import os
+
+@dataclass
+class IPEXAttentionMetadata(FlashAttentionMetadata):
+    seq_start_loc: torch.Tensor = torch.tensor([0], dtype=torch.int64)
+
+
+class IPEXAttentionBackend(AttentionBackend):
+
+    @staticmethod
+    def get_supported_head_sizes() -> List[int]:
+        return [32, 64, 96, 128, 160, 192, 224, 256]
+
+    @staticmethod
+    def get_name() -> str:
+        return "IPEX_V1"
+
+    @staticmethod
+    def get_impl_cls() -> Type["IPEXAttentionBackendImpl"]:
+        return IPEXAttentionBackendImpl
+
+    @staticmethod
+    def get_metadata_cls() -> Type["AttentionMetadata"]:
+        return IPEXAttentionMetadata
+
+    @staticmethod
+    def get_kv_cache_shape(
+        num_blocks: int,
+        block_size: int,
+        num_kv_heads: int,
+        head_size: int,
+    ) -> Tuple[int, ...]:
+        # if block_size % 16 != 0:
+            # raise ValueError("Block size must be a multiple of 16.")
+        # This needs to be changed...
+        return (2, num_blocks, block_size, num_kv_heads, head_size)
+        # return PagedAttention.get_kv_cache_shape(num_blocks, block_size,
+        #                                          num_kv_heads, head_size)
+
+
+
+class IPEXAttentionBackendImpl(AttentionImpl):
+
+    def __init__(
+        self,
+        num_heads: int,
+        head_size: int,
+        scale: float,
+        num_kv_heads: int,
+        alibi_slopes: Optional[list[float]],
+        sliding_window: Optional[int],
+        kv_cache_dtype: str,
+        blocksparse_params: Optional[dict[str, Any]] = None,
+        logits_soft_cap: Optional[float] = None,
+        attn_type: str = AttentionType.DECODER,
+        use_irope: bool = False,
+    ) -> None:
+        if blocksparse_params is not None:
+            raise ValueError(
+                "FlashAttention does not support block-sparse attention.")
+        self.num_heads = num_heads
+        self.head_size = head_size
+        self.scale = float(scale)
+        self.num_kv_heads = num_kv_heads
+        if alibi_slopes is not None:
+            alibi_slopes = torch.tensor(alibi_slopes, dtype=torch.float32)
+        self.alibi_slopes = alibi_slopes
+        if sliding_window is None:
+            self.sliding_window = (-1, -1)
+        else:
+            self.sliding_window = (sliding_window - 1, 0)
+        self.kv_cache_dtype = kv_cache_dtype
+        self.use_irope = use_irope
+        
+        if logits_soft_cap is None:
+            # In flash-attn, setting logits_soft_cap as 0 means no soft cap.
+            logits_soft_cap = 0
+        self.logits_soft_cap = logits_soft_cap
+
+        assert self.num_heads % self.num_kv_heads == 0
+        self.num_queries_per_kv = self.num_heads // self.num_kv_heads
+
+        support_head_sizes = IPEXAttentionBackend.get_supported_head_sizes()
+        self.using_gqa_kernel = use_gqa_kernel(num_heads, num_kv_heads, head_size, logits_soft_cap)
+        self.is_bmg_platform = is_bmg_platform()
+        if head_size not in support_head_sizes:
+            raise ValueError(
+                f"Head size {head_size} is not supported by FlashAttention. "
+                f"Supported head sizes are: {support_head_sizes}.")
+        if attn_type != AttentionType.DECODER:
+            raise NotImplementedError("Encoder self-attention and "
+                                      "encoder/decoder cross-attention "
+                                      "are not implemented for "
+                                      "IpexAttnBackendImpl")
+
+    def forward(
+        self,
+        layer: AttentionLayer,
+        query: torch.Tensor,
+        key: torch.Tensor,
+        value: torch.Tensor,
+        kv_cache: torch.Tensor,
+        attn_metadata: IPEXAttentionBackend,
+        output: Optional[torch.Tensor] = None,
+    ) -> torch.Tensor:
+        """Forward pass with IPEXAttention.
+
+        Args:
+            query: shape = [num_tokens, num_heads * head_size]
+            key: shape = [num_tokens, num_kv_heads * head_size]
+            value: shape = [num_tokens, num_kv_heads * head_size]
+            kv_cache = [2, num_blocks, block_size, num_kv_heads, head_size]
+            attn_metadata: Metadata for attention.
+        Returns:
+            shape = [num_tokens, num_heads * head_size]
+        """
+        # # NOTE(woosuk): IPEXAttention does not support FP8 KV cache.
+        # assert k_scale == 1.0 and v_scale == 1.0, (
+        #     "key/v_scale is not supported in IPEXAttention.")
+
+        k_scale = layer._k_scale
+        v_scale = layer._v_scale
+        output = torch.empty_like(query)
+        # torch.ops.vllm.ipex_attn_chunked_prefill(
+        ipex_llm_chunked_prefill(
+            output,
+            query,
+            key,
+            value,
+            self.num_heads,
+            self.head_size,
+            self.num_kv_heads,
+            kv_cache,
+            self.kv_cache_dtype,
+            k_scale,
+            v_scale,
+            self.scale,
+            self.using_gqa_kernel,
+            self.is_bmg_platform,
+            self.sliding_window,
+            self.alibi_slopes,
+            self.logits_soft_cap,
+        )
+        return output.view(-1, self.num_heads * self.head_size)
+
+def split_kv_cache_ipexllm(
+    kv_cache: torch.Tensor,
+    num_kv_heads: int,
+    head_size: int,
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    # For GQA kernel, key_cache and value_cache shape should be [num_blocks, num_kv_heads, head_size, block_size]
+    num_blocks = kv_cache.shape[1]
+
+    key_cache = kv_cache[0]
+    key_cache = key_cache.view(num_blocks, num_kv_heads, -1, head_size)
+    value_cache = kv_cache[1]
+    value_cache = value_cache.view(num_blocks, num_kv_heads, -1, head_size)
+    return key_cache, value_cache
+
+def split_kv_cache(
+    kv_cache: torch.Tensor,
+    num_kv_heads: int,
+    head_size: int,
+) -> Tuple[torch.Tensor, torch.Tensor]:
+    x = 16 // kv_cache.element_size()
+    num_blocks = kv_cache.shape[1]
+
+    key_cache = kv_cache[0]
+    key_cache = key_cache.view(num_blocks, num_kv_heads, head_size // x,
+                                -1, x)
+
+    value_cache = kv_cache[1]
+    value_cache = value_cache.view(num_blocks, num_kv_heads, head_size, -1)
+    return key_cache, value_cache
+
+
+
+@torch.library.custom_op("vllm::ipex_attn_fake",
+                         mutates_args=["output", "kv_cache"])
+def ipex_attn_fake(
+    output: torch.Tensor,
+    query: torch.Tensor,
+    key: torch.Tensor,
+    value: torch.Tensor,
+    num_heads: int,
+    head_size: int,
+    num_kv_heads: int,
+    kv_cache: torch.Tensor,
+    kv_cache_dtype: str,
+    k_scale: float,
+    v_scale: float,
+    scale: float,
+    sliding_window: Optional[List[int]] = None,
+    alibi_slopes: Optional[torch.Tensor] = None,
+    logits_soft_cap: Optional[float] = None,
+) -> None:
+    pass
+
+def ipex_llm_chunked_prefill(
+    output: torch.Tensor,
+    query: torch.Tensor,
+    key: torch.Tensor,
+    value: torch.Tensor,
+    num_heads: int,
+    head_size: int,
+    num_kv_heads: int,
+    kv_cache: torch.Tensor,
+    kv_cache_dtype: str,
+    k_scale: float,
+    v_scale: float,
+    scale: float,
+    using_gqa_kernel: bool,
+    is_bmg_platform: bool,
+    sliding_window: Optional[List[int]] = None,
+    alibi_slopes: Optional[torch.Tensor] = None,
+    logits_soft_cap: Optional[float] = None,
+) -> None:
+    context = get_forward_context()
+    current_metadata = context.attn_metadata
+    if current_metadata is None:
+        # Profiling run.
+        return
+    assert current_metadata is not None
+    assert isinstance(current_metadata, FlashAttentionMetadata)
+    attn_metadata: FlashAttentionMetadata = current_metadata
+    num_actual_tokens = attn_metadata.num_actual_tokens
+
+    query = query.view(-1, num_heads, head_size)
+    key = key.view(-1, num_kv_heads, head_size)
+    value = value.view(-1, num_kv_heads, head_size)
+
+    if is_bmg_platform:
+        key_cache, value_cache = kv_cache.unbind(0)
+        ipex_ops.reshape_and_cache_flash(
+            key[:num_actual_tokens],
+            value[:num_actual_tokens],
+            key_cache,
+            value_cache,
+            attn_metadata.slot_mapping,
+            kv_cache_dtype,
+            k_scale,
+            v_scale,
+        )
+        ipex_ops.chunked_prefill(
+            query[:num_actual_tokens].contiguous(),
+            key_cache,
+            value_cache,
+            output[:num_actual_tokens],
+            attn_metadata.query_start_loc,
+            attn_metadata.seq_start_loc,
+            None,
+            attn_metadata.block_table,
+            alibi_slopes,
+            attn_metadata.max_query_len,
+            attn_metadata.max_seq_len,
+            0.0,
+            scale,
+            False,
+            True,
+            False,
+            None,
+        )
+    else:
+        if using_gqa_kernel:
+            key_cache, value_cache = split_kv_cache_ipexllm(
+                    kv_cache, num_kv_heads, head_size)
+            ipex_ops.reshape_and_cache_ipexllm(
+                key[:num_actual_tokens],
+                value[:num_actual_tokens],
+                key_cache,
+                value_cache,
+                attn_metadata.slot_mapping.flatten(),
+                kv_cache_dtype,
+                k_scale,
+                v_scale,
+            )
+        else:
+            key_cache, value_cache = split_kv_cache(
+                kv_cache, num_kv_heads, head_size)
+            ipex_ops.reshape_and_cache(
+                key[:num_actual_tokens],
+                value[:num_actual_tokens],
+                key_cache,
+                value_cache,
+                attn_metadata.slot_mapping.flatten(),
+                kv_cache_dtype,
+                k_scale,
+                v_scale,
+            )
+        # Invoke chunked prefill method...
+        import vllm._C.ops
+        assert head_size == 128 or head_size == 64
+        value = os.environ.get('USE_CONTEXT_V1')
+        query_len = attn_metadata.query_start_loc[1:] - attn_metadata.query_start_loc[:-1]
+        seq_len = attn_metadata.seq_start_loc[1:] - attn_metadata.seq_start_loc[:-1]
+        context_len = seq_len - query_len
+        if using_gqa_kernel:
+            # if using_gqa_kernel, then only the v1 kernel can be used
+            out = vllm._C.ops.context_attention_forward_v1(query[:num_actual_tokens], key_cache, value_cache, attn_metadata.block_table, attn_metadata.query_start_loc, seq_len, context_len, attn_metadata.max_seq_len, torch.amax(context_len).item())
+        elif value is None:
+            # Otherwise, by default use v2 attention forward kernel...
+            out = vllm._C.ops.context_attention_forward_v2(query[:num_actual_tokens], key_cache, value_cache, attn_metadata.block_table, attn_metadata.query_start_loc, seq_len, context_len, attn_metadata.max_seq_len, torch.amax(context_len).item(), torch.amax(query_len).item())
+        else:
+            out = vllm._C.ops.context_attention_forward_v1(query[:num_actual_tokens], key_cache, value_cache, attn_metadata.block_table, attn_metadata.query_start_loc, seq_len, context_len, attn_metadata.max_seq_len, torch.amax(context_len).item())
+
+        # output[:num_actual_tokens] = out
+        output[:num_actual_tokens] = out.view(out.shape[0], -1)
+
+
+
+@torch.library.custom_op("vllm::ipex_attn_chunked_prefill",
+                         mutates_args=["output", "kv_cache"])
+def ipex_attn_chunked_prefill(
+    output: torch.Tensor,
+    query: torch.Tensor,
+    key: torch.Tensor,
+    value: torch.Tensor,
+    num_heads: int,
+    head_size: int,
+    num_kv_heads: int,
+    kv_cache: torch.Tensor,
+    kv_cache_dtype: str,
+    k_scale: float,
+    v_scale: float,
+    scale: float,
+    sliding_window: Optional[List[int]] = None,
+    alibi_slopes: Optional[torch.Tensor] = None,
+    logits_soft_cap: Optional[float] = None,
+) -> None:
+    context = get_forward_context()
+    current_metadata = context.attn_metadata
+    if current_metadata is None:
+        # Profiling run.
+        return
+
+    assert current_metadata is not None
+    assert isinstance(current_metadata, FlashAttentionMetadata)
+    attn_metadata: FlashAttentionMetadata = current_metadata
+    num_actual_tokens = attn_metadata.num_actual_tokens
+
+    query = query.view(-1, num_heads, head_size)
+    key = key.view(-1, num_kv_heads, head_size)
+    value = value.view(-1, num_kv_heads, head_size)
+
+    # Reshape the input keys and values and store them in the cache.
+    key_cache = kv_cache[0]
+    value_cache = kv_cache[1]
+
+    ipex_ops.reshape_and_cache_flash(
+        key[:num_actual_tokens],
+        value[:num_actual_tokens],
+        key_cache,
+        value_cache,
+        attn_metadata.slot_mapping,
+        kv_cache_dtype,
+        k_scale,
+        v_scale,
+    )
+
+    ipex_ops.chunked_prefill(
+        query[:num_actual_tokens],
+        key_cache,
+        value_cache,
+        output[:num_actual_tokens],
+        attn_metadata.query_start_loc,
+        attn_metadata.seq_start_loc,
+        None,
+        attn_metadata.block_table,
+        alibi_slopes,
+        attn_metadata.max_query_len,
+        attn_metadata.max_seq_len,
+        0.0,
+        scale,
+        False,
+        True,
+        False,
+        None,
+    )
diff --git a/vllm/v1/engine/core.py b/vllm/v1/engine/core.py
index 39caca0c2..666e59eb3 100644
--- a/vllm/v1/engine/core.py
+++ b/vllm/v1/engine/core.py
@@ -345,6 +345,9 @@ class EngineCoreProc(EngineCore):
                         ready_pipe,
                         **kwargs):
         """Launch EngineCore busy loop in background process."""
+        from ipex_llm.vllm.xpu.model_convert import _ipex_llm_convert
+        lowbit = os.getenv("IPEX_LLM_LOWBIT", "sym_int4")
+        _ipex_llm_convert(lowbit)
 
         # Signal handler used for graceful termination.
         # SystemExit exception is only raised once to allow this and worker
diff --git a/vllm/v1/engine/llm_engine.py b/vllm/v1/engine/llm_engine.py
index 4c67186f7..daf8f539a 100644
--- a/vllm/v1/engine/llm_engine.py
+++ b/vllm/v1/engine/llm_engine.py
@@ -70,6 +70,10 @@ class LLMEngine:
         self.should_execute_dummy_batch = False
 
         # Tokenizer (+ ensure liveness if running in another process).
+        # Create tokenizer, which is needed...
+        # Q1: Is tokenizer needed in the critical path?...
+        # If detokenizer is only needed for user outputs.  Then tokenizer is on the critical path.
+        # User input needs to be tokenized for further processing...
         self.tokenizer = init_tokenizer_from_configs(
             model_config=vllm_config.model_config,
             scheduler_config=vllm_config.scheduler_config,
diff --git a/vllm/v1/executor/abstract.py b/vllm/v1/executor/abstract.py
index e3a4cd98c..79fd179b6 100644
--- a/vllm/v1/executor/abstract.py
+++ b/vllm/v1/executor/abstract.py
@@ -35,9 +35,15 @@ class Executor(ExecutorBase):
                     f"ExecutorBase. Got {distributed_executor_backend}.")
             executor_class = distributed_executor_backend
         elif distributed_executor_backend == "ray":
-            from vllm.v1.executor.ray_distributed_executor import (  # noqa
-                RayDistributedExecutor)
-            executor_class = RayDistributedExecutor
+            from vllm.platforms import current_platform
+            if current_platform.is_xpu():
+                from vllm.v1.executor.ray_distributed_executor import (  # noqa
+                    XPURayDistributedExecutor)
+                executor_class = XPURayDistributedExecutor
+            else:
+                from vllm.v1.executor.ray_distributed_executor import (  # noqa
+                    RayDistributedExecutor)
+                executor_class = RayDistributedExecutor
         elif distributed_executor_backend == "mp":
             from vllm.v1.executor.multiproc_executor import MultiprocExecutor
             executor_class = MultiprocExecutor
diff --git a/vllm/v1/executor/ray_distributed_executor.py b/vllm/v1/executor/ray_distributed_executor.py
index 320ebfd37..6ca3179af 100644
--- a/vllm/v1/executor/ray_distributed_executor.py
+++ b/vllm/v1/executor/ray_distributed_executor.py
@@ -59,3 +59,30 @@ class RayDistributedExecutor(RayDistributedExecutorV0, Executor):
         # When PP is used, we return a FutureWrapper immediately so that
         # the scheduler can yield to the next batch.
         return FutureWrapper(refs[0])
+
+class XPURayDistributedExecutor(RayDistributedExecutorV0, Executor):
+    """XPU Ray distributed executor without Compiled Graphs."""
+
+    def __init__(self, *args, **kwargs):
+        import os
+        lowbit = os.getenv("IPEX_LLM_LOWBIT", None)
+        if lowbit is not None:
+            from ipex_llm.vllm.xpu.model_convert import _ipex_llm_convert
+            _ipex_llm_convert(lowbit)
+        super().__init__(*args, **kwargs)
+
+
+    def execute_model(
+        self,
+        scheduler_output,
+    ) -> Union[ModelRunnerOutput, Future[ModelRunnerOutput]]:
+        output = self.collective_rpc("execute_model",
+                                     args=(scheduler_output, ))
+        return output[0]
+
+    @property
+    def max_concurrent_batches(self) -> int:
+        """Ray distributed executor supports pipeline parallelism,
+        meaning that it allows PP size batches to be executed concurrently.
+        """
+        return self.parallel_config.pipeline_parallel_size
diff --git a/vllm/v1/executor/ray_utils.py b/vllm/v1/executor/ray_utils.py
new file mode 100644
index 000000000..cce9315e8
--- /dev/null
+++ b/vllm/v1/executor/ray_utils.py
@@ -0,0 +1,272 @@
+import time
+from collections import defaultdict
+from typing import TYPE_CHECKING, Dict, List, Optional, Tuple
+
+from vllm.config import ParallelConfig
+from vllm.logger import init_logger
+from vllm.platforms import current_platform
+from vllm.utils import get_ip
+from vllm.v1.outputs import ModelRunnerOutput
+from vllm.worker.worker_base import WorkerWrapperBase
+
+if TYPE_CHECKING:
+    from vllm.v1.core.scheduler import SchedulerOutput
+
+logger = init_logger(__name__)
+PG_WAIT_TIMEOUT = 60
+
+try:
+    import ray
+    from ray.util import placement_group_table
+    from ray.util.placement_group import PlacementGroup
+    try:
+        from ray._private.state import available_resources_per_node
+    except ImportError:
+        # Ray 2.9.x doesn't expose `available_resources_per_node`
+        from ray._private.state import state as _state
+        available_resources_per_node = _state._available_resources_per_node
+
+    class RayWorkerWrapper(WorkerWrapperBase):
+
+        def __init__(self, *args, **kwargs) -> None:
+            super().__init__(*args, **kwargs)
+            # Since the compiled DAG runs a main execution
+            # in a different thread that calls cuda.set_device.
+            # The flag indicates is set_device is called on
+            # that thread. It will be removed soon.
+            self.compiled_dag_cuda_device_set = False
+
+        def get_node_ip(self) -> str:
+            return get_ip()
+
+        def get_node_and_gpu_ids(self) -> Tuple[str, List[int]]:
+            node_id = ray.get_runtime_context().get_node_id()
+            gpu_ids = ray.get_gpu_ids()
+            return node_id, gpu_ids
+
+        def setup_device_if_necessary(self):
+            # TODO(swang): This is needed right now because Ray CG executes
+            # on a background thread, so we need to reset torch's current
+            # device.
+            # We can remove this API after it is fixed in compiled graph.
+            import torch
+            assert self.worker is not None, "Worker is not initialized"
+            if not self.compiled_dag_cuda_device_set \
+                and current_platform.is_cuda():
+                torch.cuda.set_device(self.worker.device)
+                self.compiled_dag_cuda_device_set = True
+
+        def execute_model(
+            self,
+            scheduler_output: "SchedulerOutput",
+        ) -> ModelRunnerOutput:
+            self.setup_device_if_necessary()
+            assert self.worker is not None, "Worker is not initialized"
+            output = self.worker.model_runner.execute_model(scheduler_output)
+            return output
+
+    ray_import_err = None
+
+except ImportError as e:
+    ray = None  # type: ignore
+    ray_import_err = e
+    RayWorkerWrapper = None  # type: ignore
+
+
+def ray_is_available() -> bool:
+    """Returns True if Ray is available."""
+    return ray is not None
+
+
+def assert_ray_available():
+    """
+    Raise an exception if Ray is not available.
+    """
+    if ray is None:
+        raise ValueError("Failed to import Ray, please install Ray with "
+                         "`pip install ray`.") from ray_import_err
+
+
+def _verify_bundles(placement_group: "PlacementGroup",
+                    parallel_config: ParallelConfig, device_str: str):
+    """
+    Verify a given placement group has bundles located in the right place.
+
+    There are 2 rules.
+    - Warn if all tensor parallel workers cannot fit in a single node.
+    - Fail if driver node is not included in a placement group.
+
+    Args:
+        placement_group: The placement group to verify.
+        parallel_config: The parallel configuration.
+        device_str: The required device.
+    """
+    assert ray.is_initialized(), (
+        "Ray is not initialized although distributed-executor-backend is ray.")
+    pg_data = placement_group_table(placement_group)
+    # bundle_idx -> node_id
+    bundle_to_node_ids = pg_data["bundles_to_node_id"]
+    # bundle_idx -> bundle (e.g., {"GPU": 1})
+    bundles = pg_data["bundles"]
+    # node_id -> List of bundle (e.g., {"GPU": 1})
+    node_id_to_bundle: Dict[str, List[Dict[str, float]]] = defaultdict(list)
+
+    for bundle_idx, node_id in bundle_to_node_ids.items():
+        node_id_to_bundle[node_id].append(bundles[bundle_idx])
+    driver_node_id = ray.get_runtime_context().get_node_id()
+
+    if driver_node_id not in node_id_to_bundle:
+        raise RuntimeError(
+            f"driver node id {driver_node_id} is not included in a placement "
+            f"group {placement_group.id}. Node id -> bundles "
+            f"{node_id_to_bundle}. "
+            "You don't have enough GPUs available in a current node. Check "
+            "`ray status` to see if you have available GPUs in a node "
+            f"{driver_node_id} before starting an vLLM engine.")
+
+    for node_id, bundles in node_id_to_bundle.items():
+        if len(bundles) < parallel_config.tensor_parallel_size:
+            logger.warning(
+                "tensor_parallel_size=%d "
+                "is bigger than a reserved number of %ss (%d "
+                "%ss) in a node %s. Tensor parallel workers can be "
+                "spread out to 2+ nodes which can degrade the performance "
+                "unless you have fast interconnect across nodes, like "
+                "Infiniband. To resolve this issue, make sure you have more "
+                "than %d GPUs available at each node.",
+                parallel_config.tensor_parallel_size, device_str, len(bundles),
+                device_str, node_id, parallel_config.tensor_parallel_size)
+
+
+def _wait_until_pg_ready(current_placement_group: "PlacementGroup"):
+    """Wait until a placement group is ready.
+
+    It prints the informative log messages if the placement group is
+    not created within time.
+
+    """
+    # Wait until PG is ready - this will block until all
+    # requested resources are available, and will timeout
+    # if they cannot be provisioned.
+    placement_group_specs = current_placement_group.bundle_specs
+
+    s = time.time()
+    pg_ready_ref = current_placement_group.ready()
+    wait_interval = 10
+    while time.time() - s < PG_WAIT_TIMEOUT:
+        ready, _ = ray.wait([pg_ready_ref], timeout=wait_interval)
+        if len(ready) > 0:
+            break
+
+        # Exponential backoff for warning print.
+        wait_interval *= 2
+        logger.info(
+            "Waiting for creating a placement group of specs for "
+            "%d seconds. specs=%s. Check "
+            "`ray status` to see if you have enough resources.",
+            int(time.time() - s), placement_group_specs)
+
+    try:
+        ray.get(pg_ready_ref, timeout=0)
+    except ray.exceptions.GetTimeoutError:
+        raise ValueError(
+            "Cannot provide a placement group of "
+            f"{placement_group_specs=} within {PG_WAIT_TIMEOUT} seconds. See "
+            "`ray status` to make sure the cluster has enough resources."
+        ) from None
+
+
+def initialize_ray_cluster(
+    parallel_config: ParallelConfig,
+    ray_address: Optional[str] = None,
+):
+    """Initialize the distributed cluster with Ray.
+
+    it will connect to the Ray cluster and create a placement group
+    for the workers, which includes the specification of the resources
+    for each distributed worker.
+
+    Args:
+        parallel_config: The configurations for parallel execution.
+        ray_address: The address of the Ray cluster. If None, uses
+            the default Ray cluster address.
+    """
+    assert_ray_available()
+
+    # Connect to a ray cluster.
+    if current_platform.is_rocm() or current_platform.is_xpu():
+        # Try to connect existing ray instance and create a new one if not found
+        try:
+            ray.init("auto")
+        except ConnectionError:
+            logger.warning(
+                "No existing RAY instance detected. "
+                "A new instance will be launched with current node resources.")
+            ray.init(address=ray_address,
+                     ignore_reinit_error=True,
+                     num_gpus=parallel_config.world_size)
+    else:
+        ray.init(address=ray_address, ignore_reinit_error=True)
+
+    if parallel_config.placement_group:
+        # Placement group is already set.
+        return
+
+    device_str = "GPU" if not current_platform.is_tpu() else "TPU"
+    # Create placement group for worker processes
+    current_placement_group = ray.util.get_current_placement_group()
+    if current_placement_group:
+        # We are in a placement group
+        bundles = current_placement_group.bundle_specs
+        # Verify that we can use the placement group.
+        device_bundles = 0
+        for bundle in bundles:
+            bundle_devices = bundle.get(device_str, 0)
+            if bundle_devices > 1:
+                raise ValueError(
+                    "Placement group bundle cannot have more than 1 "
+                    f"{device_str}.")
+            if bundle_devices:
+                device_bundles += 1
+        if parallel_config.world_size > device_bundles:
+            raise ValueError(
+                f"The number of required {device_str}s exceeds the total "
+                f"number of available {device_str}s in the placement group."
+                f"Required number of devices: {parallel_config.world_size}. "
+                f"Total number of devices: {device_bundles}.")
+    else:
+        num_devices_in_cluster = ray.cluster_resources().get(device_str, 0)
+        if parallel_config.world_size > num_devices_in_cluster:
+            raise ValueError(
+                f"The number of required {device_str}s exceeds the total "
+                f"number of available {device_str}s in the placement group.")
+        # Create a new placement group
+        placement_group_specs: List[Dict[str, float]] = ([{
+            device_str: 1.0
+        } for _ in range(parallel_config.world_size)])
+
+        # vLLM engine is also a worker to execute model with an accelerator,
+        # so it requires to have the device in a current node. Check if
+        # the current node has at least one device.
+        current_ip = get_ip()
+        current_node_id = ray.get_runtime_context().get_node_id()
+        current_node_resource = available_resources_per_node()[current_node_id]
+        if current_node_resource.get(device_str, 0) < 1:
+            raise ValueError(
+                f"Current node has no {device_str} available. "
+                f"{current_node_resource=}. vLLM engine cannot start without "
+                f"{device_str}. Make sure you have at least 1 {device_str} "
+                f"available in a node {current_node_id=} {current_ip=}.")
+        # This way, at least bundle is required to be created in a current
+        # node.
+        placement_group_specs[0][f"node:{current_ip}"] = 0.001
+
+        # By default, Ray packs resources as much as possible.
+        current_placement_group = ray.util.placement_group(
+            placement_group_specs, strategy="PACK")
+        _wait_until_pg_ready(current_placement_group)
+
+    assert current_placement_group is not None
+    _verify_bundles(current_placement_group, parallel_config, device_str)
+    # Set the placement group in the parallel config
+    parallel_config.placement_group = current_placement_group
diff --git a/vllm/v1/executor/xpu_ray_executor.py b/vllm/v1/executor/xpu_ray_executor.py
new file mode 100644
index 000000000..ed3948331
--- /dev/null
+++ b/vllm/v1/executor/xpu_ray_executor.py
@@ -0,0 +1,31 @@
+from vllm.v1.executor.ray_executor import RayExecutor
+from vllm.v1.executor.ray_utils import ray
+from vllm.v1.outputs import ModelRunnerOutput
+
+
+class RayXPUExecutor(RayExecutor):
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self._device = "xpu"
+        self._dispatch_key = "XPU"
+
+    # FIXME: add XPU parameters.
+    # def _init_workers_ray(self, placement_group: "PlacementGroup",
+    #                      **ray_remote_kwargs):
+    #    pass
+
+    def execute_model(
+        self,
+        scheduler_output,
+    ) -> ModelRunnerOutput:
+        # FIXME: XPU do not support ray dag now.
+        # Only the first worker (with rank 0) returns the execution result.
+        # Others return None.
+        async_outputs = [
+            worker.execute_model.remote(  # type: ignore[attr-defined]
+                scheduler_output) for worker in self.workers
+        ]
+        outputs = [ray.get(output) for output in async_outputs]
+        output = outputs[0]
+        return output
diff --git a/vllm/v1/executor/xpu_uniproc_executor.py b/vllm/v1/executor/xpu_uniproc_executor.py
new file mode 100644
index 000000000..a55d6e96d
--- /dev/null
+++ b/vllm/v1/executor/xpu_uniproc_executor.py
@@ -0,0 +1,31 @@
+from typing import Optional
+
+from vllm.config import VllmConfig
+from vllm.logger import init_logger
+from vllm.utils import get_distributed_init_method, get_ip, get_open_port
+from vllm.v1.executor.uniproc_executor import UniprocExecutor
+from vllm.v1.worker.xpu_worker import XPUWorker
+
+logger = init_logger(__name__)
+
+
+class XPUUniprocExecutor(UniprocExecutor):
+
+    def __init__(self, vllm_config: VllmConfig) -> None:
+        super().__init__(vllm_config)
+
+    def _create_worker(
+            self,
+            local_rank: int = 0,
+            rank: int = 0,
+            distributed_init_method: Optional[str] = None) -> XPUWorker:
+        """Return worker init args for a given rank."""
+        if distributed_init_method is None:
+            distributed_init_method = get_distributed_init_method(
+                get_ip(), get_open_port())
+        return XPUWorker(
+            vllm_config=self.vllm_config,
+            local_rank=local_rank,
+            rank=rank,
+            distributed_init_method=distributed_init_method,
+        )
diff --git a/vllm/v1/worker/gpu_model_runner.py b/vllm/v1/worker/gpu_model_runner.py
index 5133c637f..1c6ed43cf 100644
--- a/vllm/v1/worker/gpu_model_runner.py
+++ b/vllm/v1/worker/gpu_model_runner.py
@@ -1255,6 +1255,7 @@ class GPUModelRunner(LoRAModelRunnerMixin):
         return draft_token_ids
 
     def load_model(self) -> None:
+        # We will need to replace the model loading position here...
         logger.info("Starting to load model %s...", self.model_config.model)
         with DeviceMemoryProfiler() as m:  # noqa: SIM117
             time_before_load = time.perf_counter()
diff --git a/vllm/v1/worker/xpu_model_runner.py b/vllm/v1/worker/xpu_model_runner.py
new file mode 100644
index 000000000..8612d3d77
--- /dev/null
+++ b/vllm/v1/worker/xpu_model_runner.py
@@ -0,0 +1,370 @@
+# SPDX-License-Identifier: Apache-2.0
+import gc
+from typing import TYPE_CHECKING
+
+import numpy as np
+import torch
+
+from vllm.config import CompilationLevel, VllmConfig
+from vllm.inputs import INPUT_REGISTRY
+from vllm.logger import init_logger
+from vllm.multimodal import MULTIMODAL_REGISTRY
+from vllm.utils import (STR_DTYPE_TO_TORCH_DTYPE, LayerBlockType, cdiv,
+                        is_pin_memory_available)
+from vllm.v1.attention.backends.ipex_attn import (IPEXAttentionBackend,
+                                                  IPEXAttentionMetadata)
+from vllm.v1.core.encoder_cache_manager import compute_encoder_budget
+from vllm.v1.kv_cache_interface import (AttentionSpec, FullAttentionSpec,
+                                        KVCacheConfig, KVCacheSpec,
+                                        SlidingWindowSpec)
+from vllm.v1.utils import bind_kv_cache
+from vllm.v1.worker.gpu_input_batch import CachedRequestState, InputBatch
+from vllm.v1.worker.gpu_model_runner import GPUModelRunner
+
+if TYPE_CHECKING:
+    from vllm.v1.core.scheduler import SchedulerOutput
+
+logger = init_logger(__name__)
+
+
+class XPUModelRunner(GPUModelRunner):
+    """A model runner for XPU devices."""
+
+    def __init__(
+        self,
+        vllm_config: VllmConfig,
+        device: torch.device,
+    ):
+        self.vllm_config = vllm_config
+        self.model_config = vllm_config.model_config
+        self.cache_config = vllm_config.cache_config
+        self.lora_config = vllm_config.lora_config
+        self.load_config = vllm_config.load_config
+        self.parallel_config = vllm_config.parallel_config
+        self.scheduler_config = vllm_config.scheduler_config
+        self.speculative_config = vllm_config.speculative_config
+        self.prompt_adapter_config = vllm_config.prompt_adapter_config
+        self.observability_config = vllm_config.observability_config
+
+        model_config = self.model_config
+        cache_config = self.cache_config
+        scheduler_config = self.scheduler_config
+        parallel_config = self.parallel_config
+        self.device = device
+        self.pin_memory = is_pin_memory_available()
+        self.dtype = self.model_config.dtype
+        if cache_config.cache_dtype == "auto":
+            self.kv_cache_dtype = self.dtype
+        else:
+            self.kv_cache_dtype = STR_DTYPE_TO_TORCH_DTYPE[
+                cache_config.cache_dtype]
+
+        self.is_multimodal_model = model_config.is_multimodal_model
+        self.sliding_window = model_config.get_sliding_window()
+        self.block_size = cache_config.block_size
+        self.max_model_len = model_config.max_model_len
+        self.max_num_blocks_per_req = cdiv(self.max_model_len, self.block_size)
+        self.max_num_tokens = scheduler_config.max_num_batched_tokens
+        self.max_num_reqs = scheduler_config.max_num_seqs
+
+        # Model-related.
+        self.num_attn_layers = model_config.get_num_layers_by_block_type(
+            parallel_config, LayerBlockType.attention)
+        self.num_query_heads = model_config.get_num_attention_heads(
+            parallel_config)
+        self.num_kv_heads = model_config.get_num_kv_heads(parallel_config)
+        self.head_size = model_config.get_head_size()
+        self.hidden_size = model_config.get_hidden_size()
+
+        # Multi-modal data support
+        self.input_registry = INPUT_REGISTRY
+        self.mm_registry = MULTIMODAL_REGISTRY
+        # FIXME: support mrope
+        self.uses_mrope = False
+
+        encoder_compute_budget, encoder_cache_size = compute_encoder_budget(
+            model_config=model_config,
+            scheduler_config=scheduler_config,
+            mm_registry=self.mm_registry,
+        )
+        self.max_num_encoder_input_tokens = encoder_compute_budget
+        self.encoder_cache_size = encoder_cache_size
+
+        # Lazy initialization
+        # self.model: nn.Module  # Set after load_model
+        self.kv_caches: list[torch.Tensor] = []
+        # req_id -> (input_id -> encoder_output)
+        self.encoder_cache: dict[str, dict[int, torch.Tensor]] = {}
+        self.use_spec_decode = False
+        # Request states.
+        self.requests: dict[str, CachedRequestState] = {}
+        # Persistent batch.
+        self.input_batch = InputBatch(
+            max_num_reqs=self.max_num_reqs,
+            max_model_len=self.max_model_len,
+            max_num_blocks_per_req=self.max_num_blocks_per_req,
+            device=self.device,
+            pin_memory=self.pin_memory,
+            vocab_size=model_config.get_vocab_size(),
+        )
+
+        self.use_cuda_graph = (self.vllm_config.compilation_config.level
+                               == CompilationLevel.PIECEWISE
+                               and not self.model_config.enforce_eager)
+        # TODO(woosuk): Provide an option to tune the max cudagraph batch size.
+        # The convention is different.
+        # self.cudagraph_batch_sizes sorts in ascending order.
+        # The batch sizes in the config are in descending order.
+        self.cudagraph_batch_sizes = list(
+            reversed(
+                self.vllm_config.compilation_config.cudagraph_capture_sizes))
+
+        # Persistent buffers for CUDA graphs.
+        self.input_ids = torch.zeros(self.max_num_tokens,
+                                     dtype=torch.int32,
+                                     device=self.device)
+        self.positions = torch.zeros(self.max_num_tokens,
+                                     dtype=torch.int64,
+                                     device=self.device)
+        self.inputs_embeds = torch.zeros(
+            (self.max_num_tokens, self.hidden_size),
+            dtype=self.dtype,
+            device=self.device)
+
+        # OPTIMIZATION: Cache the tensors rather than creating them every step.
+        self.arange_np = np.arange(max(self.max_num_reqs + 1,
+                                       self.max_model_len),
+                                   dtype=np.int32)
+        # NOTE(woosuk): These tensors are "stateless", i.e., they are literally
+        # a faster version of creating a new tensor every time. Thus, we should
+        # not make any assumptions about the values in these tensors.
+        self.input_ids_cpu = torch.zeros(self.max_num_tokens,
+                                         dtype=torch.int32,
+                                         device="cpu",
+                                         pin_memory=self.pin_memory)
+        self.input_ids_np = self.input_ids_cpu.numpy()
+        self.positions_cpu = torch.zeros(self.max_num_tokens,
+                                         dtype=torch.int64,
+                                         device="cpu",
+                                         pin_memory=self.pin_memory)
+        self.positions_np = self.positions_cpu.numpy()
+        self.slot_mapping_cpu = torch.zeros(self.max_num_tokens,
+                                            dtype=torch.int32,
+                                            device="cpu",
+                                            pin_memory=self.pin_memory)
+        self.slot_mapping_np = self.slot_mapping_cpu.numpy()
+        self.query_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+                                               dtype=torch.int32,
+                                               device="cpu",
+                                               pin_memory=self.pin_memory)
+        self.query_start_loc_np = self.query_start_loc_cpu.numpy()
+        self.seq_start_loc_cpu = torch.zeros(self.max_num_reqs + 1,
+                                             dtype=torch.int32,
+                                             device="cpu",
+                                             pin_memory=self.pin_memory)
+        self.seq_start_loc_np = self.seq_start_loc_cpu.numpy()
+        self.seq_lens_cpu = torch.zeros(self.max_num_reqs,
+                                        dtype=torch.int32,
+                                        device="cpu",
+                                        pin_memory=self.pin_memory)
+        self.seq_lens_np = self.seq_lens_cpu.numpy()
+
+    def _prepare_inputs(self, scheduler_output: "SchedulerOutput"):
+        total_num_scheduled_tokens = scheduler_output.total_num_scheduled_tokens
+        assert total_num_scheduled_tokens > 0
+        num_reqs = self.input_batch.num_reqs
+        assert num_reqs > 0
+
+        # OPTIMIZATION: Start copying the block table first.
+        # This way, we can overlap the copy with the following CPU operations.
+        self.input_batch.block_table.commit(num_reqs)
+
+        # Get the number of scheduled tokens for each request.
+        # TODO: The Python loop can be slow. Optimize.
+        num_scheduled_tokens = []
+        max_num_scheduled_tokens = 0
+        for req_id in self.input_batch.req_ids[:num_reqs]:
+            assert req_id is not None
+            num_tokens = scheduler_output.num_scheduled_tokens[req_id]
+            num_scheduled_tokens.append(num_tokens)
+            max_num_scheduled_tokens = max(max_num_scheduled_tokens,
+                                           num_tokens)
+        num_scheduled_tokens = np.array(num_scheduled_tokens, dtype=np.int32)
+        assert max_num_scheduled_tokens > 0
+
+        # Get request indices.
+        # E.g., [2, 5, 3] -> [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]
+        req_indices = np.repeat(self.arange_np[:num_reqs],
+                                num_scheduled_tokens)
+
+        # Get batched arange.
+        # E.g., [2, 5, 3] -> [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]
+        arange = np.concatenate(
+            [self.arange_np[:n] for n in num_scheduled_tokens])
+
+        # Get positions.
+        positions_np = self.positions_np[:total_num_scheduled_tokens]
+        np.add(self.input_batch.num_computed_tokens_cpu[req_indices],
+               arange,
+               out=positions_np)
+
+        # Get token indices.
+        # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]
+        # -> [0, 1, M, M + 1, M + 2, M + 3, M + 4, 2 * M, 2 * M + 1, 2 * M + 2]
+        # where M is the max_model_len.
+        token_indices = (positions_np +
+                         req_indices * self.input_batch.token_ids_cpu.shape[1])
+        # NOTE(woosuk): We use torch.index_select instead of np.take here
+        # because torch.index_select is much faster than np.take for large
+        # tensors.
+        torch.index_select(self.input_batch.token_ids_cpu_tensor.flatten(),
+                           0,
+                           torch.from_numpy(token_indices),
+                           out=self.input_ids_cpu[:total_num_scheduled_tokens])
+
+        # Calculate the slot mapping.
+        # E.g., [0, 1, 0, 1, 2, 3, 4, 0, 1, 2]
+        # -> [0, 0, K, K, K + 1, K + 1, K + 2, 2 * K, 2 * K, 2 * K + 1]
+        # where K is the max_num_blocks_per_req and the block size is 2.
+        # NOTE(woosuk): We can't simply use `token_indices // block_size` here
+        # because M (max_model_len) is not necessarily divisible by block_size.
+        block_table_indices = (req_indices * self.max_num_blocks_per_req +
+                               positions_np // self.block_size)
+        # NOTE(woosuk): We use torch.index_select instead of np.take here
+        # because torch.index_select is much faster than np.take for large
+        # tensors.
+        block_table_cpu = self.input_batch.block_table.get_cpu_tensor()
+        block_numbers = block_table_cpu.flatten()[block_table_indices].numpy()
+        block_offsets = positions_np % self.block_size
+        np.add(block_numbers * self.block_size,
+               block_offsets,
+               out=self.slot_mapping_np[:total_num_scheduled_tokens])
+
+        # Prepare the attention metadata.
+        self.query_start_loc_np[0] = 0
+        np.cumsum(num_scheduled_tokens,
+                  out=self.query_start_loc_np[1:num_reqs + 1])
+
+        seq_lens = (self.input_batch.num_computed_tokens_cpu[:num_reqs] +
+                    num_scheduled_tokens)
+        max_seq_len = seq_lens.max()
+        self.seq_start_loc_np[0] = 0
+        np.cumsum(seq_lens, out=self.seq_start_loc_np[1:num_reqs + 1])
+
+        self.seq_lens_np[:num_reqs] = (
+            self.input_batch.num_computed_tokens_cpu[:num_reqs] +
+            num_scheduled_tokens)
+        # max_seq_len = self.seq_lens_np[:num_reqs].max()
+
+        # Copy the tensors to the GPU.
+        self.input_ids[:total_num_scheduled_tokens].copy_(
+            self.input_ids_cpu[:total_num_scheduled_tokens], non_blocking=True)
+        self.positions[:total_num_scheduled_tokens].copy_(
+            self.positions_cpu[:total_num_scheduled_tokens], non_blocking=True)
+        query_start_loc = self.query_start_loc_cpu[:num_reqs + 1].to(
+            self.device, non_blocking=True)
+        seq_start_loc = self.seq_start_loc_cpu[:num_reqs + 1].to(
+            self.device, non_blocking=True)
+        seq_lens = self.seq_lens_cpu[:num_reqs].to(self.device,
+                                                   non_blocking=True)
+        slot_mapping = self.slot_mapping_cpu[:total_num_scheduled_tokens].to(
+            self.device, non_blocking=True).long()
+
+        # TODO: enable cascade attention in the future.
+        common_prefix_len = 0
+        use_cascade = False
+
+        if use_cascade:
+            # TODO: Optimize.
+            cu_prefix_query_lens = torch.tensor(
+                [0, total_num_scheduled_tokens],
+                dtype=torch.int32,
+                device=self.device)
+            prefix_kv_lens = torch.tensor([common_prefix_len],
+                                          dtype=torch.int32,
+                                          device=self.device)
+            suffix_kv_lens = (self.seq_lens_np[:num_reqs] - common_prefix_len)
+            suffix_kv_lens = torch.from_numpy(suffix_kv_lens).to(self.device)
+        else:
+            cu_prefix_query_lens = None
+            prefix_kv_lens = None
+            suffix_kv_lens = None
+
+        attn_metadata = IPEXAttentionMetadata(
+            num_actual_tokens=total_num_scheduled_tokens,
+            max_query_len=max_num_scheduled_tokens,
+            query_start_loc=query_start_loc,
+            max_seq_len=max_seq_len,
+            seq_start_loc=seq_start_loc,
+            seq_lens=torch.empty(0, dtype=torch.int32, device=self.device),
+            block_table=(
+                self.input_batch.block_table.get_device_tensor()[:num_reqs]),
+            slot_mapping=slot_mapping,
+            use_cascade=use_cascade,
+            common_prefix_len=common_prefix_len,
+            cu_prefix_query_lens=cu_prefix_query_lens,
+            prefix_kv_lens=prefix_kv_lens,
+            suffix_kv_lens=suffix_kv_lens,
+        )
+        # NOTE(woosuk): Due to chunked prefills, there can be at most 1 partial
+        # request in the batch. While we should not sample any token from this
+        # partial request, we do so for simplicity. We will ignore the sampled
+        # token from the partial request.
+        # TODO: Support prompt logprobs.
+        logits_indices = query_start_loc[1:] - 1
+        spec_decode_metadata = None
+        return attn_metadata, logits_indices, spec_decode_metadata
+
+    def profile_run(self) -> None:
+        # Trigger compilation for general shape.
+        hidden_states = self._dummy_run(self.max_num_tokens)
+        logits = self.model.compute_logits(hidden_states, None)
+        logits = logits[:self.max_num_tokens]
+        torch.xpu.synchronize()
+        gc.collect()
+
+    def initialize_kv_cache(self, kv_cache_config: KVCacheConfig) -> None:
+        """
+        Initialize KV cache based on `kv_cache_config`.
+        Args:
+            kv_cache_config: Configuration for the KV cache, including the KV
+            cache size of each layer
+        """
+        if len(kv_cache_config.kv_cache_groups) > 1:
+            raise NotImplementedError(
+                "Hybrid models with more than one KV cache type are not "
+                "supported yet.")
+
+        kv_caches: dict[str, torch.Tensor] = {}
+
+        for kv_cache_group in kv_cache_config.kv_cache_groups:
+            kv_cache_spec = kv_cache_group.kv_cache_spec
+            for layer_name in kv_cache_group.layer_names:
+                tensor_config = kv_cache_config.tensors[layer_name]
+                assert tensor_config.size % kv_cache_spec.page_size_bytes == 0
+                num_blocks = tensor_config.size // kv_cache_spec.page_size_bytes
+                # `num_blocks` is the number of blocks the model runner can use.
+                # `kv_cache_config.num_blocks` is the number of blocks that
+                # KVCacheManager may allocate.
+                # Since different GPUs may have different number of layers and
+                # different memory capacities, `num_blocks` can be different on
+                # different GPUs, and `kv_cache_config.num_blocks` is set to
+                # the min of all `num_blocks`. Verify it here.
+                assert num_blocks >= kv_cache_config.num_blocks
+                if isinstance(kv_cache_spec, AttentionSpec):
+                    kv_cache_shape = IPEXAttentionBackend.get_kv_cache_shape(
+                    num_blocks, kv_cache_spec.block_size, kv_cache_spec.num_kv_heads,
+                    kv_cache_spec.head_size)
+                    dtype = kv_cache_spec.dtype
+                    kv_caches[layer_name] = torch.zeros(kv_cache_shape,
+                                                        dtype=dtype,
+                                                        device=self.device)
+                else:
+                    # TODO: add new branches when introducing more types of
+                    # KV cache specs.
+                    raise ValueError("Unknown KV cache spec type.")
+
+        bind_kv_cache(
+            kv_caches,
+            self.vllm_config.compilation_config.static_forward_context,
+            self.kv_caches)
diff --git a/vllm/v1/worker/xpu_worker.py b/vllm/v1/worker/xpu_worker.py
new file mode 100644
index 000000000..1fb0dca87
--- /dev/null
+++ b/vllm/v1/worker/xpu_worker.py
@@ -0,0 +1,175 @@
+# SPDX-License-Identifier: Apache-2.0
+import os
+from typing import Optional
+
+import torch
+import torch.distributed
+
+from vllm.config import ParallelConfig, VllmConfig
+from vllm.distributed import (ensure_model_parallel_initialized,
+                              init_distributed_environment)
+from vllm.model_executor import set_random_seed
+from vllm.platforms import current_platform
+from vllm.v1.worker.gpu_worker import Worker
+from vllm.v1.worker.xpu_model_runner import XPUModelRunner
+
+
+class XPUWorker(Worker):
+    """A XPU worker class."""
+
+    def __init__(
+        self,
+        vllm_config: VllmConfig,
+        local_rank: int,
+        rank: int,
+        distributed_init_method: str,
+        is_driver_worker: bool = False,
+    ):
+        super().__init__(vllm_config, local_rank, rank,
+                         distributed_init_method, is_driver_worker)
+        device_config = self.device_config
+        assert device_config.device_type == "xpu"
+        assert current_platform.is_xpu()
+
+        import os
+        lowbit = os.getenv("IPEX_LLM_LOWBIT", None)
+        if lowbit is not None:
+            from ipex_llm.vllm.xpu.model_convert import _ipex_llm_convert
+            _ipex_llm_convert(lowbit)
+
+
+    def compile_or_warm_up_model(self) -> None:
+        pass
+        
+    # we provide this function due to `torch.xpu.mem_get_info()` doesn't
+    # return correct free_gpu_memory on intel client GPU. We need to
+    # calculate/estiamte it.
+    def xpu_get_mem_info(self):
+        if current_platform.is_data_center_gpu():
+            return torch.xpu.mem_get_info()
+        else:
+            _, total_gpu_memory = torch.xpu.mem_get_info()
+            # FIXME: memory_allocated() doesn't count non-torch allocations,
+            # and we don't have any API to get it. so we mark it as 128MB.
+            used_memory = torch.xpu.memory_allocated()
+            non_torch_allocations = 128 * 1024 * 1024
+            free_gpu_memory = total_gpu_memory - (used_memory +
+                                                  non_torch_allocations)
+            return free_gpu_memory, total_gpu_memory
+
+    @torch.inference_mode()
+    def determine_available_memory(self) -> int:
+        """Profiles the peak memory usage of the model to determine how many
+        KV blocks may be allocated without OOMs.
+        The engine will first conduct a profiling of the existing memory usage.
+        Then, it calculate the maximum possible number of GPU and CPU blocks
+        that can be allocated with the remaining free memory.
+        .. tip::
+            You may limit the usage of GPU memory
+            by adjusting the `gpu_memory_utilization` parameter.
+        """
+        # Profile the memory usage of the model and get the maximum number of
+        # cache blocks that can be allocated with the remaining free memory.
+        torch.xpu.empty_cache()
+        torch.xpu.reset_peak_memory_stats()
+
+        # Note(Xiangyu): torch.xpu.mem_get_info() is missing with torch 2.6
+        # _, total_gpu_memory = torch.xpu.mem_get_info()
+        # Execute a forward pass with dummy inputs to profile the memory usage
+        # of the model.
+        self.model_runner.profile_run()
+
+        torch.xpu.synchronize()
+        used_memory = torch.xpu.memory_allocated()
+        total_gpu_memory = torch.xpu.get_device_properties(
+            self.local_rank).total_memory
+        free_gpu_memory = total_gpu_memory - used_memory
+
+        # free_gpu_memory, _ = self.xpu_get_mem_info()
+        # NOTE(woosuk): Here we assume that the other processes using the same
+        # GPU did not change their memory usage during the profiling.
+        peak_memory = self.init_gpu_memory - free_gpu_memory
+        assert self.init_gpu_memory > free_gpu_memory, (
+            "Error in memory profiling. "
+            f"Initial free memory {self.init_gpu_memory}, current free memory"
+            f" {free_gpu_memory}. This happens when the GPU memory was "
+            "not properly cleaned up before initializing the vLLM instance.")
+
+        # # Get the peak memory allocation recorded by torch
+        # peak_memory = torch.xpu.memory_stats()["allocated_bytes.all.peak"]
+
+        torch.xpu.empty_cache()
+        # torch_allocated_bytes = torch.xpu.memory_stats(
+        # )["allocated_bytes.all.current"]
+        # total_allocated_bytes = self.xpu_get_mem_info(
+        # )[1] - self.xpu_get_mem_info()[0]
+
+        # non_torch_allocations = total_allocated_bytes - torch_allocated_bytes
+        # if non_torch_allocations > 0:
+        #     peak_memory += non_torch_allocations
+        available_kv_cache_memory = (
+            total_gpu_memory * self.cache_config.gpu_memory_utilization -
+            peak_memory) - 128 * 1024 * 1024
+
+        return int(available_kv_cache_memory)
+
+    def init_device(self):
+        if self.device_config.device.type == "xpu" and current_platform.is_xpu(
+        ):
+            self.device = torch.device(f"xpu:{self.local_rank}")
+            torch.xpu.set_device(self.device)
+            torch.xpu.empty_cache()
+            self.init_gpu_memory = torch.xpu.get_device_properties(
+                self.local_rank).total_memory
+        else:
+            raise RuntimeError(
+                f"Not support device type: {self.device_config.device}")
+        init_worker_distributed_environment(self.parallel_config, self.rank,
+                                            self.distributed_init_method,
+                                            self.local_rank)
+        # Set random seed.
+        set_random_seed(self.model_config.seed)
+        self.model_runner = XPUModelRunner(  # type: ignore
+            self.vllm_config, self.device)
+
+
+def init_worker_distributed_environment(
+    parallel_config: ParallelConfig,
+    rank: int,
+    distributed_init_method: Optional[str] = None,
+    local_rank: int = -1,
+) -> None:
+    """Initialize the distributed environment."""
+
+    if torch.distributed.is_initialized():
+        torch_world_size = torch.distributed.get_world_size()
+        if torch_world_size != parallel_config.world_size:
+            raise RuntimeError(
+                "torch.distributed is already initialized but the torch "
+                "world size does not match parallel_config.world_size "
+                f"({torch_world_size} vs. {parallel_config.world_size}).")
+    elif not distributed_init_method:
+        raise ValueError(
+            "distributed_init_method must be set if torch.distributed "
+            "is not already initialized")
+    else:
+        # oneapi 2025 will use pidfd as default
+        ENV_CCL_ZE_IPC_EXCHANGE = os.getenv("CCL_ZE_IPC_EXCHANGE", "drmfd")
+        ENV_CCL_ATL_TRANSPORT = os.getenv("CCL_ATL_TRANSPORT", "ofi")
+        ENV_LOCAL_WORLD_SIZE = os.getenv("LOCAL_WORLD_SIZE",
+                                         str(parallel_config.world_size))
+        os.environ["CCL_ZE_IPC_EXCHANGE"] = ENV_CCL_ZE_IPC_EXCHANGE
+        os.environ["CCL_ATL_TRANSPORT"] = ENV_CCL_ATL_TRANSPORT
+        os.environ["LOCAL_WORLD_SIZE"] = ENV_LOCAL_WORLD_SIZE
+        os.environ["LOCAL_RANK"] = str(local_rank)
+        init_distributed_environment(
+            world_size=parallel_config.world_size,
+            rank=rank,
+            distributed_init_method=distributed_init_method,
+            local_rank=local_rank,
+            backend="ccl")
+
+    ensure_model_parallel_initialized(parallel_config.tensor_parallel_size,
+                                      parallel_config.pipeline_parallel_size)
+    # global all_reduce needed for overall oneccl warm up
+    # torch.distributed.all_reduce(torch.zeros(1).xpu())
diff --git a/vllm/worker/model_runner.py b/vllm/worker/model_runner.py
index 86e6d9752..ad80bf54e 100644
--- a/vllm/worker/model_runner.py
+++ b/vllm/worker/model_runner.py
@@ -70,9 +70,9 @@ _NUM_WARMUP_ITERS = 2
 
 TModelInputForGPU = TypeVar('TModelInputForGPU', bound="ModelInputForGPU")
 
-# For now, bump up cache limits for recompilations during CUDA graph warmups.
-torch._dynamo.config.cache_size_limit = 128
-torch._dynamo.config.accumulated_cache_size_limit = 128
+# # For now, bump up cache limits for recompilations during CUDA graph warmups.
+# torch._dynamo.config.cache_size_limit = 128
+# torch._dynamo.config.accumulated_cache_size_limit = 128
 
 
 @dataclass(frozen=True)
diff --git a/vllm/worker/xpu_model_runner.py b/vllm/worker/xpu_model_runner.py
index 9d49b4385..7396b0c89 100644
--- a/vllm/worker/xpu_model_runner.py
+++ b/vllm/worker/xpu_model_runner.py
@@ -5,8 +5,8 @@ import time
 import weakref
 from collections import defaultdict
 from dataclasses import dataclass
-from typing import (TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple,
-                    Type, TypeVar)
+from typing import (TYPE_CHECKING, Any, Callable, Dict, List, Optional, Set,
+                    Tuple, Type, TypeVar)
 
 import torch
 import torch.nn as nn
@@ -17,9 +17,15 @@ from vllm.distributed import get_pp_group
 from vllm.forward_context import set_forward_context
 from vllm.inputs import INPUT_REGISTRY, InputRegistry
 from vllm.logger import init_logger
+from vllm.lora.layers import LoRAMapping
+from vllm.lora.request import LoRARequest
+from vllm.lora.worker_manager import LRUCacheWorkerLoRAManager
 from vllm.model_executor import SamplingMetadataCache
 from vllm.model_executor.layers.sampler import SamplerOutput
 from vllm.model_executor.model_loader import get_model
+from vllm.model_executor.model_loader.tensorizer import TensorizerConfig
+from vllm.model_executor.models.interfaces import (supports_lora,
+                                                   supports_multimodal)
 from vllm.multimodal import (MULTIMODAL_REGISTRY, BatchedTensorInputs,
                              MultiModalKwargs, MultiModalPlaceholderMap,
                              MultiModalRegistry)
@@ -27,19 +33,25 @@ from vllm.sampling_params import SamplingParams
 from vllm.sequence import IntermediateTensors, SequenceGroupMetadata
 from vllm.utils import DeviceMemoryProfiler, GiB_bytes, make_tensor_with_pad
 from vllm.worker.model_runner import AttentionMetadata, SamplingMetadata
+from vllm.model_executor.layers.rotary_embedding import MRotaryEmbedding
 from vllm.worker.model_runner_base import (
     ModelRunnerBase, ModelRunnerInputBase, ModelRunnerInputBuilderBase,
     _add_attn_metadata_broadcastable_dict,
     _add_sampling_metadata_broadcastable_dict,
     _init_attn_metadata_from_tensor_dict,
     _init_sampling_metadata_from_tensor_dict)
-
+from vllm.attention.backends.utils import is_block_tables_empty
 if TYPE_CHECKING:
     from vllm.attention.backends.abstract import AttentionBackend
 
 logger = init_logger(__name__)
 
 _PAD_SLOT_ID = -1
+LORA_WARMUP_RANK = 8
+_BATCH_SIZE_ALIGNMENT = 8
+_BATCH_SIZES_TO_CAPTURE = [1, 2, 4] + [
+    _BATCH_SIZE_ALIGNMENT * i for i in range(1, 33)
+]
 
 TModelInputForXPU = TypeVar('TModelInputForXPU', bound="ModelInputForXPU")
 
@@ -51,6 +63,8 @@ class ModelInputForXPU(ModelRunnerInputBase):
     """
     input_tokens: Optional[torch.Tensor] = None
     input_positions: Optional[torch.Tensor] = None
+    lora_mapping: Optional["LoRAMapping"] = None
+    lora_requests: Optional[Set[LoRARequest]] = None
     attn_metadata: Optional["AttentionMetadata"] = None
     multi_modal_kwargs: Optional[BatchedTensorInputs] = None
     virtual_engine: Optional[int] = None
@@ -62,6 +76,9 @@ class ModelInputForXPU(ModelRunnerInputBase):
         tensor_dict = {
             "input_tokens": self.input_tokens,
             "input_positions": self.input_positions,
+            "multi_modal_kwargs": self.multi_modal_kwargs,
+            "lora_requests": self.lora_requests,
+            "lora_mapping": self.lora_mapping,
         }
         _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)
 
@@ -90,6 +107,9 @@ class ModelInputForXPUWithSamplingMetadata(ModelInputForXPU):
         tensor_dict = {
             "input_tokens": self.input_tokens,
             "input_positions": self.input_positions,
+            "multi_modal_kwargs": self.multi_modal_kwargs,
+            "lora_requests": self.lora_requests,
+            "lora_mapping": self.lora_mapping,
         }
         _add_attn_metadata_broadcastable_dict(tensor_dict, self.attn_metadata)
         _add_sampling_metadata_broadcastable_dict(tensor_dict,
@@ -112,7 +132,7 @@ class ModelInputForXPUWithSamplingMetadata(ModelInputForXPU):
 class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
 
     def __init__(self,
-                 runner: "XPUModelRunner",
+                 runner: "XPUModelRunnerBase",
                  finished_requests_ids: Optional[List[str]] = None) -> None:
         super().__init__()
         self.runner = runner
@@ -121,6 +141,10 @@ class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
         self.sliding_window = self.runner.sliding_window
         self.block_size = self.runner.block_size
         self.device = self.runner.device
+        self.scheduler_config = self.runner.scheduler_config
+        self.cache_config = self.runner.cache_config
+        # Multi-modal data support
+        self.multi_modal_input_mapper = self.runner.multi_modal_input_mapper
 
     def prepare(self,
                 finished_requests_ids: Optional[List[str]] = None) -> None:
@@ -130,33 +154,275 @@ class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
         self.seq_group_metadata_list.append(seq_group_metadata)
 
     def build(self) -> ModelInputForXPU:
-        is_prompt = self.seq_group_metadata_list[0].is_prompt
-        # Prepare input tensors.
-        if is_prompt:
-            (input_tokens, input_positions, attn_metadata, seq_lens,
-             multi_modal_kwargs) = self._prepare_prompt(
-                 self.seq_group_metadata_list)
+        input_tokens: List[int] = []
+        input_positions: List[int] = []
+        input_mrope_positions: List[List[int]] = [[] for _ in range(3)]
+        slot_mapping: List[int] = []
+
+        seq_lens: List[int] = []
+        # Prefill's seq_len: query_length + chunked_prefill length
+        prefill_seq_lens: List[int] = []
+        decode_seq_lens: List[int] = []
+        context_lens: List[int] = []
+        query_lens: List[int] = []
+        # One for each sequence, physical blocks
+        block_tables: List[List[int]] = []
+        multi_modal_kwargs_list: List[MultiModalKwargs] = []
+
+        num_prefills = 0
+        num_prefill_tokens = 0
+        num_decode_tokens = 0
+        mrope_input_positions = None
+
+        if len(self.seq_group_metadata_list) == 0:
+            return None
+
+        for seq_group_metadata in self.seq_group_metadata_list:
+            seq_ids = list(seq_group_metadata.seq_data.keys())
+            # is_prompt indicates that it is still in prompt states
+            # TODO: remove this is_prompt
+            is_prompt = seq_group_metadata.is_prompt
+
+            # Iterate over all the seqs in the seq_group
+            for seq_id in seq_ids:
+                # Check for prefix caching
+                computed_block_nums = seq_group_metadata.computed_block_nums
+                if (self.scheduler_config is not None
+                    and self.scheduler_config.chunked_prefill_enabled
+                    and not (computed_block_nums is None or computed_block_nums == [])):
+                    raise RuntimeError("chunked prefill cannot be used with prefix caching")
+                seq_data = seq_group_metadata.seq_data[seq_id]
+                # Context_len: how many tokens that have been computed
+                if is_prompt:
+                    if self.scheduler_config.chunked_prefill_enabled:
+                        context_len = seq_data.get_num_computed_tokens()
+                    elif computed_block_nums is not None:
+                        context_len = len(computed_block_nums) * self.block_size
+                    else:
+                        context_len = 0
+                else:
+                    context_len = seq_data.get_len() - 1
+
+                # Get tokens for this sequence
+                # For prefill, the seq_len will be the second one.
+                # For decoding, the seq_len will be the first one.
+                seq_len = min(seq_data.get_len(), context_len + seq_group_metadata.token_chunk_size)
+
+                if is_prompt:
+                    if context_len == seq_len and self.cache_config.enable_prefix_caching:
+                        context_len = 0
+                    tokens = seq_data.get_token_ids()[context_len: seq_len]
+                else:
+                    # Last token
+                    tokens = [seq_data.get_last_token_id()]
+
+                if (computed_block_nums is not None or self.scheduler_config.chunked_prefill_enabled or not is_prompt):
+                    # Chunked prefill or decoding
+                    # For chunked prefill, the block tables may not be None
+                    if seq_group_metadata.block_tables is not None:
+                        block_table = seq_group_metadata.block_tables[seq_id]
+                        if context_len == 0:
+                            block_table = []
+                    else:
+                        block_table = []
+                else:
+                    # Prefill without chunked prefill
+                    block_table = []
+                if self.sliding_window is not None:
+                    sliding_window_blocks = (self.sliding_window //
+                                             self.block_size)
+                    block_table = block_table[-sliding_window_blocks:]
+                block_tables.append(block_table)
+                # Total seq_lens
+                seq_len = seq_len if self.sliding_window is None else min(
+                    seq_len, self.sliding_window)
+                seq_lens.append(seq_len)
+                input_tokens.extend(tokens)
+                query_len = seq_len - context_len
+                query_lens.append(query_len)
+                if is_prompt or self.scheduler_config.chunked_prefill_enabled:
+                    context_lens.append(context_len)
+                    input_positions.extend(list(range(context_len, seq_len)))
+                else:
+                    context_lens = seq_lens
+                    #query_lens = seq_lens
+                    position = seq_len - 1
+                    input_positions.append(position)
+                if is_prompt:
+                    mm_data = seq_group_metadata.multi_modal_data
+                    if mm_data and not self.runner.model_is_mrope and not self.runner.mm_registry.has_processor(self.runner.model_config):
+                        mm_kwargs = self.multi_modal_input_mapper(mm_data)
+                    else:
+                        mm_kwargs = mm_data
+                    if mm_kwargs is not None:
+                        multi_modal_kwargs_list.append(mm_kwargs)
+                    if self.runner.model_is_mrope and mm_data:
+                        image_grid_thw = mm_kwargs.get("image_grid_thw", None)
+                        video_grid_thw = mm_kwargs.get("video_grid_thw", None)
+                        assert image_grid_thw is not None or video_grid_thw is not None, (
+                            "mrope embedding type requires multi-modal input mapper "
+                            "returns 'image_grid_thw' or 'video_grid_thw'.")
+
+                        second_per_grid_ts = mm_kwargs.get("second_per_grid_ts", None)
+                        hf_config = self.runner.model_config.hf_config
+                        token_ids = seq_data.get_token_ids()
+                        temp_mrope_input_positions, mrope_position_delta = \
+                            MRotaryEmbedding.get_input_positions(
+                                token_ids,
+                                hf_config=hf_config,
+                                image_grid_thw=image_grid_thw,
+                                video_grid_thw=video_grid_thw,
+                                second_per_grid_ts=second_per_grid_ts,
+                                seq_len=seq_len,
+                                context_len=context_len,
+                            )
+                        seq_data.mrope_position_delta = mrope_position_delta
+                        if mrope_input_positions is None:
+                            mrope_input_positions = [[] for _ in range(3)]
+                        for idx in range(3):
+                            # msections = temp_mrope_input_positions
+                            # for _seq_mrope_input_positions in msections:
+                            mrope_input_positions[idx].extend(
+                                temp_mrope_input_positions[idx])
+                else:
+                    if seq_data.mrope_position_delta is not None:
+                        context_len = seq_data.get_num_computed_tokens()
+                        next_pos = MRotaryEmbedding.get_next_input_positions(
+                            seq_data.mrope_position_delta,
+                            context_len,
+                            seq_len,
+                        )
+                        for idx in range(3):
+                            input_mrope_positions[idx].extend(next_pos[idx])
+                if is_prompt:
+                    assert len(seq_ids) == 1
+                    num_prefills += 1
+                    num_prefill_tokens += len(tokens)
+                    prefill_seq_lens.append(seq_len)
+                else:
+                    assert query_len == 1, "Wrong query length in decoding"
+                    num_decode_tokens += 1
+                    decode_seq_lens = seq_lens
+                    #decode_seq_lens.append(seq_len)
+                if is_block_tables_empty(seq_group_metadata.block_tables):
+                    slot_mapping.extend([_PAD_SLOT_ID] * seq_len)
+                    continue
+                # seq_id: List[int]
+                block_table = seq_group_metadata.block_tables[seq_id]
+
+                start_idx=0
+                if self.sliding_window is not None:
+                    start_idx = max(0, seq_len - self.sliding_window)
+                # TODO: add sliding window
+                for i in range(context_len, seq_len):
+                    if i < start_idx:
+                        slot_mapping.append(_PAD_SLOT_ID)
+                        continue
+                    block_number = block_table[i // self.block_size]
+                    block_offset = i % self.block_size
+                    # slot_mapping is when we flatteren the blocks, and see which block it is located
+                    # block_table is a logical -> to physical transition...
+                    # i // block_size is the logical block number
+                    slot_mapping.append(block_number * self.block_size + block_offset)
+        max_decode_seq_len = max(decode_seq_lens, default=0)
+        is_prompt = (self.seq_group_metadata_list[0].is_prompt
+                if self.seq_group_metadata_list else None)
+        need_block_table = False
+        if  self.scheduler_config.chunked_prefill_enabled or not is_prompt or self.cache_config.enable_prefix_caching:
+            need_block_table = True
+            # max_block_table_len = max(
+            #     len(block_table) for block_table in block_tables)
+            block_tables = make_tensor_with_pad(
+                block_tables,
+                # max_len=max_block_table_len,
+                pad=0,
+                dtype=torch.int,
+                device=self.device,
+            )
+
+        input_tokens_tensor = torch.tensor(input_tokens,
+                                           dtype=torch.long,
+                                           device=self.device)
+
+        if self.runner.model_is_mrope and (mrope_input_positions is not None or any(input_mrope_positions)):
+            if any(input_mrope_positions):
+                mrope_input_positions = input_mrope_positions
+            input_positions_tensor = torch.tensor(mrope_input_positions,
+                                                  dtype=torch.long,
+                                                  device=self.device)
         else:
-            (input_tokens, input_positions,
-             attn_metadata) = self._prepare_decode(
-                 self.seq_group_metadata_list)
-            seq_lens = None
-            multi_modal_kwargs = None
+            input_positions_tensor = torch.tensor(input_positions,
+                                                  dtype=torch.long,
+                                                  device=self.device)
+
+        slot_mapping_tensor = torch.tensor(slot_mapping,
+                                           dtype=torch.long,
+                                           device=self.device)
+        if need_block_table or "bge" in self.runner.model_config.model.lower():
+            seq_lens_tensor = torch.tensor(seq_lens,
+                                        dtype=torch.int,
+                                        device=self.device)
+        else:
+            seq_lens_tensor = torch.tensor([])
+        if self.scheduler_config.chunked_prefill_enabled or self.cache_config.enable_prefix_caching:
+            context_lens_tensor = torch.tensor(context_lens,
+                                            dtype=torch.int,
+                                            device=self.device)
+            query_lens_tensor = torch.tensor(query_lens,
+                                            dtype=torch.long,
+                                            device=self.device)
+            query_start_loc = torch.zeros(query_lens_tensor.shape[0] + 1,
+                                        dtype=torch.int32,
+                                        device=self.device)
+
+            torch.cumsum(query_lens_tensor,
+                        dim=0,
+                        dtype=query_start_loc.dtype,
+                        out=query_start_loc[1:])
+        else:
+            context_lens_tensor = context_lens
+            query_start_loc = query_lens
 
+        # Generate attn_metadata
+        attn_metadata = self.attn_backend.make_metadata(
+            # FIXME: Later maybe we can get rid of this parameter
+            is_prompt=is_prompt, #1
+            num_prefills=num_prefills, # 6
+            slot_mapping=slot_mapping_tensor, # 2
+            num_prefill_tokens=num_prefill_tokens, # 7
+            num_decode_tokens=num_decode_tokens, # 8
+            seq_lens=seq_lens, # 3
+            seqlen_q=torch.tensor([]), # 4
+            multi_modal_placeholder_index_maps=None,
+            enable_kv_scales_calculation=False,
+            # max_seqlen=max_seqlen, # 5
+            max_seqlen=max(query_lens),
+            seq_lens_tensor=seq_lens_tensor, # 9
+            # max_query_len=max_query_len,
+            max_decode_seq_len=max_decode_seq_len, # 10
+            query_start_loc=query_start_loc,
+            # seq_start_loc=seq_start_loc,
+            context_lens=context_lens_tensor,
+            block_tables=block_tables if need_block_table else torch.tensor([], device=self.device, dtype=torch.int) # 11
+        )
+        if multi_modal_kwargs_list is not None and len(multi_modal_kwargs_list) > 0:
+            multi_modal_kwargs = MultiModalKwargs.batch(multi_modal_kwargs_list)
+        else:
+            multi_modal_kwargs = None
         return self.model_input_cls(
-            input_tokens=input_tokens,
-            input_positions=input_positions,
+            input_tokens=input_tokens_tensor,
+            input_positions=input_positions_tensor,
             attn_metadata=attn_metadata,
             multi_modal_kwargs=multi_modal_kwargs,
             seq_lens=seq_lens,
-            query_lens=seq_lens,
+            query_lens=query_lens,
         )
 
     def _prepare_prompt(
         self,
         seq_group_metadata_list: List[SequenceGroupMetadata],
     ) -> Tuple[torch.Tensor, torch.Tensor, AttentionMetadata, List[int],
-               BatchedTensorInputs]:
+               BatchedTensorInputs, List[int], List[int], Set[LoRARequest]]:
         assert len(seq_group_metadata_list) > 0
         input_tokens: List[int] = []
         input_positions: List[int] = []
@@ -166,6 +432,9 @@ class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
         multi_modal_placeholder_maps: Dict[
             str,
             MultiModalPlaceholderMap] = defaultdict(MultiModalPlaceholderMap)
+        lora_index_mapping: List[int] = []
+        lora_prompt_mapping: List[int] = []
+        lora_requests: Set[LoRARequest] = set()
 
         for seq_group_metadata in seq_group_metadata_list:
             assert seq_group_metadata.is_prompt
@@ -184,29 +453,55 @@ class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
             # Token position ids
             # NOTE(woosuk): Here we assume that the first token in the prompt
             # is always the first token in the sequence.
-            positions_range = range(computed_len, seq_len)
-            input_positions.extend(list(positions_range))
-
-            if seq_group_metadata.multi_modal_data:
-                # NOTE: mm_data only includes the subset of multi-modal items
-                # that intersect with the current prefill positions.
-                mm_data, placeholder_maps = MultiModalPlaceholderMap \
-                    .from_seq_group(seq_group_metadata, positions_range)
-
-                if self.runner.mm_registry.has_processor(
-                        self.runner.model_config):
-                    mm_kwargs = mm_data
-                else:
-                    mm_kwargs = self.runner.multi_modal_input_mapper(
-                        mm_data,
-                        seq_group_metadata.mm_processor_kwargs,
-                    )
-
+            input_positions.extend(list(range(computed_len, seq_len)))
+            mm_data = seq_group_metadata.multi_modal_data
+            if mm_data:
+                mm_kwargs = self.multi_modal_input_mapper(mm_data)
                 multi_modal_kwargs_list.append(mm_kwargs)
-
-                for modality, placeholder_map in placeholder_maps.items():
-                    multi_modal_placeholder_maps[modality].extend(
-                        placeholder_map)
+            if self.runner.model_is_mrope and mm_data:
+                image_grid_thw = mm_kwargs.get("image_grid_thw", None)
+                video_grid_thw = mm_kwargs.get("video_grid_thw", None)
+                assert image_grid_thw is not None or video_grid_thw is not None, (
+                    "mrope embedding type requires multi-modal input mapper "
+                    "returns 'image_grid_thw' or 'video_grid_thw'.")
+
+                hf_config = self.runner.model_config.hf_config
+                token_ids = seq_data.get_token_ids()
+                temp_mrope_input_positions, mrope_position_delta = \
+                    MRotaryEmbedding.get_input_positions(
+                        token_ids,
+                        image_grid_thw=image_grid_thw,
+                        video_grid_thw=video_grid_thw,
+                        image_token_id=hf_config.image_token_id,
+                        video_token_id=hf_config.video_token_id,
+                        vision_start_token_id=hf_config.vision_start_token_id,
+                        vision_end_token_id=hf_config.vision_end_token_id,
+                        spatial_merge_size=hf_config.vision_config.spatial_merge_size,
+                        context_len=0,
+                    )
+                seq_data.mrope_position_delta = mrope_position_delta
+                mrope_input_positions = [[] for _ in range(3)]
+                for idx in range(3):
+                    # msections = temp_mrope_input_positions
+                    # for _seq_mrope_input_positions in msections:
+                    mrope_input_positions[idx].extend(
+                        temp_mrope_input_positions[idx])
+                input_positions = mrope_input_positions
+
+
+            if self.runner.lora_config:
+                lora_id = seq_group_metadata.lora_int_id
+
+                if lora_id > 0:
+                    lora_requests.add(seq_group_metadata.lora_request)
+
+                lora_index_mapping += [lora_id] * (seq_len - computed_len)
+                lora_prompt_mapping.extend(
+                    [lora_id] *
+                    (seq_len -
+                     computed_len if seq_group_metadata.sampling_params
+                     and seq_group_metadata.sampling_params.prompt_logprobs
+                     is not None else 1))
 
             if seq_group_metadata.block_tables is None:
                 # During memory profiling, the block tables are not initialized
@@ -276,26 +571,34 @@ class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
         )
 
         multi_modal_kwargs = MultiModalKwargs.batch(multi_modal_kwargs_list)
-
         return (input_tokens, input_positions, attn_metadata, seq_lens,
-                multi_modal_kwargs)
+                multi_modal_kwargs, lora_index_mapping, lora_prompt_mapping,
+                lora_requests)
 
     def _prepare_decode(
         self,
         seq_group_metadata_list: List[SequenceGroupMetadata],
-    ) -> Tuple[torch.Tensor, torch.Tensor, AttentionMetadata]:
+    ) -> Tuple[torch.Tensor, torch.Tensor, AttentionMetadata, List[int],
+               List[int], Set[LoRARequest]]:
         assert len(seq_group_metadata_list) > 0
         input_tokens: List[int] = []
         input_positions: List[int] = []
         slot_mapping: List[int] = []
         seq_lens: List[int] = []
         block_tables: List[List[int]] = []
+        lora_index_mapping: List[int] = []
+        lora_prompt_mapping: List[int] = []
+        lora_requests: Set[LoRARequest] = set()
 
         for seq_group_metadata in seq_group_metadata_list:
             assert not seq_group_metadata.is_prompt
             assert seq_group_metadata.token_chunk_size == 1
 
             seq_ids = list(seq_group_metadata.seq_data.keys())
+            lora_id = seq_group_metadata.lora_int_id
+
+            if lora_id > 0:
+                lora_requests.add(seq_group_metadata.lora_request)
 
             for seq_id in seq_ids:
                 seq_data = seq_group_metadata.seq_data[seq_id]
@@ -315,6 +618,8 @@ class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
                 block_offset = position % self.block_size
                 slot = block_number * self.block_size + block_offset
                 slot_mapping.append(slot)
+                lora_index_mapping.append(lora_id)
+                lora_prompt_mapping.append(lora_id)
 
                 if self.sliding_window is not None:
                     sliding_window_blocks = (self.sliding_window //
@@ -359,17 +664,14 @@ class ModelInputForXPUBuilder(ModelRunnerInputBuilderBase[ModelInputForXPU]):
             num_prefills=0,
             block_tables=block_tables,
         )
-        return (
-            input_tokens,
-            input_positions,
-            attn_metadata,
-        )
+        return (input_tokens, input_positions, attn_metadata,
+                lora_index_mapping, lora_prompt_mapping, lora_requests)
 
 
-class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
-    _model_input_cls: Type[ModelInputForXPUWithSamplingMetadata] = (
-        ModelInputForXPUWithSamplingMetadata)
-    _builder_cls: Type[ModelInputForXPUBuilder] = ModelInputForXPUBuilder
+class XPUModelRunnerBase(ModelRunnerBase[TModelInputForXPU]):
+
+    _model_input_cls: Type[TModelInputForXPU]
+    _builder_cls: Type[ModelInputForXPUBuilder]
 
     def __init__(
         self,
@@ -410,6 +712,12 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
 
         # Lazy initialization.
         self.model: nn.Module  # Set after init_Model
+        # Set after load_model.
+        self.lora_manager: Optional[LRUCacheWorkerLoRAManager] = None
+
+        from vllm.model_executor.models.utils import set_cpu_offload_max_bytes
+        set_cpu_offload_max_bytes(
+            int(self.cache_config.cpu_offload_gb * 1024**3))
 
         self.sampling_metadata_cache: SamplingMetadataCache = \
               SamplingMetadataCache() \
@@ -432,6 +740,15 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
     def vocab_size(self) -> int:
         return self.model_config.get_vocab_size()
 
+    @property
+    def model_is_mrope(self) -> bool:
+        """Detect if the model has "mrope" rope_scaling type.
+        mrope requires keep "rope_deltas" between prompt and decoding phases."""
+        rope_scaling = getattr(self.model_config.hf_config, "rope_scaling", {})
+        if rope_scaling is None:
+            return False
+        return rope_scaling.get("type", None) == "mrope" or rope_scaling.get("mrope_section", None) is not None
+
     @torch.inference_mode()
     def profile_run(self) -> None:
         # Enable top-k sampling to reflect the accurate memory usage.
@@ -439,6 +756,30 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
         max_num_batched_tokens = self.scheduler_config.max_num_batched_tokens
         max_num_seqs = self.scheduler_config.max_num_seqs
 
+        # This represents the maximum number of different requests
+        # that will have unique loras, an therefore the max amount of memory
+        # consumption create dummy lora request copies from the lora request
+        # passed in, which contains a lora from the lora warmup path.
+        dummy_lora_requests: List[LoRARequest] = []
+        dummy_lora_requests_per_seq: List[LoRARequest] = []
+        if self.lora_config:
+            assert self.lora_manager is not None
+            with self.lora_manager.dummy_lora_cache():
+                for idx in range(self.lora_config.max_loras):
+                    lora_id = idx + 1
+                    dummy_lora_request = LoRARequest(
+                        lora_name=f"warmup_{lora_id}",
+                        lora_int_id=lora_id,
+                        lora_path="/not/a/real/path",
+                    )
+                    self.lora_manager.add_dummy_lora(dummy_lora_request,
+                                                     rank=LORA_WARMUP_RANK)
+                    dummy_lora_requests.append(dummy_lora_request)
+                dummy_lora_requests_per_seq = [
+                    dummy_lora_requests[idx % len(dummy_lora_requests)]
+                    for idx in range(max_num_seqs)
+                ]
+
         # Profile memory usage with max_num_sequences sequences and the total
         # number of tokens equal to max_num_batched_tokens.
         seqs: List[SequenceGroupMetadata] = []
@@ -448,6 +789,7 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
         # To exercise the worst scenario for GPU memory consumption,
         # the number of seqs (batch_size) is chosen to maximize the number
         # of images processed.
+        '''
         max_mm_tokens = self.mm_registry.get_max_multimodal_tokens(
             self.model_config)
         if max_mm_tokens > 0:
@@ -461,8 +803,18 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
                     "Computed max_num_seqs (%s) to be less than 1. "
                     "Setting it to the minimum value of 1.", expr)
                 max_num_seqs = 1
+        '''
 
         batch_size = 0
+        import os
+        self_max_num_batched_tokens = os.getenv("IPEX_LLM_SELF_MAX_NUM_BATCHED_TOKENS", None)
+        if self_max_num_batched_tokens is not None:
+            max_num_batched_tokens = int(self_max_num_batched_tokens)
+            self_max_num_seqs = os.getenv("IPEX_LLM_SELF_MAX_NUM_SEQS", None)
+            if self_max_num_seqs is not None:
+                max_num_seqs = int(self_max_num_seqs)
+            else:
+                max_num_seqs = 1
         for group_id in range(max_num_seqs):
             seq_len = (max_num_batched_tokens // max_num_seqs +
                        (group_id < max_num_batched_tokens % max_num_seqs))
@@ -479,11 +831,14 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
                 seq_data={group_id: dummy_data.seq_data},
                 sampling_params=sampling_params,
                 block_tables=None,
-                lora_request=None,
+                lora_request=dummy_lora_requests_per_seq[group_id]
+                if dummy_lora_requests_per_seq else None,
                 multi_modal_data=dummy_data.multi_modal_data,
                 multi_modal_placeholders=dummy_data.multi_modal_placeholders)
             seqs.append(seq)
 
+        num_layers = self.model_config.get_num_layers(self.parallel_config)
+        kv_caches = [None] * num_layers
         finished_requests_ids = [seq.request_id for seq in seqs]
         model_input = self.prepare_model_input(
             seqs, finished_requests_ids=finished_requests_ids)
@@ -493,25 +848,39 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
                 batch_size=batch_size,
                 dtype=self.model_config.dtype,
                 device=self.device)
-        self.execute_model(model_input, None, intermediate_tensors)
+        self.execute_model(model_input, kv_caches, intermediate_tensors)
         torch.xpu.synchronize()
         return
 
-    def make_model_input_from_broadcasted_tensor_dict(
-            self,
-            tensor_dict: Dict[str,
-                              Any]) -> ModelInputForXPUWithSamplingMetadata:
-        return (
-            ModelInputForXPUWithSamplingMetadata.from_broadcasted_tensor_dict(
-                tensor_dict,
-                attn_backend=self.attn_backend,
-            ))
+    def save_sharded_state(
+        self,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+    ) -> None:
+        from vllm.model_executor.model_loader.loader import ShardedStateLoader
+        ShardedStateLoader.save_model(
+            self.model,
+            path,
+            pattern=pattern,
+            max_size=max_size,
+        )
+
+    def save_tensorized_model(
+        self,
+        tensorizer_config: TensorizerConfig,
+    ) -> None:
+        from vllm.model_executor.model_loader.loader import TensorizerLoader
+        TensorizerLoader.save_model(
+            self.model,
+            tensorizer_config=tensorizer_config,
+        )
 
     def _prepare_model_input_tensors(
         self,
         seq_group_metadata_list: List[SequenceGroupMetadata],
         finished_requests_ids: Optional[List[str]] = None
-    ) -> ModelInputForXPUWithSamplingMetadata:
+    ) -> TModelInputForXPU:
         """Helper method to prepare the model input based on a given sequence
         group. Prepares metadata needed for the base model forward pass but not
         metadata for possible additional steps, e.g., sampling.
@@ -524,6 +893,22 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
 
         return builder.build()  # type: ignore
 
+
+class XPUModelRunner(XPUModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
+    _model_input_cls: Type[ModelInputForXPUWithSamplingMetadata] = (
+        ModelInputForXPUWithSamplingMetadata)
+    _builder_cls: Type[ModelInputForXPUBuilder] = ModelInputForXPUBuilder
+
+    def make_model_input_from_broadcasted_tensor_dict(
+            self,
+            tensor_dict: Dict[str,
+                              Any]) -> ModelInputForXPUWithSamplingMetadata:
+        return (
+            ModelInputForXPUWithSamplingMetadata.from_broadcasted_tensor_dict(
+                tensor_dict,
+                attn_backend=self.attn_backend,
+            ))
+
     def prepare_model_input(
         self,
         seq_group_metadata_list: List[SequenceGroupMetadata],
@@ -563,6 +948,12 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
             raise ValueError(
                 "XPUModelRunner does not support multi-step execution.")
 
+        if self.lora_config:
+            assert model_input.lora_requests is not None
+            assert model_input.lora_mapping is not None
+            self.set_active_loras(model_input.lora_requests,
+                                  model_input.lora_mapping)
+
         model_executable = self.model
         if (self.observability_config is not None
                 and self.observability_config.collect_model_forward_time):
@@ -612,3 +1003,9 @@ class XPUModelRunner(ModelRunnerBase[ModelInputForXPUWithSamplingMetadata]):
             output.model_forward_time = model_forward_time
 
         return [output]
+
+    def set_active_loras(self, lora_requests: Set[LoRARequest],
+                         lora_mapping: LoRAMapping) -> None:
+        if not self.lora_manager:
+            raise RuntimeError("LoRA is not enabled.")
+        self.lora_manager.set_active_adapters(lora_requests, lora_mapping)
diff --git a/vllm/worker/xpu_multi_step_model_runner.py b/vllm/worker/xpu_multi_step_model_runner.py
new file mode 100644
index 000000000..2bd1f8469
--- /dev/null
+++ b/vllm/worker/xpu_multi_step_model_runner.py
@@ -0,0 +1,678 @@
+import dataclasses
+import functools
+from dataclasses import dataclass, field
+from typing import (TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple,
+                    Union)
+import time
+import torch
+
+from vllm import _custom_ops as ops
+from vllm.distributed import get_pp_group
+from vllm.logger import init_logger
+from vllm.model_executor.layers.sampler import (PromptLogprobs, SampleLogprobs,
+                                                SamplerOutput,
+                                                SamplingMetadata, get_logprobs,
+                                                get_pythonized_sample_results)
+from vllm.sequence import (CompletionSequenceGroupOutput, IntermediateTensors,
+                           Logprob, SequenceGroupMetadata, SequenceOutput)
+from vllm.worker.xpu_model_runner import (XPUModelRunnerBase,
+                                        ModelInputForXPUWithSamplingMetadata)
+from vllm.worker.multi_step_model_runner import PythonizationCache, deferred_pythonize_logprobs
+from vllm.worker.model_runner_base import (
+    BroadcastableModelInput, _init_attn_metadata_from_tensor_dict,
+    _init_frozen_model_input_from_tensor_dict,
+    _init_sampling_metadata_from_tensor_dict)
+from vllm.attention.backends.ipex_attn import IpexAttnMetadata
+from ..model_executor.model_loader.tensorizer import TensorizerConfig
+
+if TYPE_CHECKING:
+    from vllm.attention.backends.abstract import AttentionBackend
+
+import time
+test_count = 0
+test_total_time = 0
+
+@dataclass
+class XPUModelOutput:
+    """The output of a single model forward pass.
+
+    The sampler_output_ready_event is set when the tensors in
+    sampler_output are ready (the model+sampler forward pass has
+    completed). We use the event to synchronize the GPU->CPU transfer,
+    which we want to only run when the data has been written to the
+    GPU tensors. Until the event is ready, the tensors in sampler_output
+    will have garbage data.
+
+    There are two scenarios:
+    1. The output tensors are ready and we can pythonize them immediately.
+    2. The output tensors are not ready and we need to wait for the event to be
+    ready.
+    """
+    sampler_output: SamplerOutput
+    sampler_output_ready_event: torch.xpu.Event
+    sampled_token_ids: Optional[torch.Tensor] = None
+    pythonized: bool = False
+    # On-device tensor containing the logprobs of each token.
+    logprobs: Optional["torch.Tensor"] = None
+    pythonization_cache: Optional[PythonizationCache] = None
+
+    def pythonize(self, input_metadata: "XPUStatefulModelInput",
+                  copy_stream: torch.xpu.Stream,
+                  pinned_sampled_token_buffer: torch.Tensor) -> None:
+        """Pythonize the output. Blocking."""
+        if not self.pythonized:
+            self._pythonize_sampler_output(input_metadata, copy_stream,
+                                           pinned_sampled_token_buffer, True)
+            self.pythonized = True
+
+    def maybe_pythonize(self, input_metadata: "XPUStatefulModelInput",
+                        copy_stream: torch.xpu.Stream,
+                        pinned_sampled_token_buffer: torch.Tensor) -> None:
+        """Pythonize the output if ready, else return None. Non-blocking."""
+        if not self.pythonized:
+            self.pythonized = self._pythonize_sampler_output(
+                input_metadata, copy_stream, pinned_sampled_token_buffer,
+                False)
+
+    def _pythonize_sampler_output(self, input_metadata: "XPUStatefulModelInput",
+                                  copy_stream: torch.xpu.Stream,
+                                  pinned_sampled_token_buffer: torch.Tensor,
+                                  blocking: bool) -> bool:
+        """
+        If blocking is set, will block until the forward pass for the output is
+        ready and pythonize the output. Upon completing Pythonization, erases
+        self.logprobs (note that a non-blocking call that is performed when
+        the sampler output is not yet ready, will not erase self.logprobs.)
+        """
+        assert self.sampled_token_ids is not None
+        if not blocking and not self.sampler_output_ready_event.query():
+            return False
+
+        if blocking:
+            self.sampler_output_ready_event.synchronize()
+        with torch.xpu.stream(copy_stream):
+            _pythonize_sampler_output(input_metadata, self.sampler_output,
+                                      pinned_sampled_token_buffer,
+                                      self.sampled_token_ids, self.logprobs,
+                                      self.pythonization_cache)
+
+        # Erase the logprobs GPU-side tensor.
+        # Note that although _pythonize_sampler_output() runs in its
+        # own XPU stream, nonetheless _pythonize_sampler_output()
+        # cannot return until Pythonization is complete; therefore
+        # we know that by the time the CPU reaches this point,
+        # `self.logprobs` is no longer needed.
+        self.logprobs = None
+        return True
+
+
+
+@dataclass(frozen=False)
+class XPUStatefulModelInput(BroadcastableModelInput):
+    # actual frozen model input dataclass passed to _base_model_runner
+    frozen_model_input: Optional[ModelInputForXPUWithSamplingMetadata] = None
+
+    # list of model outputs for each step, may not be all pythonized
+    cached_outputs: List[XPUModelOutput] = field(default_factory=list)
+
+    # used to pass sampled token ids from the last step to the current step for
+    # TP workers. Used to append to end of outputs and used by advance_step
+    last_sampled_token_ids: Optional[torch.Tensor] = None
+    current_step: int = 0
+    is_multi_step: bool = True
+    is_last_step: bool = False
+    is_first_multi_step: bool = False
+    # ping-pong data structures for multi-step to wait on the previous step
+    step_xpu_events: List[torch.xpu.Event] = field(
+        default_factory=lambda: [torch.xpu.Event()] * 2)
+        # FIXME: use blocking
+        # default_factory=lambda: [torch.xpu.Event(blocking=True)] * 2)
+    num_seqs: int = -1
+    num_queries: int = -1
+
+    def as_broadcastable_tensor_dict(self) -> Dict[str, Any]:
+        assert self.frozen_model_input is not None
+        tensor_dict = self.frozen_model_input.as_broadcastable_tensor_dict()
+        new_tensor_dict = {
+            'last_sampled_token_ids': self.last_sampled_token_ids,
+            'current_step': self.current_step,
+            'is_multi_step': self.is_multi_step,
+            'is_last_step': self.is_last_step,
+            'is_first_multi_step': self.is_first_multi_step,
+            'num_seqs': self.num_seqs,
+            'num_queries': self.num_queries,
+        }
+        tensor_dict.update(new_tensor_dict)
+        return tensor_dict
+
+    @classmethod
+    def from_broadcasted_tensor_dict(
+        cls,
+        tensor_dict: Dict[str, Any],
+        attn_backend: Optional["AttentionBackend"] = None,
+    ) -> "XPUStatefulModelInput":
+        tensor_dict = _init_sampling_metadata_from_tensor_dict(tensor_dict)
+        if attn_backend is not None:
+            tensor_dict = _init_attn_metadata_from_tensor_dict(
+                attn_backend, tensor_dict)
+        tensor_dict = _init_frozen_model_input_from_tensor_dict(
+            ModelInputForXPUWithSamplingMetadata, tensor_dict)
+
+        return cls(**tensor_dict)
+
+    def record_step_event(self, current_stream: torch.xpu.Stream):
+        # record the event for the current step so that the next step can sync
+        # on it. We modulo by 2 to keep the events in a circular buffer and
+        # support any attn backends that may be supported in the future. ie
+        # Flashinfer would want two DecodeWrappers to overlap the CPU and GPU.
+        
+        self.step_xpu_events[self.current_step & 1] = \
+            torch.xpu.Event()
+            # torch.xpu.Event(blocking=True)
+        self.step_xpu_events[self.current_step & 1].record(current_stream)
+
+    def wait_previous_step(self):
+        # These xpu events are an explicit synchronization to ensure that
+        # advance_step() (for other attn backends that may be supported in the
+        # future) do not clobber any data structures that is also used by any
+        # enqueued forwards steps. For distributed case, only a single event is
+        # needed, but for single GPU case, since we can let the CPU run much
+        # further ahead, two events allow us to overlap the advance_step with
+        # the previous forward (ie using two DecodeWrappers for flashinfer
+        # backend)
+        self.step_xpu_events[(self.current_step + 1) & 1].wait()
+
+    def add_sampler_output(self,
+                           sampler_output: SamplerOutput,
+                           sampled_token_ids: Optional[torch.Tensor] = None):
+        self.cached_outputs.append(
+            XPUModelOutput(sampler_output=sampler_output,
+                            sampler_output_ready_event=None,
+                            sampled_token_ids=sampled_token_ids,
+                            pythonized=False))
+
+
+# mypy: disable-error-code=type-var
+class XPUMultiStepModelRunner(XPUModelRunnerBase[XPUStatefulModelInput]):
+    # mypy: enable-error-code=type-var
+    def __init__(self, base_model_runner: XPUModelRunnerBase, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        
+        # uses the base model runner to execute the model and wraps it with
+        # multi-step logic
+        self._base_model_runner: XPUModelRunnerBase = base_model_runner
+    
+        self.is_multi_step = self.scheduler_config.is_multi_step
+        # used to copy tensors from GPU to CPU asynchronously
+        self._copy_stream = torch.xpu.Stream()
+        self.pinned_sampled_token_ids: Optional[torch.Tensor] = None
+
+        self.pythonization_cache = PythonizationCache()
+
+    def make_model_input_from_broadcasted_tensor_dict(
+            self, tensor_dict: Dict[str, Any]) -> XPUStatefulModelInput:
+        model_input = (XPUStatefulModelInput.from_broadcasted_tensor_dict(
+            tensor_dict,
+            attn_backend=self.attn_backend,
+        ))
+        return model_input
+
+    def prepare_model_input(
+        self,
+        seq_group_metadata_list: List[SequenceGroupMetadata],
+        virtual_engine: int = 0,
+        finished_requests_ids: Optional[List[str]] = None
+    ) -> XPUStatefulModelInput:
+        frozen_model_input = self._base_model_runner.prepare_model_input(
+            seq_group_metadata_list, virtual_engine, finished_requests_ids)
+
+        model_input = XPUStatefulModelInput(
+            frozen_model_input=frozen_model_input,
+            num_seqs=len(frozen_model_input.seq_lens),
+            num_queries=len(frozen_model_input.query_lens),
+        )
+        return model_input
+
+    def _async_process_outputs(self, model_input: XPUStatefulModelInput,
+                               output_proc_callback: Callable):
+        # Proceed with pythonization and output_proc in order.
+        # Stop on the first one that fails to pythonize
+        output_proc_callback()
+
+        cont = True
+        for step_num, model_output in enumerate(model_input.cached_outputs):
+            if not model_output.pythonized:
+                model_output.maybe_pythonize(model_input, self._copy_stream,
+                                             self.pinned_sampled_token_ids)
+                if model_output.pythonized:
+                    ctx = output_proc_callback.keywords["ctx"]
+                    ctx.append_output(
+                        outputs=[model_output.sampler_output], 
+                        seq_group_metadata_list=ctx.seq_group_metadata_list,
+                        scheduler_outputs=ctx.scheduler_outputs,
+                        is_async=False,
+                        is_last_step=False)
+                        # is_first_step_output=step_num == 0)
+                    output_proc_callback()
+                else:
+                    cont = False
+
+            if not cont:
+                break
+
+    def _final_process_outputs(self, model_input: XPUStatefulModelInput,
+                               output_proc_callback: Optional[Callable]):
+        assert model_input.frozen_model_input is not None
+
+        has_async_callback = output_proc_callback is not None
+
+        outputs = []
+        for output_id in range(len(model_input.cached_outputs)):
+            output = model_input.cached_outputs[output_id]
+            is_last_step = output_id == len(model_input.cached_outputs) - 1
+
+            # For non-async case:
+            #   -- We simply add the outputs
+            # For async case:
+            #   -- Invoke callback, pythonize, add to callback queue and repeat
+            #   -- For last output, just add to callback queue
+            if has_async_callback:
+                assert output_proc_callback is not None
+
+                # Invoke callback before pythonize (to overlap with GPU)
+                output_proc_callback()
+
+                # Pythonize
+                if not output.pythonized:
+                    output.pythonize(model_input, self._copy_stream,
+                                     self.pinned_sampled_token_ids)
+
+                    # For non last step, add to callback queue to chain
+                    # callbacks=>pythonize pairs (for GPU overlap)
+                    if not is_last_step:
+                        ctx = output_proc_callback.keywords[  # type: ignore
+                            "ctx"]  # type: ignore
+                        is_async = False
+                        is_last_step = False
+                        ctx.output_queue.append(
+                            ([output.sampler_output
+                              ], ctx.seq_group_metadata_list,
+                             ctx.scheduler_outputs, is_async, is_last_step))
+                    else:
+                        outputs.append(output.sampler_output)
+            else:
+                output.pythonize(model_input, self._copy_stream,
+                                 self.pinned_sampled_token_ids)
+                outputs.append(output.sampler_output)
+
+        return outputs
+    
+    @torch.inference_mode()
+    def execute_model(
+        self,
+        model_input: XPUStatefulModelInput,
+        kv_caches: List[torch.Tensor],
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        num_steps: int = 1,
+    ) -> Optional[Union[List[SamplerOutput], IntermediateTensors]]:
+        """ 
+        Execute the model for a single step and update multi-step
+        metadata
+        """
+        assert num_steps == 1, "MultiStepModelRunner only supports num_steps=1"
+        frozen_model_input = model_input.frozen_model_input
+        assert frozen_model_input is not None
+
+        # path for warm up runs
+        if not model_input.is_multi_step:
+            return self._base_model_runner.execute_model(
+                frozen_model_input, kv_caches, intermediate_tensors, num_steps)
+
+        # make sure we skip the sampler on the lask rank and only pythonize
+        # if CPU is ahead.
+        if self.is_driver_worker and get_pp_group().is_last_rank:
+            if self.pinned_sampled_token_ids is None:
+
+                self.pinned_sampled_token_ids = torch.zeros(
+                    (self.scheduler_config.max_num_seqs, 1),
+                    dtype=torch.long,
+                    device="cpu",
+                    pin_memory=False)
+                    # pin_memory=False)
+
+            self._base_model_runner.model.sampler.include_gpu_probs_tensor = (
+                True)
+            if frozen_model_input.sampling_metadata:
+                frozen_model_input.sampling_metadata.skip_sampler_cpu_output = (
+                    True)
+
+        # some pre-execute model logic for multi-step:
+        #   - if it's the first step, we need to reset the sampling tensors
+        #   - if it's not the first step, we need to advance the step using the
+        #   appended sampler output from last iteration
+        #   - also maybe pythonize if CPU is ahead of GPU
+
+        current_stream = torch.xpu.current_stream()
+        if not model_input.is_first_multi_step:
+            # Explicitly block on the previous step's forward to make sure we
+            # don't clobber any GPU tensors still in use.
+            # This is not needed for flashattn backend, but for other attn
+            # backends such as flashinfer that performs extra CPU operations on
+            # input metadata we may need to synchronize any CPU operations that
+            # might clobber enqueued forwards. (prevents CPU from running too
+            # far ahead if needed)
+            model_input.wait_previous_step()
+            model_input = self._advance_step(
+                model_input, model_input.cached_outputs[-1].sampler_output)
+
+        output_proc_callback = None
+        if frozen_model_input.async_callback is not None:
+            output_proc_callback = frozen_model_input.async_callback
+            assert output_proc_callback is not None
+            async_callback = functools.partial(
+                self._async_process_outputs,
+                model_input=model_input,
+                output_proc_callback=output_proc_callback)
+
+            frozen_model_input = dataclasses.replace(  # type: ignore
+                model_input.frozen_model_input,
+                async_callback=async_callback)
+            assert frozen_model_input is not None
+
+        # Execute the model
+        output = self._base_model_runner.execute_model(frozen_model_input,
+                                                       kv_caches,
+                                                       intermediate_tensors,
+                                                       num_steps=1)
+
+        # record the event for the current step so that the next step can sync
+        model_input.record_step_event(current_stream)
+
+        if get_pp_group().is_last_rank and self.is_driver_worker:
+            assert len(
+                output
+            ) == 1, "MultiStepModelRunner requires single-step base_models"
+
+            # event for the pythonization so that we only pythonize if the
+            # tensors are ready. May be able to be combined with the step event
+            output_ready_event = torch.xpu.Event()
+            output_ready_event.record(current_stream)
+            if self.parallel_config.pipeline_parallel_size > 1:
+                output[0].sampled_token_ids_cpu = output[
+                    0].sampled_token_ids.cpu()
+            model_input.cached_outputs.append(
+                XPUModelOutput(output[0], output_ready_event,
+                            output[0].sampled_token_ids, False,
+                            output[0].logprobs, self.pythonization_cache))
+
+            # These GPU tensors are not required by multi-step;
+            # erase them to ensure they are not pythonized or
+            # transferred to CPU
+            output[0].sampled_token_ids = None
+            output[0].sampled_token_probs = None
+            output[0].logprobs = None
+
+            # Pythonize the output if CPU is ahead and the previous step is
+            # ready.
+            if frozen_model_input.async_callback is None:
+                for model_output in model_input.cached_outputs:
+                    model_output.maybe_pythonize(model_input,
+                                                 self._copy_stream,
+                                                 self.pinned_sampled_token_ids)
+
+        model_input.current_step += 1
+
+        if not get_pp_group().is_last_rank:
+            # Should be IntermediateTensors
+            assert isinstance(output, IntermediateTensors)
+            return output
+        if not self.is_driver_worker:
+            return []
+
+        # Pythonize the output and block if needed since it is the last step
+        if model_input.is_last_step:
+            outputs = self._final_process_outputs(model_input,
+                                                  output_proc_callback)
+            self.pythonization_cache.reset()
+            return outputs
+
+        # should be [SamplerOutput]
+        return output
+
+    def _update_sampling_metadata(self, sampling_metadata, num_seqs,
+                                  num_queries):
+
+        assert sampling_metadata.num_prompts == 0
+        assert len(sampling_metadata.seq_groups) == num_queries
+        assert sampling_metadata.selected_token_indices.shape == (
+            num_queries, )
+        # assert sampling_metadata.categorized_sample_indices == TODO: Add if needed # noqa: E501
+
+        # Verify that all sequences are decodes
+        for i in range(num_queries):
+            seq_group = sampling_metadata.seq_groups[i]
+
+            assert seq_group.is_prompt is False  # No prompt
+            assert seq_group.prompt_logprob_indices == []  # No prompt
+            assert seq_group.sample_indices == [i]  # Simple
+            assert seq_group.seq_len is None  # Decode
+            assert seq_group.query_len is None  # Decode
+
+    def _advance_step(self, model_input: XPUStatefulModelInput,
+                      out: SamplerOutput) -> XPUStatefulModelInput:
+        frozen_model_input = model_input.frozen_model_input
+        assert frozen_model_input is not None
+        assert frozen_model_input.attn_metadata is not None
+
+        num_seqs = model_input.num_seqs
+        num_queries = model_input.num_queries
+        assert num_seqs > 0
+        assert num_queries > 0
+        assert num_seqs >= num_queries
+
+        attn_metadata = frozen_model_input.attn_metadata
+        assert isinstance(attn_metadata, IpexAttnMetadata)
+        attn_metadata.advance_step(num_seqs, num_queries)
+
+        # refer ops.advance_step()
+        next_seq_len = attn_metadata.seq_lens_tensor + 1
+        next_input_pos = next_seq_len - 1
+        attn_metadata.seq_lens_tensor = next_seq_len
+
+        block_index = next_input_pos // self.block_size
+        block_offset = next_input_pos % self.block_size
+        slot = attn_metadata.block_tables
+        slot_num = slot[torch.arange(num_queries), block_index] * self.block_size + block_offset
+        attn_metadata.slot_mapping = slot_num.to(dtype=torch.long)
+
+        tmp_input_tokens = frozen_model_input.input_tokens
+        sampled_token_ids = model_input.cached_outputs[-1].sampled_token_ids
+        if sampled_token_ids.dim() > 1 and sampled_token_ids.size(-1) == 1:
+            sampled_token_ids = sampled_token_ids.squeeze(-1)
+        tmp_input_tokens[:num_queries] = sampled_token_ids[:num_queries]
+        tmp_input_positions = frozen_model_input.input_positions
+        tmp_input_positions[:num_queries] = next_input_pos[:num_queries]
+        frozen_model_input = dataclasses.replace(
+            frozen_model_input,
+            input_tokens=tmp_input_tokens,
+            input_positions=tmp_input_positions,
+        )
+
+        if frozen_model_input.seq_lens is not None:
+            tmp_seq_lens = frozen_model_input.seq_lens
+            tmp_seq_lens[:num_queries] = attn_metadata.seq_lens[:num_queries]
+            frozen_model_input = dataclasses.replace(
+                frozen_model_input,
+                seq_lens=tmp_seq_lens,
+            )
+
+        return model_input
+
+    def load_model(self) -> None:
+        return self._base_model_runner.load_model()
+    
+    def save_sharded_state(
+        self,
+        path: str,
+        pattern: Optional[str] = None,
+        max_size: Optional[int] = None,
+    ) -> None:
+        return self._base_model_runner.save_sharded_state(
+            path, pattern, max_size)
+
+    def save_tensorized_model(self,
+                              tensorizer_config: TensorizerConfig) -> None:
+        return self._base_model_runner.save_tensorized_model(tensorizer_config)
+
+    def profile_run(self) -> None:
+        return self._base_model_runner.profile_run()
+
+
+    
+def _pythonize_sampler_output(
+    model_input: XPUStatefulModelInput,
+    output: SamplerOutput,
+    pinned_sampled_token_buffer: torch.Tensor,
+    sampled_token_ids: torch.Tensor,
+    logprobs_tensor: Optional[torch.Tensor],
+    cache: Optional[PythonizationCache],
+) -> None:
+    """ This function is only called when the output tensors are ready. 
+    See :class:`ModelOutput`. 
+    
+    Modifies `output.outputs` and `pinned_sampled_token_buffer` in-place, 
+    adding a Pythonized output data structure
+    (:class:`CompletionSequenceGroupOutput`) for each :class:`SequenceGroup`.
+
+    Args:
+      model_input
+      output: sampler output
+      pinned_sampled_token_token_buffer: CPU-side pinned memory
+                                         (receives copy of
+                                         GPU-side token buffer.)
+      sampled_token_ids: GPU-side token buffer
+      logprobs_tensor: GPU-side tensor containing 
+                       logprobs computed during sampling
+    """
+
+    assert model_input.frozen_model_input is not None
+
+    frozen_model_input = model_input.frozen_model_input
+    assert frozen_model_input.sampling_metadata is not None
+    # samples generation should have been skipped
+    assert not output.outputs
+
+    pinned_buffer = pinned_sampled_token_buffer[:model_input.num_queries]
+
+    # CPU GPU sync
+    pinned_buffer = pinned_buffer.copy_(sampled_token_ids, non_blocking=False)
+
+    # this will not block as the tensors are already on CPU
+    samples_list = pinned_buffer.tolist()
+
+    sampling_metadata = frozen_model_input.sampling_metadata
+
+    skip_sampler_cpu_output = (
+        frozen_model_input.sampling_metadata.skip_sampler_cpu_output)
+
+    # We are guaranteed output tensors are ready, so it is safe to
+    # pythonize the sampler output & obtain CPU-side logprobs.
+    #
+    # However this computation may be skipped entirely
+    # if no pythonization was deferred.
+    seq_groups = sampling_metadata.seq_groups
+    logprobs_are_requested = any([
+        sg.sampling_params.logprobs is not None
+        or sg.sampling_params.prompt_logprobs is not None for sg in seq_groups
+    ])
+    do_pythonize_logprobs = (skip_sampler_cpu_output
+                             and logprobs_are_requested)
+    (
+        prompt_logprobs,
+        sample_logprobs,
+    ) = (deferred_pythonize_logprobs(output, sampling_metadata,
+                                     logprobs_tensor)
+         if do_pythonize_logprobs else (None, None))
+
+    for sgdx, (seq_group,
+               sample_result) in enumerate(zip(seq_groups, samples_list)):
+        if seq_group.sampling_params.logits_processors:
+            assert len(seq_group.sampling_params.logits_processors) == 0, (
+                "Logits Processors are not supported in multi-step decoding")
+
+        if do_pythonize_logprobs:
+            assert prompt_logprobs is not None
+            assert sample_logprobs is not None
+
+            (
+                group_prompt_logprobs,
+                group_sample_logprobs,
+            ) = (  # Utilize deferred pythonization results
+                prompt_logprobs[sgdx],
+                sample_logprobs[sgdx],
+            )
+        elif logprobs_are_requested:
+            (
+                group_prompt_logprobs,
+                group_sample_logprobs,
+            ) = (
+                # profile_run: use already-computed logprobs
+                output.outputs[sgdx].prompt_logprobs,
+                [sample.logprobs for sample in output.outputs[sgdx].samples])
+
+        seq_ids = seq_group.seq_ids
+        next_token_ids = sample_result
+        parent_ids = [0]
+
+        if cache is not None:
+            completion_seq_group_output: CompletionSequenceGroupOutput = \
+                cache.cached_completion_seq_group_output.get_object()
+            completion_seq_group_output.samples.clear()
+            seq_outputs: List[
+                SequenceOutput] = completion_seq_group_output.samples
+        else:
+            seq_outputs = []
+
+        for tdx, (parent_id,
+                  next_token_id) in enumerate(zip(parent_ids, next_token_ids)):
+            if cache is not None:
+                seq_output: SequenceOutput = cache.cached_seq_output.get_object(
+                )
+                seq_output.parent_seq_id = seq_ids[parent_id]
+                seq_output.output_token = next_token_id
+
+                if logprobs_are_requested:
+                    seq_output.logprobs = group_sample_logprobs[tdx]
+                else:
+                    logprobs = next(iter(seq_output.logprobs.values()))
+                    seq_output.logprobs.clear()
+
+                    logprobs.logprob = float('inf')
+                    logprobs.rank = None
+                    logprobs.decoded_token = None
+
+                    seq_output.logprobs[next_token_id] = logprobs
+
+                seq_outputs.append(seq_output)
+
+            else:
+                seq_outputs.append(
+                    SequenceOutput(seq_ids[parent_id], next_token_id,
+                                   (group_sample_logprobs[tdx]
+                                    if logprobs_are_requested else {
+                                        next_token_id:
+                                        Logprob(logprob=float('inf'),
+                                                rank=None,
+                                                decoded_token=None)
+                                    })))
+        if cache is not None:
+            completion_seq_group_output.prompt_logprobs = \
+                group_prompt_logprobs if logprobs_are_requested else None
+            output.outputs.append(completion_seq_group_output)
+        else:
+            output.outputs.append(
+                CompletionSequenceGroupOutput(
+                    seq_outputs, (group_prompt_logprobs
+                                  if logprobs_are_requested else None)))
+
+    assert len(output.outputs) > 0
diff --git a/vllm/worker/xpu_multi_step_worker.py b/vllm/worker/xpu_multi_step_worker.py
new file mode 100644
index 000000000..6ad951824
--- /dev/null
+++ b/vllm/worker/xpu_multi_step_worker.py
@@ -0,0 +1,203 @@
+
+import dataclasses
+from dataclasses import dataclass
+from typing import Any, Dict, List, Optional, Tuple
+
+import torch
+
+from vllm.distributed import broadcast_tensor_dict, get_pp_group
+from vllm.model_executor.layers.sampler import SamplerOutput
+from vllm.sequence import ExecuteModelRequest
+from vllm.worker.model_runner_base import BroadcastableModelInput
+from vllm.worker.xpu_multi_step_model_runner import XPUMultiStepModelRunner,XPUStatefulModelInput
+from vllm.worker.worker import Worker, WorkerInput
+
+from vllm.worker.multi_step_worker import MultiStepWorker
+from vllm.worker.xpu_worker import XPUWorker
+from vllm.worker.xpu_model_runner import XPUModelRunnerBase
+
+@dataclass
+class XPUMultiStepState:
+    worker_input: WorkerInput
+    model_input: XPUStatefulModelInput
+
+
+class XPUMultiStepWorker(XPUWorker):
+
+    def __init__(self, *args, **kwargs) -> None:
+        super().__init__(*args, **kwargs)
+        base_model_runner = self.model_runner
+        self.model_runner = XPUMultiStepModelRunner( # type: ignore
+            base_model_runner, # type: ignore
+            base_model_runner.model_config,
+            base_model_runner.parallel_config,
+            base_model_runner.scheduler_config,
+            base_model_runner.device_config,
+            base_model_runner.cache_config,
+            load_config=base_model_runner.load_config,
+            lora_config=self.lora_config,
+            kv_cache_dtype=self.cache_config.cache_dtype,
+            is_driver_worker=base_model_runner.is_driver_worker,
+            prompt_adapter_config=base_model_runner.prompt_adapter_config,
+            observability_config=base_model_runner.observability_config,
+        )
+        pipeline_parallel_size = self.parallel_config.pipeline_parallel_size
+        self.multi_step_states: List[
+            Optional[XPUMultiStepState]] = [None] * pipeline_parallel_size
+        self.temp_output = None
+    def _get_driver_input_and_broadcast(
+        self, execute_model_req: ExecuteModelRequest
+    ) -> Tuple[BroadcastableModelInput, WorkerInput, Dict[str, torch.Tensor]]:
+        """
+        Get the driver input and broadcast it to other workers.
+        """
+        assert self.is_driver_worker
+        virtual_engine = execute_model_req.virtual_engine
+        is_first_multi_step = execute_model_req.is_first_multi_step
+        if is_first_multi_step:
+            # on first step we prepare the worker input and model input normally
+            worker_input: WorkerInput = self.prepare_worker_input(
+                execute_model_req=execute_model_req)
+            model_input: XPUStatefulModelInput = (
+                self.model_runner.prepare_model_input(
+                    execute_model_req.seq_group_metadata_list,
+                    execute_model_req.virtual_engine,
+                    execute_model_req.finished_requests_ids))
+
+            if execute_model_req.async_callback:
+                model_input.frozen_model_input = dataclasses.replace(  # type: ignore
+                    model_input.frozen_model_input,
+                    async_callback=execute_model_req.async_callback)
+        else:
+            # on subsequent steps we reuse the worker input and model input
+            multi_step_state = self.multi_step_states[virtual_engine]
+            worker_input = multi_step_state.worker_input
+            model_input = multi_step_state.model_input
+            frozen_model_input = model_input.frozen_model_input
+            assert frozen_model_input is not None
+            assert frozen_model_input.attn_metadata is not None
+            # clear the cached decode metadata so that it can be recomputed on
+            # the workers
+            frozen_model_input.attn_metadata._cached_decode_metadata = None
+
+        model_input.is_first_multi_step = is_first_multi_step
+        model_input.is_last_step = execute_model_req.is_last_step
+
+        if not is_first_multi_step:
+            # we broadcast the last sampled token ids to all TP workers so they
+            # can update their model input metadata in-place.
+            self._prepare_last_sampled_token_ids_for_tp_workers(
+                execute_model_req=execute_model_req, model_input=model_input)
+
+        if self.do_metadata_broadcast:
+            broadcast_data = worker_input.as_broadcastable_tensor_dict()
+            broadcast_data.update(model_input.as_broadcastable_tensor_dict())
+            broadcast_tensor_dict(broadcast_data, src=0)
+
+        # Retuning empty dict here to keep this compatible with
+        # `LocalOrDistributedWorkerBase._get_driver_input_and_broadcast`
+        return model_input, worker_input, {}
+
+    # mypy: disable-error-code=type-var
+    def _prepare_last_sampled_token_ids_for_tp_workers(
+        self,
+        execute_model_req: ExecuteModelRequest,
+        model_input: XPUStatefulModelInput,
+    ) -> None:
+        # mypy: enable-error-code=type-var
+        """ 
+        Prepare the last sampled token ids for TP workers. If it's the last 
+        PP rank, then the last sampled token ids are already in the model_input.
+        If it is NOT the last PP rank, then we need to get the last sampled
+        token that is cached in the execute_model_req.
+        """
+        if get_pp_group().is_last_rank:
+            assert model_input.cached_outputs[
+                -1].sampler_output.sampled_token_ids is None
+            assert model_input.cached_outputs[-1].sampled_token_ids is not None
+            model_input.last_sampled_token_ids = model_input.cached_outputs[
+                -1].sampled_token_ids
+            # free sampled token ids from the previous step if it has been
+            # pythonized. Cannot free the last sampled token ids because
+            # we need it for GPU advance_step.
+            for output in model_input.cached_outputs[:-1]:
+                if output.pythonized:
+                    output.sampled_token_ids = None
+        else:
+            # otherwise we need to get the cached sampled token ids from the
+            # execute_model_req
+            assert execute_model_req.last_sampled_token_ids is not None
+            model_input.last_sampled_token_ids = (
+                execute_model_req.last_sampled_token_ids.xpu())
+            model_input.add_sampler_output(
+                SamplerOutput(outputs=[], sampled_token_ids=None),
+                model_input.last_sampled_token_ids)
+
+            # free sampled token ids from the previous step.
+            # TODO(will) we could reuse the sampled token ids tensor from
+            # the previous step instead.
+            for output in model_input.cached_outputs[:-1]:
+                output.sampled_token_ids = None
+            assert model_input.cached_outputs[-1].sampled_token_ids is not None
+
+    def prepare_input(
+        self,
+        execute_model_req: Optional[ExecuteModelRequest] = None,
+    ) -> Optional[Tuple[XPUStatefulModelInput, WorkerInput, Dict[str,
+                                                              torch.Tensor]]]:
+        """
+        Depending on the current state of the request and multi step worker,
+        this method may skip the normal _prepare_model_input and
+        _prepare_worker_input methods and instead used cached values.
+        """
+        if self.is_driver_worker:
+            if execute_model_req is None:
+                if self.do_metadata_broadcast:
+                    # This signals that there's no more requests to process for
+                    # now. All workers are running infinite loop with
+                    # broadcast_tensor_dict, and it stops the loop when the
+                    # driver broadcasts an empty input. Send an empty input to
+                    # notify all other workers to stop their execution loop.
+                    broadcast_tensor_dict({}, src=0)
+                return None
+
+            virtual_engine = execute_model_req.virtual_engine
+            (model_input, worker_input,
+             kwargs) = self._get_driver_input_and_broadcast(execute_model_req)
+            assert isinstance(model_input, XPUStatefulModelInput)
+            if execute_model_req.is_first_multi_step:
+                # cache the worker input and model input for the next steps
+                self.multi_step_states[virtual_engine] = XPUMultiStepState(
+                    worker_input=worker_input, model_input=model_input)
+        # if TP workers
+        else:
+            broadcast_data = self._get_worker_input_from_broadcast()
+            # if the driver has sent an empty input, we should stop the worker
+            # loop
+            if broadcast_data is None:
+                return None
+            model_input, worker_input, kwargs = broadcast_data
+            assert isinstance(model_input, XPUStatefulModelInput)
+            virtual_engine = worker_input.virtual_engine
+            if model_input.is_first_multi_step:
+                pass
+                # TODO(will) Can cache the worker input and model input for the
+                # next steps. See below for details
+            else:
+                # TODO(will) possible to also cache and reuse the cached worker
+                # input and model input. The idea is essentially the delta
+                # optimization for model_inputs. Where the TP workers can cache
+                # the model input states and we only broadcast the delta need
+                # for the next step (sampled_token_ids from the previous step)
+
+                assert isinstance(model_input, XPUStatefulModelInput)
+                # we need to update the last sampled token ids in the model
+                # input for the workers so that they can run inplace
+                # advance_step
+                model_input.add_sampler_output(
+                    SamplerOutput(outputs=[], sampled_token_ids=None),
+                    model_input.last_sampled_token_ids)
+
+        assert model_input is not None
+        assert worker_input is not None
+        return model_input, worker_input, kwargs
\ No newline at end of file
diff --git a/vllm/worker/xpu_pooling_model_runner.py b/vllm/worker/xpu_pooling_model_runner.py
new file mode 100644
index 000000000..550bf81e8
--- /dev/null
+++ b/vllm/worker/xpu_pooling_model_runner.py
@@ -0,0 +1,135 @@
+import dataclasses
+from typing import Any, Dict, List, Optional, Tuple, Type, Union
+
+import torch
+
+from vllm.forward_context import set_forward_context
+from vllm.model_executor.pooling_metadata import PoolingMetadata
+from vllm.multimodal import MultiModalKwargs
+from vllm.pooling_params import PoolingParams
+from vllm.sequence import (IntermediateTensors, PoolerOutput, SequenceData,
+                           SequenceGroupMetadata)
+# from vllm.worker.cpu_model_runner import (CPUModelRunnerBase, ModelInputForCPU,
+                                        #   ModelInputForCPUBuilder)
+from vllm.worker.xpu_model_runner import ModelInputForXPU, XPUModelRunnerBase, ModelInputForXPUBuilder
+
+
+@dataclasses.dataclass(frozen=True)
+class ModelInputForXPUWithPoolingMetadata(ModelInputForXPU):
+    """
+    Used by the CPUPoolingModelRunner.
+    """
+    pooling_metadata: Optional["PoolingMetadata"] = None
+
+
+class XPUPoolingModelRunner(
+        XPUModelRunnerBase[ModelInputForXPUWithPoolingMetadata]):
+    _model_input_cls: Type[ModelInputForXPUWithPoolingMetadata] = (
+        ModelInputForXPUWithPoolingMetadata)
+    _builder_cls: Type[ModelInputForXPUBuilder] = ModelInputForXPUBuilder
+
+    @torch.inference_mode()
+    def execute_model(
+        self,
+        model_input: ModelInputForXPUWithPoolingMetadata,
+        kv_caches: List[torch.Tensor],
+        intermediate_tensors: Optional[IntermediateTensors] = None,
+        num_steps: int = 1,
+    ) -> Optional[Union[List[PoolerOutput], IntermediateTensors]]:
+        if num_steps > 1:
+            raise ValueError(
+                "Currently multi-step worker does not support multi-steps...")
+
+        # num_layers = self.model_config.get_num_layers(self.parallel_config)
+        # use an empty tensor instead of `None`` to force Dynamo to pass
+        # it by reference, rather by specializing on the value ``None``.
+        # the `dtype` argument does not matter, and we use `float32` as
+        # a placeholder (it has wide hardware support).
+        # TODO: check if we need float16...
+        # kv_caches = [
+        #     torch.tensor([], dtype=torch.float32, device=self.device)
+        #     for _ in range(num_layers)
+        # ]
+
+        model_executable = self.model
+        cross_enc_kwargs = {}
+        # if model_input.token_type_ids is not None:
+        #     cross_enc_kwargs["token_type_ids"] = model_input.token_type_ids
+        execute_model_kwargs = {
+            "input_ids":
+            model_input.input_tokens,
+            "positions":
+            model_input.input_positions,
+            # "kv_caches":
+            # kv_caches,
+            # "attn_metadata":
+            # model_input.attn_metadata,
+            **MultiModalKwargs.as_kwargs(model_input.multi_modal_kwargs or {},
+                                         device=self.device),
+            **cross_enc_kwargs,
+            "intermediate_tensors":
+            intermediate_tensors,
+        }
+
+        with set_forward_context(model_input.attn_metadata, self.vllm_config):
+            hidden_states = model_executable(**execute_model_kwargs)
+
+        # Only perform pooling in the driver worker.
+        if not self.is_driver_worker:
+            return []
+
+        return [
+            self.model.pooler(hidden_states=hidden_states,
+                              pooling_metadata=model_input.pooling_metadata)
+        ]
+
+    def make_model_input_from_broadcasted_tensor_dict(
+            self,
+            tensor_dict: Dict[str,
+                              Any]) -> ModelInputForXPUWithPoolingMetadata:
+        return ModelInputForXPUWithPoolingMetadata.from_broadcasted_tensor_dict(
+            tensor_dict,
+            attn_backend=self.attn_backend,
+        )
+
+    def prepare_model_input(
+        self,
+        seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],
+        virtual_engine: int = 0,
+        finished_requests_ids: Optional[List[str]] = None
+    ) -> ModelInputForXPUWithPoolingMetadata:
+        assert seq_group_metadata_list is not None
+        model_input = self._prepare_model_input_tensors(
+            seq_group_metadata_list, finished_requests_ids)
+        # Prepare PoolingMetadata.
+        assert model_input.seq_lens is not None
+        pooling_metadata = self._prepare_pooling(seq_group_metadata_list,
+                                                 model_input.seq_lens)
+
+        return dataclasses.replace(model_input,
+                                   virtual_engine=virtual_engine,
+                                   pooling_metadata=pooling_metadata)
+
+    def _prepare_pooling(
+        self,
+        seq_group_metadata_list: List[SequenceGroupMetadata],
+        prompt_lens: List[int],
+    ) -> PoolingMetadata:
+        """Prepare PoolingMetadata for the sequence group metadata list."""
+        seq_groups: List[Tuple[List[int], PoolingParams]] = []
+        for i, seq_group_metadata in enumerate(seq_group_metadata_list):
+            seq_ids = list(seq_group_metadata.seq_data.keys())
+            pooling_params = seq_group_metadata.pooling_params
+            seq_groups.append((seq_ids, pooling_params))
+
+        seq_data: Dict[int, SequenceData] = {}
+        for seq_group_metadata in seq_group_metadata_list:
+            seq_data.update(seq_group_metadata.seq_data)
+
+        pooling_metadata = PoolingMetadata(
+            seq_groups=seq_groups,
+            seq_data=seq_data,
+            prompt_lens=prompt_lens,
+        )
+
+        return pooling_metadata
diff --git a/vllm/worker/xpu_worker.py b/vllm/worker/xpu_worker.py
index 3aea0d741..060bb10ab 100644
--- a/vllm/worker/xpu_worker.py
+++ b/vllm/worker/xpu_worker.py
@@ -2,9 +2,10 @@
 """A XPU worker class."""
 import gc
 import os
-from typing import List, Optional, Tuple
+from typing import List, Optional, Tuple, Type
 
 import intel_extension_for_pytorch  # noqa: F401
+# TODO: handle case for oneccl_bindings for dual cards
 import oneccl_bindings_for_pytorch  # noqa: F401
 import torch
 import torch.distributed
@@ -19,7 +20,8 @@ from vllm.platforms import current_platform
 from vllm.worker.cache_engine import CacheEngine
 from vllm.worker.worker import Worker
 from vllm.worker.worker_base import LoRANotSupportedWorkerBase, WorkerBase
-from vllm.worker.xpu_model_runner import XPUModelRunner
+from vllm.worker.xpu_model_runner import XPUModelRunner, XPUModelRunnerBase
+from vllm.worker.xpu_pooling_model_runner import XPUPoolingModelRunner
 
 logger = init_logger(__name__)
 
@@ -56,8 +58,12 @@ class XPUWorker(LoRANotSupportedWorkerBase, Worker):
         if parallel_config and is_driver_worker:
             assert rank % parallel_config.tensor_parallel_size == 0, \
                    "Driver worker should be rank 0 of tensor parallel group."
+        ModelRunnerClass: Type[XPUModelRunnerBase] = XPUModelRunner
+        model_config = self.model_config
+        if model_config.task == "embed":
+            ModelRunnerClass = XPUPoolingModelRunner
 
-        self.model_runner = XPUModelRunner(  # type: ignore
+        self.model_runner = ModelRunnerClass(  # type: ignore
             vllm_config=vllm_config,
             kv_cache_dtype=self.cache_config.cache_dtype,
             is_driver_worker=is_driver_worker,
@@ -65,7 +71,7 @@ class XPUWorker(LoRANotSupportedWorkerBase, Worker):
         # Uninitialized cache engine. Will be initialized by
         # initialize_cache.
         self.cache_engine: List[CacheEngine]
-        self.gpu_cache: Optional[List[List[torch.Tensor]]]
+        self.gpu_cache: Optional[List[List[torch.Tensor]]] = None
 
     def init_device(self) -> None:
         if self.device_config.device.type == "xpu" and current_platform.is_xpu(
@@ -100,6 +106,7 @@ class XPUWorker(LoRANotSupportedWorkerBase, Worker):
         # Profile the memory usage of the model and get the maximum number of
         # cache blocks that can be allocated with the remaining free memory.
         torch.xpu.empty_cache()
+        before_memory = torch.xpu.memory_reserved()
 
         # Execute a forward pass with dummy inputs to profile the memory usage
         # of the model.
@@ -108,7 +115,7 @@ class XPUWorker(LoRANotSupportedWorkerBase, Worker):
         # Calculate the number of blocks that can be allocated with the
         # profiled peak memory.
         torch.xpu.synchronize()
-        used_memory = torch.xpu.memory_allocated()
+        used_memory = torch.xpu.memory_reserved()
         total_gpu_memory = torch.xpu.get_device_properties(
             self.local_rank).total_memory
         free_gpu_memory = total_gpu_memory - used_memory
@@ -132,6 +139,20 @@ class XPUWorker(LoRANotSupportedWorkerBase, Worker):
         num_cpu_blocks = max(num_cpu_blocks, 0)
         gc.collect()
         torch.xpu.empty_cache()
+        flag = os.getenv("IPEX_LLM_MAX_INPUT_LENGTH_DETAIL", None)
+        if flag is not None:
+            in_len = self.scheduler_config.max_num_batched_tokens / 1024
+            logger.info(f"model first init memory {before_memory/(1024**3)} GB")
+            logger.info(f"one card total_gpu_memory = {total_gpu_memory/(1024**3)} GB")
+            logger.info(f"after first_token running, peak_memory {peak_memory/(1024**3)} GB")
+            add_memory = peak_memory-before_memory
+            total_add_memory = total_gpu_memory*self.cache_config.gpu_memory_utilization-before_memory
+            max_input = total_add_memory / (1024/self.cache_config.block_size*cache_block_size + add_memory/in_len)
+            logger.info(f"total_add_memory {total_add_memory/(1024**3)} GB")
+            logger.info(f"input max-model-len(or max-num-batched-tokens) {in_len} K")
+            logger.info(f"Theoretical max input length A: {max_input} K")
+            logger.info(f"Actually support max input length on this num_gpu_blocks B:{num_gpu_blocks*self.cache_config.block_size/1024} K")
+            logger.info(f"We need to increase A and decrease B (B>A) so that they reach a close value.")
         return num_gpu_blocks, num_cpu_blocks
 
     def _warm_up_model(self) -> None:
@@ -177,9 +198,10 @@ class XPUWorker(LoRANotSupportedWorkerBase, Worker):
             parallel_config.tensor_parallel_size,
             parallel_config.pipeline_parallel_size)
         # global all_reduce needed for overall oneccl warm up
-        torch.distributed.all_reduce(torch.zeros(1).xpu())
-
+        # torch.distributed.all_reduce(torch.zeros(1).xpu())
+        from vllm.distributed.parallel_state import get_pp_group
         if parallel_config.pipeline_parallel_size > 1:
-            # Add pp group init to avoid
-            # p2p communication as the first call
-            get_pp_group().all_reduce(torch.zeros(1).xpu())
+            # torch-ccl xpu need a collective API warm up
+            # before calling send/recv API
+            get_pp_group().all_gather(torch.zeros(1).xpu())
+
